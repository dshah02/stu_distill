{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devan\\AppData\\Local\\Temp\\ipykernel_12764\\3629752296.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stu_layer_full = torch.load(f\"./stu_layer_{args.layer_i}_500m_param_full.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to import FlashFFTConv: No module named 'flashfftconv'. Falling back to PyTorch implementation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\devan\\anaconda3\\envs\\flashstu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.3745590448379517\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 84\u001b[0m\n\u001b[0;32m     82\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     83\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(lds\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 84\u001b[0m lds_loss_values\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     85\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the LDS class\n",
    "class LDS(nn.Module):\n",
    "    def __init__(self, state_dim, input_dim, output_dim):\n",
    "        super(LDS, self).__init__()\n",
    "        self.d_out = output_dim\n",
    "        self.h0 = nn.Parameter(torch.randn(state_dim))\n",
    "        init_A = torch.randn(state_dim)\n",
    "        self.A = nn.Parameter(init_A / torch.max(torch.abs(init_A)))\n",
    "        self.B = nn.Parameter(torch.randn(input_dim, state_dim) / input_dim)\n",
    "        self.C = nn.Parameter(torch.randn(state_dim, output_dim) / state_dim)\n",
    "        self.D = nn.Parameter(torch.randn(input_dim, output_dim) / output_dim)\n",
    "        self.M = nn.Parameter(torch.randn(output_dim, output_dim) / output_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        bsz, seq_len, _ = inputs.shape\n",
    "        h_t = self.h0.expand(bsz, self.h0.shape[0]).to(inputs.device)\n",
    "        all_h_t = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            u_t = inputs[:, t, :]\n",
    "            h_t = self.A.flatten() * h_t + u_t @ self.B\n",
    "            all_h_t.append(h_t.unsqueeze(1))\n",
    "\n",
    "        all_h_t = torch.cat(all_h_t, dim=1)\n",
    "        outputs = torch.matmul(all_h_t, self.C)\n",
    "        return outputs\n",
    "\n",
    "    def compute_loss(self, inputs, targets):\n",
    "        mse_loss = nn.MSELoss()\n",
    "        outputs = self(inputs)\n",
    "        return mse_loss(outputs, targets)\n",
    "\n",
    "\n",
    "# Command-line argument parsing\n",
    "# parser = argparse.ArgumentParser(description=\"Train LDS model\")\n",
    "# parser.add_argument(\"--layer_i\", type=int, help=\"Layer index\", default = 2)\n",
    "# parser.add_argument(\"--state_dim\", type=int, help=\"State dimension\", default = 100)\n",
    "# parser.add_argument(\"--batch_size\", type=int,  help=\"Batch size\", default = 5)\n",
    "# parser.add_argument(\"--epochs\", type=int,  help=\"Number of epochs\", default = 100)\n",
    "# parser.add_argument(\"--seq_len\", type=int,help=\"Sequence length\", default = 1000)\n",
    "# parser.add_argument(\"--lr\", type=float, help=\"Learning rate\", default = 0.01)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "class Obj():\n",
    "    def __init__(self):\n",
    "        self.layer_i = 2\n",
    "        self.state_dim = 100\n",
    "        self.batch_size = 5\n",
    "        self.epochs = 100\n",
    "        self.seq_len = 1000\n",
    "        self.lr = .02\n",
    "\n",
    "args = Obj() \n",
    "\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the layer i weights\n",
    "stu_layer_full = torch.load(f\"./stu_layer_{args.layer_i}_500m_param_full.pt\")\n",
    "stu_layer_full.to(device)\n",
    "\n",
    "# Initialize LDS model\n",
    "lds = LDS(args.state_dim, 768, 768).to(device)\n",
    "optimizer = torch.optim.Adam(lds.parameters(), lr=args.lr)\n",
    "\n",
    "# Training\n",
    "lds_epochs = args.epochs\n",
    "lds_loss_values = []\n",
    "\n",
    "for epoch in range(lds_epochs):\n",
    "    inputs = torch.randn(args.batch_size, args.seq_len, 768).to(device).to(torch.bfloat16)\n",
    "    stu_outputs = stu_layer_full(inputs).to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = lds.compute_loss(inputs.to(stu_outputs.dtype), stu_outputs)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(lds.parameters(), max_norm=1)\n",
    "    lds_loss_values.append(loss.item())\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        lds.A.data.clamp_(max=1, min=-1)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Save the loss progression to a file\n",
    "with open(\"loss_progression.txt\", \"w\") as f:\n",
    "    for loss_value in lds_loss_values:\n",
    "        f.write(f\"{loss_value}\\n\")\n",
    "\n",
    "# Plot the loss progression\n",
    "plt.figure()\n",
    "plt.plot(range(len(lds_loss_values)), lds_loss_values, label=\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Progression\")\n",
    "plt.legend()\n",
    "plt.savefig(\"loss_progression.png\")\n",
    "plt.close()\n",
    "\n",
    "# Save the LDS model and optimizer state\n",
    "torch.save({\n",
    "    \"model_state_dict\": lds.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "}, \"lds_model_and_optimizer.pt\")\n",
    "\n",
    "print(\"Training complete. Files saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available objects for config:\n",
      "    AliasManager\n",
      "    DisplayFormatter\n",
      "    HistoryManager\n",
      "    IPCompleter\n",
      "    IPKernelApp\n",
      "    LoggingMagics\n",
      "    MagicsManager\n",
      "    OSMagics\n",
      "    PrefilterManager\n",
      "    ScriptMagics\n",
      "    StoreMagics\n",
      "    ZMQInteractiveShell\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flashstu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
