{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Zo2VTh2kBjF"
   },
   "source": [
    "# Simple Language Modeling Notebook\n",
    "\n",
    "---\n",
    "\n",
    "**Author: Windsor Nguyen '25**\n",
    "\n",
    "This is a lightweight Python notebook for language modeling experiments.\n",
    "\n",
    "The goal of this notebook is to:\n",
    "- Be easy to prototype research ideas with\n",
    "- Robust enough to get an accurate gauge for a model architecture's abilities\n",
    "\n",
    "\n",
    "To get you started, we have code for the [Transformer](https://arxiv.org/abs/1706.03762) and the [Flash STU](https://arxiv.org/abs/2409.10489) model architectures.\n",
    "\n",
    "> NOTE: It is *highly* recommended that you run everything on a GPU/TPU. This notebook was written with PyTorch/GPUs in mind, so 100% compatibility with other frameworks or TPUs is not guaranteed.\n",
    "\n",
    "**May divine benevolence be with you and your research ideas!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pigmcCYYM791"
   },
   "source": [
    "# Install required packages\n",
    "\n",
    "Pip install required Python packages here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_9IFAhARrf4"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcPldzULT7kR"
   },
   "source": [
    "# Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "caZ7igF7jzTq"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "from contextlib import nullcontext\n",
    "from functools import partial\n",
    "from typing import Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import safetensors\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pathlib import Path\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tAEwHLRZwfj"
   },
   "source": [
    "# Logging settings\n",
    "\n",
    "Adjust logging settings here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MAU7vLbjZyXy"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gk5tHpnBT4yl"
   },
   "source": [
    "# Plotting settings\n",
    "\n",
    "Adjust plotting settings here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCaJ-mvGRudF"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpCBanCaNFKL"
   },
   "source": [
    "# PyTorch settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oAp02t-fNG1D"
   },
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "SEED = 1746 # @param {type:\"integer\"}\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "CUDA_AVAILABLE = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\") if CUDA_AVAILABLE else torch.device(\"cpu\")\n",
    "if CUDA_AVAILABLE:\n",
    "    print(f\"Connected to {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    print(\"No CUDA devices found, running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pL0LQMtRM2IQ"
   },
   "source": [
    "# Google Colab settings\n",
    "\n",
    "This will point to your Google Drive so that you can access training data, save outputs, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pajeb2QlpvEN"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "save_dir = \"/content/drive/MyDrive/stu_exps\"  # @param {type:\"string\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JoNiLaANKKQ"
   },
   "source": [
    "# Utility functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_PkkM8V7lc4e"
   },
   "outputs": [],
   "source": [
    "def nearest_power_of_two(x: int, round_up: bool = False) -> int:\n",
    "    return (\n",
    "        1 << math.floor(math.log2(x)) if not round_up else 1 << math.ceil(math.log2(x))\n",
    "    )\n",
    "\n",
    "def get_hankel(seq_len: int, use_hankel_L: bool = False) -> np.ndarray:\n",
    "    entries = np.arange(1, seq_len + 1, dtype=np.float32)\n",
    "    i_plus_j = entries[:, None] + entries[None, :]\n",
    "    if use_hankel_L:\n",
    "        sgn = (-1.0) ** (i_plus_j - 2.0) + 1.0\n",
    "        denom = (i_plus_j + 3.0) * (i_plus_j - 1.0) * (i_plus_j + 1.0)\n",
    "        Z = sgn * (8.0 / denom)\n",
    "    else:\n",
    "        Z = 2.0 / (i_plus_j**3 - i_plus_j)\n",
    "    return Z\n",
    "\n",
    "def get_spectral_filters(\n",
    "    seq_len: int,\n",
    "    k: int,\n",
    "    use_hankel_L: bool = False,\n",
    "    device: torch.device = None,\n",
    "    dtype: torch.dtype = torch.float32,\n",
    ") -> torch.Tensor:\n",
    "    print(f\"Generating spectral filters...\")\n",
    "    Z_np = get_hankel(seq_len, use_hankel_L)\n",
    "    sigma, phi = np.linalg.eigh(Z_np)\n",
    "    sigma, phi = sigma[-k:], phi[:, -k:]\n",
    "    phi *= sigma**0.25\n",
    "    print(\"Spectral filters built!\")\n",
    "    return torch.tensor(phi, device=device, dtype=dtype)\n",
    "\n",
    "def stu_conv(u: torch.Tensor, v: torch.Tensor, n: int, use_tensordot: bool = True) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    bsz, seq_len, d_in = u.shape\n",
    "\n",
    "    sgn = torch.full((1, seq_len, 1), 1, device=u.device)\n",
    "    sgn[:, 1::2] *= -1\n",
    "\n",
    "    if use_tensordot:\n",
    "        _, d_out = v.shape\n",
    "        v = v.view(1, -1, d_out, 1).to(torch.float32)\n",
    "    else:\n",
    "        _, K = v.shape\n",
    "        sgn = sgn.unsqueeze(-1)\n",
    "        v = v.view(1, -1, K, 1, 1).to(torch.float32) # (bsz, seq_len, K, d_in, stack)\n",
    "        u = u.view(bsz, -1, 1, d_in).expand(bsz, -1, K, d_in)\n",
    "\n",
    "    v = torch.fft.rfft(v, n=n, dim=1)\n",
    "    U = torch.stack([u, u * sgn], dim=-1).to(torch.float32)\n",
    "    U = torch.fft.rfft(U, n=n, dim=1)\n",
    "    U_conv = torch.fft.irfft(v * U, n=n, dim=1)[:, :seq_len]\n",
    "    U_plus, U_minus = torch.unbind(U_conv, dim=-1)\n",
    "    U_minus = U_minus * sgn\n",
    "\n",
    "    return U_plus, U_minus\n",
    "\n",
    "def linear_decay_with_warmup( # https://arxiv.org/pdf/2310.07831\n",
    "    current_step: int,\n",
    "    warmup_steps: int,\n",
    "    num_steps: int,\n",
    "    max_lr: float = 3e-4,\n",
    "    min_lr: float = 3e-5,\n",
    ") -> float:\n",
    "    if current_step < warmup_steps:\n",
    "        return min_lr + (max_lr - min_lr) * float(current_step) / float(max(warmup_steps, 1))\n",
    "    else:\n",
    "        return max_lr - (max_lr - min_lr) * float(current_step - warmup_steps) / float(max(num_steps - warmup_steps, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEfmnLfMNr2y"
   },
   "source": [
    "# MLP\n",
    "\n",
    "SwiGLU variant. See more [here](https://arxiv.org/abs/2002.05202)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvhKsG2OmUZS"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim: int, inter_dim: int, dtype: torch.dtype = torch.float32):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(dim, inter_dim, dtype=dtype, bias=False)\n",
    "        self.w2 = nn.Linear(inter_dim, dim, dtype=dtype, bias=False)\n",
    "        self.w3 = nn.Linear(dim, inter_dim, dtype=dtype, bias=False)\n",
    "        self.w2.SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRJIS5bU36xS"
   },
   "source": [
    "# Rotary Positional Embeddings\n",
    "\n",
    "See more [here](https://arxiv.org/abs/2104.09864).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YqGQUr194ADF"
   },
   "outputs": [],
   "source": [
    "class RoPE(nn.Module):\n",
    "    def __init__(self, head_dim: int, max_seq_len: int = 4096, base: int = 10000):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.base = base\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # Theta computation for rotation frequencies\n",
    "        theta = 1.0 / (self.base ** (torch.arange(0, self.head_dim, 2)[: (self.head_dim // 2)].float() / self.head_dim))\n",
    "        self.register_buffer(\"theta\", theta, persistent=False)\n",
    "\n",
    "        seq_idx = torch.arange(self.max_seq_len, dtype=theta.dtype, device=theta.device)\n",
    "        idx_theta = seq_idx[:, None] * theta[None, :]\n",
    "        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)\n",
    "        self.register_buffer(\"cache\", cache, persistent=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, *, input_pos: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Applies rotary positional embeddings to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_len, num_heads, head_dim].\n",
    "            input_pos (Optional[torch.Tensor]): Optional position indices.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor with rotary embeddings applied.\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[input_pos]\n",
    "        xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n",
    "        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2)\n",
    "        x_out = torch.stack(\n",
    "            [\n",
    "                xshaped[..., 0] * rope_cache[..., 0] - xshaped[..., 1] * rope_cache[..., 1],\n",
    "                xshaped[..., 1] * rope_cache[..., 0] + xshaped[..., 0] * rope_cache[..., 1],\n",
    "            ],\n",
    "            -1,\n",
    "        )\n",
    "        x_out = x_out.flatten(3)\n",
    "        return x_out.type_as(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3E_XkMW9OETB"
   },
   "source": [
    "# Attention\n",
    "\n",
    "Standard Multi-head Attention with RoPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72Yh82Hph2gP"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Standard Multi-head Attention with Rotary Positional Embeddings\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.dim % config.num_heads == 0, (\n",
    "            f\"dim must be divisible by num_heads, got dim={dim} and num_heads={num_heads}\"\n",
    "        )\n",
    "        self.dim = config.dim\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = config.dim // config.num_heads\n",
    "        self.seq_len = config.seq_len\n",
    "        self.rope_theta = rope_theta\n",
    "\n",
    "        # Rotary positional embeddings\n",
    "        self.rope = RoPE(self.head_dim, self.seq_len, self.rope_theta)\n",
    "\n",
    "        # Learned projections\n",
    "        self.wq = nn.Linear(dim, dim)\n",
    "        self.wk = nn.Linear(dim, dim)\n",
    "        self.wv = nn.Linear(dim, dim)\n",
    "        self.wo = nn.Linear(dim, dim)\n",
    "        self.wo.SCALE_INIT = 1\n",
    "\n",
    "        # Register causal mask as a buffer\n",
    "        causal_mask = torch.tril(torch.ones(self.seq_len, self.seq_len))\n",
    "        self.register_buffer(\"causal_mask\", causal_mask)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, training: bool = False) -> torch.Tensor:\n",
    "        batch_size, seq_len, dim = x.shape\n",
    "\n",
    "        # Linear projections\n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        # Split into multiple attention heads\n",
    "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Apply positional embeddings to queries and keys\n",
    "        q, k = self.rope(q).transpose(1, 2), self.rope(k).transpose(1, 2)\n",
    "\n",
    "        # Compute scaled similarity scores\n",
    "        scale = self.head_dim ** 0.5\n",
    "        scores = torch.matmul(q, k.transpose(-1, -2)) / scale\n",
    "\n",
    "        # Apply causal mask\n",
    "        causal_mask = self.causal_mask[:seq_len, :seq_len]  # For inference\n",
    "        scores = scores.masked_fill(causal_mask == 0, float(\"-inf\"))\n",
    "\n",
    "        # Apply softmax\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        # Apply attention weights\n",
    "        ctxt = torch.matmul(attn_weights, v)\n",
    "\n",
    "        # Concatenate attention heads back together\n",
    "        ctxt = ctxt.transpose(1, 2)\n",
    "        ctxt = ctxt.contiguous().view(batch_size, seq_len, -1)\n",
    "\n",
    "        # Output projection\n",
    "        out = self.wo(ctxt)\n",
    "        return out\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attn_norm = nn.LayerNorm(config.dim, dtype=config.torch_dtype)\n",
    "        self.attn = Attention(config)\n",
    "        self.mlp_norm = nn.LayerNorm(config.dim, dtype=config.torch_dtype)\n",
    "        self.mlp = MLP(config.dim, config.inter_dim, dtype=config.torch_dtype)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.attn(self.attn_norm(x))\n",
    "        x = x + self.mlp(self.mlp_norm(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0z_M2rr5IwE9"
   },
   "source": [
    "# Transformer\n",
    "\n",
    "Classic dense Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vdUivfKgIz8y"
   },
   "outputs": [],
   "source": [
    "# Transformer configurations\n",
    "\n",
    "class TransformerConfig(PretrainedConfig):\n",
    "    model_type = \"transformer\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        bsz: int = 1,\n",
    "        dim: int = 896,\n",
    "        num_heads: int = 16,\n",
    "        num_layers: int = 12,\n",
    "        seq_len: int = 8192,\n",
    "        vocab_size: int = 200064,\n",
    "        mlp_scale: int = 12,\n",
    "        weight_tying: bool = True,\n",
    "        bias: bool = False,\n",
    "        rope_theta: float = 10000.0,\n",
    "        torch_dtype: torch.dtype = torch.bfloat16,\n",
    "        device: torch.device = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.bsz = bsz\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.mlp_scale = mlp_scale\n",
    "        self.inter_dim = self.dim * self.mlp_scale\n",
    "        self.weight_tying = weight_tying\n",
    "        self.bias = bias\n",
    "        self.rope_theta = rope_theta\n",
    "        self.torch_dtype = torch_dtype\n",
    "        self.device = device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pucny6EcIwE9"
   },
   "outputs": [],
   "source": [
    "# Transformer model architecture\n",
    "\n",
    "class Transformer(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.tok_emb = nn.Embedding(config.vocab_size, config.dim)\n",
    "        self.num_layers = config.num_layers\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            AttentionLayer(config) for _ in range(self.num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm_f = nn.LayerNorm(config.dim)\n",
    "        self.lm_head = nn.Linear(config.dim, config.vocab_size, bias=config.bias)\n",
    "\n",
    "        if config.weight_tying:\n",
    "            self.tok_emb.weight = self.lm_head.weight\n",
    "\n",
    "        self.std = config.dim ** -0.5\n",
    "        self.apply(self._init_weights)\n",
    "        print(\"Model Parameter Count: %.2fM\\n\" % (self._get_num_params() / 1e6,))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tok_emb(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return self.lm_head(self.norm_f(x))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if hasattr(module, \"SCALE_INIT\"):\n",
    "                self.std *= (2 * self.num_layers) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def _get_num_params(self):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if hasattr(self, \"pos_emb\") and self.pos_emb is not None:\n",
    "            n_params -= self.pos_emb.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "bsz = 1  # @param {type:\"integer\"}\n",
    "dim = 768 # @param {type:\"integer\"}\n",
    "num_heads = 12 # @param {type:\"integer\"}\n",
    "num_layers = 8 # @param {type:\"integer\"}\n",
    "seq_len = 1024 # @param {type:\"integer\"}\n",
    "vocab_size = 200064 # @param {type:\"integer\"}\n",
    "mlp_scale = 12 # @param {type:\"integer\"}\n",
    "weight_tying = True # @param {type:\"boolean\"}\n",
    "bias = False # @param {type:\"boolean\"}\n",
    "rope_theta = 10000.0 # @param {type:\"number\"}\n",
    "\n",
    "# Must match an attribute in torch (e.g., \"bfloat16\", \"float32\")\n",
    "torch_dtype = \"float32\"  # @param {type:\"string\"}\n",
    "torch_dtype = getattr(torch, torch_dtype)\n",
    "print(\"Torch dtype:\", torch_dtype)\n",
    "\n",
    "device_str = \"cuda\"  # @param {type:\"string\"}\n",
    "if not CUDA_AVAILABLE and device_str == \"cuda\":\n",
    "    print(\"No CUDA devices detected but CUDA requested, setting device to CPU...\")\n",
    "    device_str = \"cpu\"\n",
    "device = torch.device(device_str)\n",
    "\n",
    "config = TransformerConfig(\n",
    "    bsz=bsz,\n",
    "    dim=dim,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    seq_len=seq_len,\n",
    "    vocab_size=vocab_size,\n",
    "    mlp_scale=mlp_scale,\n",
    "    weight_tying=weight_tying,\n",
    "    bias=bias,\n",
    "    rope_theta=rope_theta,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=torch.device(device_str),\n",
    ")\n",
    "\n",
    "print(\"\\nConfigs:\")\n",
    "for key, value in vars(config).items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "model = Transformer(config).to(device=device, dtype=torch_dtype)\n",
    "x = torch.randint(0, config.vocab_size, (config.bsz, config.seq_len), device=device)\n",
    "outputs = model(x)\n",
    "\n",
    "print(\"Output shape:\", outputs.shape)\n",
    "print(\"Sample output:\", outputs[0, 0, :10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaBUKK4vN0Uk"
   },
   "source": [
    "# Spectral Transform Unit (STU)\n",
    "\n",
    "See more [here](https://arxiv.org/abs/2312.06837)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtuDBhfUhwgb"
   },
   "outputs": [],
   "source": [
    "class STU(nn.Module):\n",
    "    def __init__(self, config, filters) -> None:\n",
    "        super(STU, self).__init__()\n",
    "        self.config = config\n",
    "        self.dim = config.dim\n",
    "        self.stu_filters = filters\n",
    "        self.n = nearest_power_of_two(config.seq_len * 2 - 1, round_up=True)\n",
    "        self.K = config.num_eigh\n",
    "        self.r = config.r\n",
    "        self.use_hankel_L = config.use_hankel_L\n",
    "        self.use_tensordot = config.use_tensordot\n",
    "\n",
    "        if self.use_tensordot:\n",
    "            # Projection matrices\n",
    "            self.M_inputs = nn.Parameter(torch.empty(self.dim, self.r, dtype=config.torch_dtype))\n",
    "            self.M_filters = nn.Parameter(torch.empty(self.K, self.r, dtype=config.torch_dtype))\n",
    "            self.out_proj = nn.Linear(self.r, self.dim, bias=config.bias)\n",
    "        else:\n",
    "            # Full M matrix\n",
    "            self.M_phi_plus = nn.Parameter(torch.empty(self.K, self.dim, self.dim, dtype=config.torch_dtype))\n",
    "\n",
    "            # If not using Hankel_L, we compute the negative featurization separately\n",
    "            if not self.use_hankel_L:\n",
    "                self.M_phi_minus = nn.Parameter(torch.empty(self.K, self.dim, self.dim, dtype=config.torch_dtype))\n",
    "\n",
    "    def forward(self, u: torch.Tensor) -> torch.Tensor:\n",
    "        if self.use_tensordot:\n",
    "            # Project first\n",
    "            u_proj = u @ self.M_inputs                     # (B, L, D) x (D, r) -> (B, L, r)\n",
    "            phi_proj = self.stu_filters @ self.M_filters   # (L, K) x (K, r) -> (L, r)\n",
    "\n",
    "            # Then, convolve: (B, L, r) ⊗ (L, r) -> (B, L, r)\n",
    "            spectral_plus, spectral_minus = stu_conv(u_proj, phi_proj, self.n, self.use_tensordot)\n",
    "        else:\n",
    "            # Convolve first to get featurized inputs: (B, L, D) x (L, K) -> (B, L, K, D)\n",
    "            U_plus, U_minus = stu_conv(u, self.stu_filters, self.n, self.use_tensordot)\n",
    "\n",
    "            # Compute sum-product of featurized inputs and M matrices over the K filters\n",
    "            B, L, K, D = U_plus.shape\n",
    "\n",
    "            # Spectral output: (B, L, K * D) x (K * D, D) -> (B, L, D)\n",
    "            spectral_plus = U_plus.view(B, L, K * self.dim) @ self.M_phi_plus.view(K * self.dim, self.dim)\n",
    "\n",
    "            if not self.use_hankel_L:\n",
    "                spectral_minus = U_minus.view(B, L, K * self.dim) @ self.M_phi_minus.view(K * self.dim, self.dim)\n",
    "\n",
    "        out = spectral_plus if self.use_hankel_L else spectral_plus + spectral_minus\n",
    "        out = self.out_proj(out) if self.use_tensordot else out\n",
    "        return out\n",
    "\n",
    "class STULayer(nn.Module):\n",
    "    def __init__(self, config, stu_filters):\n",
    "        super(STULayer, self).__init__()\n",
    "        self.stu_norm = nn.LayerNorm(config.dim)\n",
    "        self.stu = STU(config, stu_filters)\n",
    "        self.mlp_norm = nn.LayerNorm(config.dim)\n",
    "        self.mlp = MLP(config.dim, config.inter_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.stu(self.stu_norm(x))\n",
    "        x = x + self.mlp(self.mlp_norm(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t58-rw5OOKaJ"
   },
   "source": [
    "# Flash STU\n",
    "\n",
    "Model definition for the Flash STU architecture. See more [here](https://arxiv.org/abs/2409.10489)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fTiayj6u-ogZ"
   },
   "outputs": [],
   "source": [
    "class FlashSTUConfig(PretrainedConfig):\n",
    "    model_type = \"flash_stu\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        bsz: int = 1,\n",
    "        dim: int = 768,\n",
    "        num_heads: int = 12,\n",
    "        num_layers: int = 8,\n",
    "        seq_len: int = 8192,\n",
    "        window_size: int = 1024,\n",
    "        vocab_size: int = 200064,\n",
    "        mlp_scale: int = 12,\n",
    "        weight_tying: bool = True,\n",
    "        bias: bool = False,\n",
    "        num_eigh: int = 24,\n",
    "        r: int = 8,\n",
    "        use_hankel_L: bool = False,\n",
    "        use_tensordot: bool = True,\n",
    "        use_attn: bool = True,\n",
    "        rope_theta: float = 10000.0,\n",
    "        torch_dtype: torch.dtype = torch.bfloat16,\n",
    "        device: torch.device = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.bsz = bsz\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.seq_len = seq_len\n",
    "        self.window_size = window_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.mlp_scale = mlp_scale\n",
    "        self.inter_dim = self.dim * self.mlp_scale\n",
    "        self.weight_tying = weight_tying\n",
    "        self.bias = bias\n",
    "        self.num_eigh = num_eigh\n",
    "        self.r = r\n",
    "        self.use_hankel_L = use_hankel_L\n",
    "        self.use_tensordot = use_tensordot\n",
    "        self.use_attn = use_attn\n",
    "        self.rope_theta = rope_theta\n",
    "        self.torch_dtype = torch_dtype\n",
    "        self.device = device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVu4lZMWmJPj"
   },
   "outputs": [],
   "source": [
    "class FlashSTU(PreTrainedModel):\n",
    "    config_class = FlashSTUConfig\n",
    "\n",
    "    def __init__(self, config, filters) -> None:\n",
    "        super(FlashSTU, self).__init__(config)\n",
    "        assert config.dim % config.num_heads == 0, f\"dim ({self.dim}) must be divisible num_heads ({self.num_heads})\"\n",
    "        self.head_dim = config.dim // config.num_heads\n",
    "\n",
    "        self.use_tensordot = config.use_tensordot\n",
    "        self.use_hankel_L = config.use_hankel_L\n",
    "\n",
    "        self.tok_emb = nn.Embedding(config.vocab_size, config.dim)\n",
    "\n",
    "        self.num_layers = config.num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for layer_idx in range(config.num_layers):\n",
    "            # For more complex %-split arrangements, see https://arxiv.org/pdf/2406.07887\n",
    "            if layer_idx % 2 == 0:\n",
    "                self.layers.append(STULayer(config, filters))\n",
    "            else:\n",
    "                self.layers.append(AttentionLayer(config) if config.use_attn else STULayer(config, filters))\n",
    "\n",
    "        self.norm_f = nn.LayerNorm(config.dim)\n",
    "        self.lm_head = nn.Linear(config.dim, config.vocab_size, bias=config.bias)\n",
    "\n",
    "        if config.weight_tying:\n",
    "            self.tok_emb.weight = self.lm_head.weight\n",
    "\n",
    "        self.std = config.dim ** -0.5\n",
    "        self.apply(self._init_weights)\n",
    "        print(\"Model Parameter Count: %.2fM\\n\" % (self._get_num_params() / 1e6,))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.tensor:\n",
    "        x = self.tok_emb(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        out = self.lm_head(self.norm_f(x))\n",
    "        return out\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if hasattr(module, \"SCALE_INIT\"):\n",
    "                self.std *= (2 * self.num_layers) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, STU):\n",
    "            if self.use_tensordot:\n",
    "                torch.nn.init.xavier_normal_(module.M_inputs)\n",
    "                torch.nn.init.xavier_normal_(module.M_filters)\n",
    "            else:\n",
    "                torch.nn.init.xavier_normal_(module.M_phi_plus)\n",
    "                if not self.use_hankel_L:\n",
    "                    torch.nn.init.xavier_normal_(module.M_phi_minus)\n",
    "\n",
    "    def _get_num_params(self):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        return n_params\n",
    "\n",
    "bsz = 1  # @param {type:\"integer\"}\n",
    "dim = 768 # @param {type:\"integer\"}\n",
    "num_heads = 12 # @param {type:\"integer\"}\n",
    "num_layers = 8 # @param {type:\"integer\"}\n",
    "seq_len = 1024 # @param {type:\"integer\"}\n",
    "window_size = 128 # @param {type:\"integer\"}\n",
    "vocab_size = 200064 # @param {type:\"integer\"}\n",
    "mlp_scale = 4 # @param {type:\"integer\"}\n",
    "weight_tying = True # @param {type:\"boolean\"}\n",
    "bias = False # @param {type:\"boolean\"}\n",
    "num_eigh = 24 # @param {type:\"integer\"}\n",
    "r = 64 # @param {type:\"integer\"}\n",
    "use_hankel_L = False # @param {type:\"boolean\"}\n",
    "use_tensordot = True # @param {type:\"boolean\"}\n",
    "use_attn = True #@param {type:\"boolean\"}\n",
    "rope_theta = 10000.0 # @param {type:\"number\"}\n",
    "\n",
    "# Must match an attribute in torch (e.g., \"bfloat16\", \"float32\")\n",
    "torch_dtype = \"float32\"  # @param {type:\"string\"}\n",
    "torch_dtype = getattr(torch, torch_dtype)\n",
    "print(\"Torch dtype:\", torch_dtype)\n",
    "\n",
    "device_str = \"cuda\"  # @param {type:\"string\"}\n",
    "if not CUDA_AVAILABLE and device_str == \"cuda\":\n",
    "    print(\"No CUDA devices detected but CUDA requested, setting device to CPU...\")\n",
    "    device_str = \"cpu\"\n",
    "device = torch.device(device_str)\n",
    "\n",
    "config = FlashSTUConfig(\n",
    "    bsz=bsz,\n",
    "    dim=dim,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    seq_len=seq_len,\n",
    "    window_size=window_size,\n",
    "    vocab_size=vocab_size,\n",
    "    mlp_scale=mlp_scale,\n",
    "    weight_tying=weight_tying,\n",
    "    bias=bias,\n",
    "    num_eigh=num_eigh,\n",
    "    r=r,\n",
    "    use_hankel_L=use_hankel_L,\n",
    "    use_tensordot=use_tensordot,\n",
    "    use_attn=use_attn,\n",
    "    rope_theta=rope_theta,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=torch.device(device_str),\n",
    ")\n",
    "\n",
    "filters = get_spectral_filters(\n",
    "    seq_len=seq_len,\n",
    "    k=num_eigh,\n",
    "    use_hankel_L=use_hankel_L,\n",
    "    device=torch.device(device_str),\n",
    ")\n",
    "\n",
    "print(\"\\nConfigs:\")\n",
    "for key, value in vars(config).items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "model = FlashSTU(config, filters).to(device=device, dtype=torch_dtype)\n",
    "x = torch.randint(0, config.vocab_size, (config.bsz, config.seq_len), device=device)\n",
    "outputs = model(x)\n",
    "\n",
    "print(\"Output shape:\", outputs.shape)\n",
    "print(\"Sample output:\", outputs[0, 0, :10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVMHaQeY-M3w"
   },
   "source": [
    "# Dataloader\n",
    "Simple dataloader for next-token prediction training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M3ojdsgZ-PHw"
   },
   "outputs": [],
   "source": [
    "def load_tokens(filename):\n",
    "    try:\n",
    "        ext = os.path.splitext(filename)[1]\n",
    "\n",
    "        if ext == \".npy\":\n",
    "            npt = np.load(filename)\n",
    "            npt = npt.astype(np.int32)\n",
    "            ptt = torch.tensor(npt, dtype=torch.long)\n",
    "            return ptt\n",
    "        elif ext == \".pt\":\n",
    "            return torch.load(filename, weights_only=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file extension: {ext}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading file {filename}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "class Dataloader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        bsz: int,\n",
    "        seq_len: int,\n",
    "        rank: int,\n",
    "        world_size: int,\n",
    "        dataset: str,\n",
    "        split: str,\n",
    "        main_process: bool = False,\n",
    "    ):\n",
    "        self.bsz = bsz\n",
    "        self.seq_len = seq_len\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        assert split in {'train', 'val', 'test'}, f\"Invalid split: {split}\"\n",
    "\n",
    "        data_root = dataset\n",
    "        shards = [s for s in os.listdir(data_root) if split in s and (s.endswith('.pt') or s.endswith('.npy'))]\n",
    "        self.shards = [os.path.join(data_root, s) for s in sorted(shards)]\n",
    "        assert len(self.shards) > 0, f'No shards found for split {split}'\n",
    "        if main_process:\n",
    "            logger.info(f'Found {len(self.shards)} shards for split {split}')\n",
    "\n",
    "        # Default shard order is just sequential\n",
    "        self.shard_order = list(range(len(self.shards)))\n",
    "        self.shard_order_idx = 0\n",
    "        self.tokens = load_tokens(self.shards[self.shard_order[self.shard_order_idx]])\n",
    "        self.current_position = (self.bsz * self.seq_len * self.rank) % len(self.tokens)\n",
    "\n",
    "    def reset(self):\n",
    "        self.shard_order_idx = 0\n",
    "        self.tokens = load_tokens(self.shards[self.shard_order[self.shard_order_idx]])\n",
    "        self.current_position = (self.bsz * self.seq_len * self.rank) % len(self.tokens)\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.generator = torch.Generator()\n",
    "        self.generator.manual_seed(epoch)\n",
    "        self.shard_order = torch.randperm(len(self.shards), generator=self.generator).tolist()\n",
    "        self.shard_order_idx = self.rank % len(self.shard_order)\n",
    "        self.tokens = load_tokens(self.shards[self.shard_order[self.shard_order_idx]])\n",
    "        self.current_position = (self.bsz * self.seq_len * self.rank) % len(self.tokens)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_position + (self.bsz * self.seq_len + 1) > len(self.tokens):\n",
    "            # Move to the next shard in the randomized order\n",
    "            self.shard_order_idx = (self.shard_order_idx + 1) % len(self.shard_order)\n",
    "            self.tokens = load_tokens(self.shards[self.shard_order[self.shard_order_idx]])\n",
    "            self.current_position = (self.bsz * self.seq_len * self.rank) % len(self.tokens)\n",
    "\n",
    "        buf = self.tokens[self.current_position : self.current_position + self.bsz * self.seq_len + 1]\n",
    "        x = buf[:-1].view(self.bsz, self.seq_len)\n",
    "        y = buf[1:].view(self.bsz, self.seq_len)\n",
    "\n",
    "        self.current_position += self.bsz * self.seq_len * self.world_size\n",
    "        return x, y.to(torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vuVfkCSOUN6"
   },
   "source": [
    "# Training Loop\n",
    "\n",
    "Basic standalone training loop setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUkPT19yoOZG"
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Distributed setup\n",
    "\n",
    "rank = 0        # This should equal 0 if NOT doing distributed training\n",
    "world_size = 1  # This should equal 1 if NOT doing distributed training\n",
    "\n",
    "# -----------------------\n",
    "# Model configurations\n",
    "\n",
    "dim = 128                # @param {type:\"integer\"}\n",
    "num_heads = 8            # @param {type:\"integer\"}\n",
    "num_layers = 4           # @param {type:\"integer\"}\n",
    "seq_len = 1024           # @param {type:\"integer\"}\n",
    "window_size = 128        # @param {type:\"integer\"}\n",
    "vocab_size = 200064      # This depends on what tokenizer was used on dataset\n",
    "mlp_scale = 4            # @param {type:\"integer\"}\n",
    "weight_tying = True      # @param {type:\"boolean\"}\n",
    "rope_theta = 10000.0     # @param {type:\"number\"}\n",
    "torch_dtype = \"float32\"  # @param {type:\"string\"}\n",
    "bias = False             # @param {type:\"boolean\"}\n",
    "num_eigh = 24            # @param {type:\"integer\"}\n",
    "r = 64                   # @param {type:\"integer\"}\n",
    "use_hankel_L = False     # @param {type:\"boolean\"}\n",
    "use_tensordot = False    # @param {type:\"boolean\"}\n",
    "use_attn = True          # @param {type:\"boolean\"}\n",
    "\n",
    "torch_dtype = getattr(torch, torch_dtype)\n",
    "print(\"Training with datatype:\", torch_dtype)\n",
    "\n",
    "# -----------------------\n",
    "# Training configurations\n",
    "\n",
    "num_epochs = 1           # @param {type:\"integer\"}\n",
    "global_bsz = 524288      # @param {type:\"integer\"}\n",
    "micro_bsz = 1            # @param {type:\"integer\"}\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "log_dir = os.path.join(save_dir, \"log\")\n",
    "checkpoint_dir = os.path.join(save_dir, \"checkpoints\")\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "device_str = \"cuda\" if CUDA_AVAILABLE else \"cpu\"\n",
    "device = torch.device(device_str)\n",
    "\n",
    "# -----------------------\n",
    "# Optimizations\n",
    "torch_compile = False    # @param {type:\"boolean\"}\n",
    "\n",
    "# -----------------------\n",
    "# Dataset configurations\n",
    "\n",
    "dataset = \"/content/drive/MyDrive/fineweb-edu-10B\"  # @param {type:\"string\"}\n",
    "os.makedirs(dataset, exist_ok=True)\n",
    "total_tokens = 10_000_000_000                       # @param {type:\"integer\"}\n",
    "\n",
    "assert (\n",
    "    global_bsz % (micro_bsz * seq_len * world_size) == 0\n",
    "), f\"global_bsz ({global_bsz}) must be divisible by micro_bsz * seq_len * world_size ({micro_bsz * seq_len * world_size}),\"\n",
    "grad_accum_steps = global_bsz // (micro_bsz * seq_len * world_size)\n",
    "\n",
    "# -----------------------\n",
    "# Compute derived parameters\n",
    "\n",
    "num_steps = total_tokens // global_bsz\n",
    "max_steps = num_steps * num_epochs\n",
    "\n",
    "# -----------------------\n",
    "# Training configurations\n",
    "\n",
    "optimizer_name = \"Adagrad\"  # @param {type:\"string\"}\n",
    "eval_period = 50            # @param {type:\"integer\"}\n",
    "save_period = 1000          # @param {type:\"integer\"}\n",
    "max_lr = 3.0e-4             # @param {type:\"number\"}\n",
    "min_lr = 3.0e-5             # @param {type:\"number\"}\n",
    "max_norm = 1.0              # @param {type:\"number\"}\n",
    "warmup_steps = 1907         # @param {type:\"integer\"}\n",
    "\n",
    "optimizer_cls = getattr(torch.optim, optimizer_name, None)\n",
    "if optimizer_cls is None:\n",
    "    raise ValueError(f\"Optimizer {optimizer_cls} is not available in torch.optim\")\n",
    "\n",
    "print(f\"Total (desired) batch size: {global_bsz}\")\n",
    "print(f\"=> Number of gradient accumulation steps: {grad_accum_steps}\")\n",
    "print(f\"\\nTraining for {max_steps} steps\")\n",
    "\n",
    "# -----------------------\n",
    "# Model configuration\n",
    "\n",
    "config = FlashSTUConfig(\n",
    "    bsz=micro_bsz,\n",
    "    dim=dim,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    seq_len=seq_len,\n",
    "    window_size=window_size,\n",
    "    vocab_size=vocab_size,\n",
    "    mlp_scale=mlp_scale,\n",
    "    weight_tying=weight_tying,\n",
    "    bias=bias,\n",
    "    num_eigh=num_eigh,\n",
    "    r=r,\n",
    "    use_hankel_L=use_hankel_L,\n",
    "    use_tensordot=use_tensordot,\n",
    "    use_attn=use_attn,\n",
    "    rope_theta=rope_theta,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=torch.device(device_str),\n",
    ")\n",
    "\n",
    "phi = get_spectral_filters(seq_len, num_eigh, use_hankel_L, device)\n",
    "model = FlashSTU(config, phi)\n",
    "if torch_compile:\n",
    "    if torch.cuda.is_available():\n",
    "        torch.compile(model)\n",
    "        print(f\"Successfully torch.compiled the {model.__class__.__name__} model.\")\n",
    "    else:\n",
    "        print(\"Warning: Torch compiler enabled but no CUDA devices detected.\")\n",
    "model = model.to(device)\n",
    "print(model.eval())\n",
    "\n",
    "optimizer = optimizer_cls(model.parameters(), lr=max_lr)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# -----------------------\n",
    "# Checkpointing\n",
    "\n",
    "# Look for the latest checkpoint file in the checkpoint directory\n",
    "checkpoint_files = glob.glob(os.path.join(checkpoint_dir, \"checkpoint_*.pt\"))\n",
    "if checkpoint_files:\n",
    "    latest_checkpoint = max(checkpoint_files, key=os.path.getmtime)\n",
    "    checkpoint = torch.load(latest_checkpoint)\n",
    "    model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "    start_step = checkpoint[\"step\"]\n",
    "    best_val_loss = checkpoint[\"val_loss\"]\n",
    "    # Use the timestamp from the checkpoint for consistency\n",
    "    timestamp = checkpoint.get(\"timestamp\", time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    log_mode = \"a\"\n",
    "    print(f\"Resumed training from checkpoint '{latest_checkpoint}' at step {start_step}, best validation loss: {best_val_loss:.6f}, timestamp: {timestamp}\")\n",
    "else:\n",
    "    start_step = 0\n",
    "    best_val_loss = float(\"inf\")\n",
    "    log_mode = \"w\"\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_file = os.path.join(log_dir, f\"log_{timestamp}.txt\")\n",
    "\n",
    "with open(log_file, log_mode) as f:\n",
    "    pass\n",
    "\n",
    "# -----------------------\n",
    "# Dataloader setup\n",
    "\n",
    "train_loader = Dataloader(bsz=micro_bsz, seq_len=seq_len, rank=rank, world_size=world_size, dataset=dataset, split=\"train\")\n",
    "val_loader = Dataloader(bsz=micro_bsz, seq_len=seq_len, rank=rank, world_size=world_size, dataset=dataset, split=\"val\")\n",
    "\n",
    "# -----------------------\n",
    "# Training loop\n",
    "\n",
    "for step in range(start_step + 1, max_steps + 1):\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Start epoch\n",
    "    epoch = step // num_steps\n",
    "    last_step = step % num_steps == 0\n",
    "    if step == 1 or step % num_steps == 1:\n",
    "        print(f\"Starting epoch {epoch + 1}...\")\n",
    "        train_loader.set_epoch(epoch)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    # Evaluate on validation set every once in a while\n",
    "    if step == 1 or step % eval_period == 0 or last_step:\n",
    "        print(f\"Evaluating the model at step {step}...\")\n",
    "        val_steps = 20  # Arbitrarily set to reduce long evaluations\n",
    "        model.eval()\n",
    "        val_loader.reset()\n",
    "\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, (val_x, val_y) in zip(range(val_steps), val_loader, strict=False):\n",
    "                val_x, val_y = val_x.to(device), val_y.to(device)\n",
    "                val_preds = model(val_x)\n",
    "                loss = loss_fn(val_preds.flatten(0, 1), val_y.flatten(0, 1))\n",
    "                total_val_loss += loss.detach().float()\n",
    "        avg_val_loss = total_val_loss / val_steps\n",
    "\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(f\"{step} val {avg_val_loss.item()}\\n\")\n",
    "\n",
    "        if step > 0 and (step % save_period == 0 or last_step):\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                print(f\"Validation loss improved from {best_val_loss:.6f} to {avg_val_loss:.6f}!\")\n",
    "                best_val_loss = avg_val_loss\n",
    "                timestamp_short = time.strftime(\"%Y%m%d-%H%M\")\n",
    "                new_checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_{timestamp_short}.pt\")\n",
    "                checkpoint_data = {\n",
    "                    \"model_class\": model.__class__.__name__,\n",
    "                    \"model_config\": config.to_dict(),\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"model_repr\": repr(model),\n",
    "                    \"optimizer_state\": optimizer.state_dict(),\n",
    "                    \"step\": step,\n",
    "                    \"val_loss\": best_val_loss,\n",
    "                    \"timestamp\": timestamp_short,\n",
    "                }\n",
    "                torch.save(checkpoint_data, new_checkpoint_path)\n",
    "                print(f\"Saved checkpoint at step {step} with validation loss: {avg_val_loss:.6f}, timestamp: {timestamp_short}, file: {new_checkpoint_path}\")\n",
    "\n",
    "    # Training step\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for micro_step, (x, y) in zip(range(grad_accum_steps), train_loader, strict=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        preds = model(x)\n",
    "        loss = loss_fn(preds.flatten(0, 1), y.flatten(0, 1))\n",
    "        del preds\n",
    "        loss = loss / grad_accum_steps\n",
    "        train_loss += loss.detach().float()\n",
    "        loss.backward()\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    dt = t1 - t0\n",
    "\n",
    "    # Clip gradients\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "    # Get next learning rate from scheduler\n",
    "    lr = linear_decay_with_warmup(step, warmup_steps, max_steps, max_lr, min_lr)\n",
    "\n",
    "    # Update the learning rate(s)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # Take grad step\n",
    "    try:\n",
    "        optimizer.step()\n",
    "    except KeyError as optim_key_err:\n",
    "        raise RuntimeError(\n",
    "            \"optimizer.step() failed; are you using the same optimizer from the checkpoint?\"\n",
    "        ) from optim_key_err\n",
    "\n",
    "    # Zero out grads for the next forward pass\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # Log step metrics\n",
    "    toks_processed = (\n",
    "        train_loader.bsz * train_loader.seq_len * grad_accum_steps * world_size\n",
    "    )\n",
    "    toks_per_sec = toks_processed / dt\n",
    "    log_message = (\n",
    "        f\"step {step:5d} | \"\n",
    "        f\"loss: {train_loss:.6f} | \"\n",
    "        f\"lr {lr:.4e} | \"\n",
    "        f\"norm: {norm:.4f} | \"\n",
    "        f\"dt: {dt*1000:.2f}ms | \"\n",
    "        f\"tok/s: {toks_per_sec:.2f}\"\n",
    "    )\n",
    "    print(log_message)\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(\n",
    "            f\"{step} train {train_loss:.6f} lr {lr:.4e} norm {norm:.4f} dt {dt*1000:.2f} tok/s {toks_per_sec:.2f}\\n\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vt_Hm7gj75ua"
   },
   "source": [
    "# Running inference\n",
    "\n",
    "As a sanity check, the following cell runs inference on a trained checkpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IctnvjAq7zCt"
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"/content/drive/MyDrive/stu_exps/checkpoints/checkpoint_20250324-0126.pt\"  # @param {type:\"string\"}\n",
    "\n",
    "def load_checkpoint(checkpoint_path: str, device: torch.device):\n",
    "    \"\"\"\n",
    "    Load a checkpoint and rebuild the model from the stored metadata.\n",
    "\n",
    "    Returns:\n",
    "        model, optimizer_state, step, val_loss, timestamp, model_repr\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model_class_name = checkpoint.get(\"model_class\")\n",
    "    model_config_dict = checkpoint.get(\"model_config\")\n",
    "    model_config_dict[\"torch_dtype\"] = getattr(torch, model_config_dict[\"torch_dtype\"])\n",
    "    if model_class_name is None or model_config_dict is None:\n",
    "        raise ValueError(\"Missing required model metadata in checkpoint.\")\n",
    "\n",
    "    # Map stored class name to actual class.\n",
    "    MODEL_MAPPING = {\n",
    "        \"FlashSTU\": FlashSTU,\n",
    "        \"Transformer\": Transformer,\n",
    "    }\n",
    "    if model_class_name not in MODEL_MAPPING:\n",
    "        raise ValueError(f\"Unknown model class: {model_class_name}\")\n",
    "    model_cls = MODEL_MAPPING[model_class_name]\n",
    "\n",
    "    # This depends on which model you trained!\n",
    "    config = FlashSTUConfig(**model_config_dict)\n",
    "\n",
    "    filters = get_spectral_filters(\n",
    "        seq_len=config.seq_len,\n",
    "        k=config.num_eigh,\n",
    "        use_hankel_L=config.use_hankel_L,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Instantiate and load the model.\n",
    "    model = model_cls(config, filters)\n",
    "    model = model.to(device=device, dtype=config.torch_dtype)\n",
    "    model.load_state_dict(checkpoint[\"model_state\"], strict=True)\n",
    "    model.eval()\n",
    "\n",
    "    optimizer_state = checkpoint.get(\"optimizer_state\")\n",
    "    step = checkpoint.get(\"step\")\n",
    "    val_loss = checkpoint.get(\"val_loss\")\n",
    "    timestamp = checkpoint.get(\"timestamp\")\n",
    "    model_repr = checkpoint.get(\"model_repr\")\n",
    "\n",
    "    return model, optimizer_state, step, val_loss, timestamp, model_repr\n",
    "\n",
    "def generate_text(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    num_return_sequences=1,\n",
    "    max_length=512,\n",
    "    device=\"cuda\",\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate text from the given prompt using top-k sampling.\n",
    "\n",
    "    Args:\n",
    "        model: The FlashSTU model instance.\n",
    "        tokenizer: The tokenizer used for encoding/decoding.\n",
    "        prompt (str): Input prompt text.\n",
    "        num_return_sequences (int): How many sequences to return.\n",
    "        max_length (int): Maximum length of generated tokens.\n",
    "        device: torch device.\n",
    "        temperature (float): Sampling temperature. Higher = more random.\n",
    "        top_k (int): Top-K sampling parameter.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of generated text sequences.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Encode prompt tokens.\n",
    "    tokens = torch.tensor(\n",
    "        [tokenizer.encode(prompt, allowed_special={\"<|endoftext|>\"})],\n",
    "        device=device,\n",
    "    )\n",
    "    tokens = tokens.repeat(num_return_sequences, 1)\n",
    "\n",
    "    sample_rng = torch.Generator(device=device)\n",
    "    sample_rng.manual_seed(1746)\n",
    "\n",
    "    eos_token_id = tokenizer.encode(\n",
    "        \"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}\n",
    "    )[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - tokens.size(1)):\n",
    "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                # Fwd pass. Inspect logits here.\n",
    "                logits = model(tokens)     # shape: [batch, seq, vocab]\n",
    "                logits = logits[:, -1, :]  # last token logits\n",
    "\n",
    "                # Apply temperature scaling.\n",
    "                if temperature > 0:\n",
    "                    logits = logits / temperature\n",
    "\n",
    "            # Compute probabilities.\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Top-K sampling.\n",
    "            top_k_probs, top_k_indices = torch.topk(probs, top_k, dim=-1)\n",
    "            ix = torch.multinomial(top_k_probs, 1, generator=sample_rng)\n",
    "            next_token = torch.gather(top_k_indices, -1, ix)\n",
    "\n",
    "            # Append next token.\n",
    "            tokens = torch.cat((tokens, next_token), dim=1)\n",
    "\n",
    "            # Stop if EOS token is generated.\n",
    "            if (next_token == eos_token_id).any():\n",
    "                break\n",
    "\n",
    "    # Decode all sequences.\n",
    "    generated_sequences = []\n",
    "    for i in range(num_return_sequences):\n",
    "        decoded = tokenizer.decode(tokens[i].tolist())\n",
    "        generated_sequences.append(decoded)\n",
    "\n",
    "    return generated_sequences\n",
    "\n",
    "\n",
    "# Load model\n",
    "model, _, _, _, _, _= load_checkpoint(CHECKPOINT_PATH, device)\n",
    "tokenizer = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "# Collect prompt(s) from user.\n",
    "prompts = []\n",
    "PROMPT_ONE = \"Hi, I'm a language model, and\"  # @param {type:\"string\"}\n",
    "PROMPT_TWO = \"The biggest scientific discovery in the 21st century was\"  # @param {type:\"string\"}\n",
    "PROMPT_THREE = \"The capital of France is\"  # @param {type:\"string\"}\n",
    "prompts.extend([PROMPT_ONE, PROMPT_TWO, PROMPT_THREE])\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# BASE SETTINGS:\n",
    "BASE_TEMPERATURE = 0.7  # @param {type:\"number\"}  Increase for more randomness.\n",
    "BASE_TOP_K = 50         # @param {type:\"integer\"} Limit sampling to the top k tokens.\n",
    "MAX_LENGTH = 512        # @param {type:\"integer\"} Maximum number of tokens to generate.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "total_tokens = 0\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    logger.info(f\"Generating text for prompt {i}: {prompt}\")\n",
    "    generated_texts = generate_text(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        num_return_sequences=1,\n",
    "        max_length=MAX_LENGTH,\n",
    "        device=device,\n",
    "        temperature=BASE_TEMPERATURE,\n",
    "        top_k=BASE_TOP_K,\n",
    "    )\n",
    "    for gen_text in generated_texts:\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(f\"Generated Text: {gen_text}\\n\")\n",
    "        total_tokens += len(\n",
    "            tokenizer.encode(gen_text, allowed_special={\"<|endoftext|>\"})\n",
    "        )\n",
    "end_time = time.perf_counter()\n",
    "tokens_per_second = total_tokens / (end_time - start_time)\n",
    "logger.info(f\"Tokens per second: {tokens_per_second:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
