{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\devan\\anaconda3\\envs\\flashstu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to import FlashFFTConv: No module named 'flashfftconv'. Falling back to PyTorch implementation.\n",
      "Unable to import Triton-based MLP: No module named 'liger_kernel'. Falling back to vanilla SwiGLU MLP instead.\n",
      "Unable to import Triton-based RMSNorm: No module named 'liger_kernel'. Falling back to PyTorch implementation.\n",
      "Unable to import Triton-based MLP: No module named 'liger_kernel'. Falling back to vanilla SwiGLU MLP instead.\n",
      "Unable to import Triton-based RMSNorm: No module named 'liger_kernel'. Falling back to PyTorch implementation.\n",
      "Unable to import Triton-based RMSNorm: No module named 'liger_kernel'. Falling back to PyTorch implementation.\n",
      "Using device: cuda\n",
      "Loading the checkpoint...\n",
      "Successfully loaded the checkpoint in 1.93 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "import logging\n",
    "import json\n",
    "from time import time\n",
    "from safetensors import safe_open\n",
    "from model import FlashSTU\n",
    "from config import FlashSTUConfig\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "def get_hankel(seq_len: int, use_hankel_L: bool = False) -> np.ndarray:\n",
    "    entries = np.arange(1, seq_len + 1, dtype=np.float64)\n",
    "    i_plus_j = entries[:, None] + entries[None, :]\n",
    "\n",
    "    if use_hankel_L:\n",
    "        sgn = (-1.0) ** (i_plus_j - 2.0) + 1.0\n",
    "        denom = (i_plus_j + 3.0) * (i_plus_j - 1.0) * (i_plus_j + 1.0)\n",
    "        Z = sgn * (8.0 / denom)\n",
    "    elif not use_hankel_L:\n",
    "        Z = 2.0 / (i_plus_j**3 - i_plus_j)\n",
    "    else:\n",
    "        raise ValueError(\"use_hankel_L must be a boolean\")\n",
    "\n",
    "    return Z\n",
    "\n",
    "def get_spectral_filters(\n",
    "    seq_len: int, \n",
    "    K: int, \n",
    "    use_hankel_L: bool = False, \n",
    "    device: torch.device = None,\n",
    "    dtype: torch.dtype = torch.bfloat16,\n",
    ") -> torch.Tensor:\n",
    "    assert torch.cuda.is_available(), \"CUDA is required.\"\n",
    "    Z = get_hankel(seq_len, use_hankel_L)\n",
    "    sigma, phi = np.linalg.eigh(Z)\n",
    "    sigma_k, phi_k = sigma[-K:], phi[:, -K:]\n",
    "    phi_k *= sigma_k ** 0.25\n",
    "    filters = torch.from_numpy(phi_k)\n",
    "    return filters.to(device=device, dtype=dtype)\n",
    "\n",
    "# Load the checkpoint\n",
    "print(\"Loading the checkpoint...\")\n",
    "start_time = time()\n",
    "state_dict = {}\n",
    "with safe_open(\n",
    "    \"model_19073.safetensors\",\n",
    "    framework=\"pt\",\n",
    "    device=\"cuda\",\n",
    ") as f:\n",
    "    for k in f.keys():\n",
    "        state_dict[k] = f.get_tensor(k)\n",
    "\n",
    "print(f\"Successfully loaded the checkpoint in {time() - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameter Count: 426.28M\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set precision for matrix multiplication\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# Load model configurations from JSON file\n",
    "with open(\"config.json\", \"r\") as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "# Extract model configurations\n",
    "n_embd = config[\"n_embd\"]\n",
    "n_heads = config[\"n_heads\"]\n",
    "n_layers = config[\"n_layers\"]\n",
    "seq_len = config[\"seq_len\"]\n",
    "window_size = config[\"window_size\"]\n",
    "vocab_size = config[\"vocab_size\"]\n",
    "mlp_scale = config[\"mlp_scale\"]\n",
    "bias = config[\"bias\"]\n",
    "dropout = config[\"dropout\"]\n",
    "num_eigh = config[\"num_eigh\"]\n",
    "use_hankel_L = config[\"use_hankel_L\"]\n",
    "use_flash_fft = config[\"use_flash_fft\"]\n",
    "use_approx = config[\"use_approx\"]\n",
    "use_attn = config[\"use_attn\"]\n",
    "softcap = config[\"softcap\"]\n",
    "\n",
    "# Model setup\n",
    "config = FlashSTUConfig(\n",
    "    n_embd=n_embd,\n",
    "    n_heads=n_heads,\n",
    "    n_layers=n_layers,\n",
    "    seq_len=seq_len,\n",
    "    window_size=window_size,\n",
    "    vocab_size=vocab_size,\n",
    "    mlp_scale=mlp_scale,\n",
    "    bias=bias,\n",
    "    dropout=dropout,\n",
    "    num_eigh=num_eigh,\n",
    "    use_hankel_L=use_hankel_L,\n",
    "    use_flash_fft=use_flash_fft,\n",
    "    use_approx=use_approx,\n",
    "    use_attn=use_attn,\n",
    "    softcap=softcap,\n",
    "    torch_dtype=getattr(torch, config[\"torch_dtype\"]),\n",
    ")\n",
    "phi = get_spectral_filters(seq_len, num_eigh, use_hankel_L, device, torch.float32)\n",
    "model = FlashSTU(config, phi)\n",
    "\n",
    "# Load state dictionary into the model\n",
    "model.load_state_dict(state_dict, strict = True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Prepare tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"o200k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "def generate_text(\n",
    "    model, tokenizer, prompt, num_return_sequences=4, max_length=1024, device=\"cuda\", temperature=1.0, top_k=50\n",
    "):\n",
    "    model.eval()\n",
    "    tokens = torch.tensor([tokenizer.encode(prompt, allowed_special={\"<|endoftext|>\"})], device=device)\n",
    "    tokens = tokens.repeat(num_return_sequences, 1)\n",
    "\n",
    "    sample_rng = torch.Generator(device=device)\n",
    "    sample_rng.manual_seed(1337)\n",
    "\n",
    "    eos_token_id = tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm.tqdm(range(max_length - tokens.size(1))):\n",
    "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                logits = model(tokens)\n",
    "                logits = logits[:, -1, :]  # Get logits for the last token\n",
    "\n",
    "                # Apply temperature scaling if temperature > 0\n",
    "                if temperature > 0:\n",
    "                    logits = logits / temperature\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)  # Compute probabilities\n",
    "\n",
    "            # Top-K sampling: set all probabilities outside the top K to 0\n",
    "            top_k_probs, top_k_indices = torch.topk(probs, top_k, dim=-1)\n",
    "            ix = torch.multinomial(top_k_probs, 1, generator=sample_rng)\n",
    "            next_token = torch.gather(top_k_indices, -1, ix)\n",
    "            tokens = torch.cat((tokens, next_token), dim=1)\n",
    "\n",
    "            # Break if EOS token is generated\n",
    "            if (next_token == eos_token_id).any():\n",
    "                break\n",
    "\n",
    "    generated_sequences = []\n",
    "    for i in range(num_return_sequences):\n",
    "        decoded = tokenizer.decode(tokens[i].tolist())\n",
    "        generated_sequences.append(decoded)\n",
    "\n",
    "    return generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating text for prompt: 'The future of artificial intelligence is'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:05<00:00,  8.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: The future of artificial intelligence is also a bit controversial in the political area. The idea of artificial intelligence is actually a part of people who might not have a clear idea of how can we make this technology come to life. People that already have computers can\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"The future of artificial intelligence is\",\n",
    "    # \"In the year 2050, the world will\",\n",
    "    # \"The most important scientific discovery of the 21st century is\",\n",
    "    # \"If I could change one thing about the education system, it would be\",\n",
    "    # \"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nGenerating text for prompt: '{prompt}'\\n\")\n",
    "    generated_texts = generate_text(model, tokenizer, prompt, num_return_sequences=1, max_length=50)\n",
    "    for i, text in enumerate(generated_texts):\n",
    "        print(f\"Sample {i + 1}: {text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from  torch import nn\n",
    "class LDS(nn.Module):\n",
    "    def __init__(self, state_dim, input_dim, output_dim):\n",
    "        super(LDS, self).__init__()\n",
    "        \n",
    "        self.d_out = output_dim\n",
    "\n",
    "        self.h0 = nn.Parameter(torch.randn(state_dim))\n",
    "        # self.A = nn.Parameter(torch.clip(torch.randn(state_dim), -.7, 0.7))\n",
    "        init_A = torch.randn(state_dim)\n",
    "        self.A = nn.Parameter(init_A/torch.max(torch.abs(init_A)))\n",
    "        self.B = nn.Parameter(torch.randn(input_dim, state_dim) / input_dim)\n",
    "        self.C = nn.Parameter(torch.randn(state_dim,output_dim) / state_dim)\n",
    "        self.D = nn.Parameter(torch.randn(input_dim,output_dim) / output_dim) #keep for more complex systems\n",
    "\n",
    "        self.M = nn.Parameter(torch.randn(output_dim, output_dim) / output_dim) #autoregressive\n",
    "    def forward(self, inputs):\n",
    "        bsz, seq_len, _ = inputs.shape\n",
    "        h_t = self.h0.expand(bsz, self.h0.shape[0]).to(device)  # Ensure h0 is on the correct device\n",
    "        outputs = []\n",
    "        A = self.A.flatten()\n",
    "\n",
    "        # Store all intermediate h_t states\n",
    "        all_h_t = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            u_t = inputs[:, t, :]  # Get input for all batches at time t\n",
    "            h_t = A * h_t + u_t @ self.B  # Update hidden states for all batches\n",
    "            all_h_t.append(h_t.unsqueeze(1))  # Store the updated h_t for each time step\n",
    "\n",
    "        # Concatenate all h_t states along the time dimension\n",
    "        all_h_t = torch.cat(all_h_t, dim=1)\n",
    "\n",
    "        # Apply C to all concatenated h_t states at once\n",
    "        outputs = torch.matmul(all_h_t, self.C)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def compute_loss(self, inputs, targets):\n",
    "        mse_loss = nn.MSELoss()\n",
    "        outputs = self(inputs)\n",
    "        return mse_loss(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devan\\AppData\\Local\\Temp\\ipykernel_644\\3562537250.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(files[idx])\n"
     ]
    }
   ],
   "source": [
    "# files  =  [\"./lds_trained/1895_interim_lds_model_and_optimizer.pt\",\n",
    "#            \"./lds_trained/8562_interim_lds_model_and_optimizer.pt\",\n",
    "#            \"./lds_trained/7452_4_20000_0.00306_lds_model_and_optimizer.pt\",\n",
    "#            \"./lds_trained/6353_6_20000_0.00283_lds_model_and_optimizer.pt\",\n",
    "#            \"./lds_trained/8167_10_20000_0.00160_lds_model_and_optimizer.pt\"\n",
    "#            ]\n",
    "\n",
    "# files  =  [\"./lds_trained/4512_0_80000_interim_lds_model_and_optimizer.pt\",\n",
    "#            \"./lds_trained/3353_2_80000_interim_lds_model_and_optimizer.pt\",\n",
    "#            \"./lds_trained/7452_4_20000_0.00306_lds_model_and_optimizer.pt\",\n",
    "#            \"./lds_trained/6353_6_20000_0.00283_lds_model_and_optimizer.pt\",\n",
    "#            \"./lds_trained/8167_10_20000_0.00160_lds_model_and_optimizer.pt\"\n",
    "#            ]\n",
    "\n",
    "\n",
    "files  =  [\"./lds_trained/4512_0_80000_interim_lds_model_and_optimizer.pt\"]\n",
    "\n",
    "lds_layers = {}\n",
    "for idx, layer_idx in enumerate([0]): #], 2, 4, 6, 10]):\n",
    "    lds_layers[layer_idx] = LDS(80000 if idx < 2 else 20000, 768, 768).to(device)\n",
    "    checkpoint = torch.load(files[idx])\n",
    "    lds_layers[layer_idx].load_state_dict(checkpoint[\"lds_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 16:34:14,939 - INFO - Found 1 shards for split val\n"
     ]
    }
   ],
   "source": [
    "from dataloader import DataLoader\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    bsz=1,\n",
    "    seq_len=seq_len, \n",
    "    dataset='./fineweb-edu', \n",
    "    split=\"val\", \n",
    "    main_process=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import autocast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import tqdm\n",
    "\n",
    "def evaluate(model):\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    val_loss = 0.0\n",
    "    torch_dtype = getattr(torch, 'bfloat16')\n",
    "    val_steps = 20 # Arbitrarily set to reduce long evaluations\n",
    "    model.eval()\n",
    "    val_loader.reset()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in tqdm.tqdm(zip(range(val_steps), val_loader, strict=False)):\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            if torch_dtype != torch.float32:\n",
    "                with autocast(device_type=device.type, dtype=torch_dtype, cache_enabled=True):\n",
    "                    preds = model(inputs)\n",
    "            else:\n",
    "                preds = model(inputs)\n",
    "\n",
    "            loss = loss_fn(preds.flatten(0, 1), targets.flatten(0, 1))\n",
    "            loss = loss / val_steps\n",
    "            val_loss += loss.detach().float()\n",
    "    return(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: LDS(), 2: LDS(), 4: LDS(), 6: LDS(), 10: LDS()}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lds_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error between lds_layers[0] and model.layers[0].stu: 7.877873576944694e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import  copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Generate random input\n",
    "random_input = torch.randn(10, 50, n_embd).to(device)\n",
    "stu = copy.deepcopy(model.layers[0].stu)\n",
    "stu.phi = stu.phi.to(torch.bfloat16)\n",
    "# Get the output from lds_layers[0]\n",
    "lds_layers[0].eval()\n",
    "with torch.no_grad():\n",
    "    lds_output = lds_layers[0](random_input)\n",
    "\n",
    "# Get the output from model.layers[0].stu\n",
    "model.layers[0].stu.eval()\n",
    "with torch.no_grad():\n",
    "    stu_output = stu(random_input.to(torch.bfloat16))\n",
    "\n",
    "# Compute the error\n",
    "error = F.mse_loss(lds_output, stu_output)\n",
    "print(f\"Error between lds_layers[0] and model.layers[0].stu: {error.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [08:38, 25.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.08203125\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "model2 = copy.deepcopy(model)\n",
    "\n",
    "for idx in [0]:\n",
    "    lds_layers[idx].eval()\n",
    "# Substitute LDS models into model2\n",
    "model2.layers[0].stu = lds_layers[0]\n",
    "# model2.layers[2].stu = lds_layers[2]\n",
    "# model2.layers[4].stu = lds_layers[4]\n",
    "# model2.layers[6].stu = lds_layers[6]\n",
    "# model2.layers[8].stu = Identity()\n",
    "# model2.layers[10].stu = lds_layers[10]\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "val_loss = evaluate(model2) #baseline of 3.5\n",
    "print(f\"Validation Loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating text for prompt: 'The future of artificial intelligence is'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:01<00:00, 24.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: The future of artificial intelligence is already in my thoughts and I cannot write much more than that. By contrast, the artificial intelligence today would be able to do things like the Internet at the speed in a matter of seconds. Therein 1 day.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"The future of artificial intelligence is\",\n",
    "    # \"In the year 2050, the world will\",\n",
    "    # \"The most important scientific discovery of the 21st century is\",\n",
    "    # \"If I could change one thing about the education system, it would be\",\n",
    "    # \"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nGenerating text for prompt: '{prompt}'\\n\")\n",
    "    generated_texts = generate_text(model2, tokenizer, prompt, num_return_sequences=1, max_length=50)\n",
    "    for i, text in enumerate(generated_texts):\n",
    "        print(f\"Sample {i + 1}: {text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mbfloat16), targets\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(device_type\u001b[38;5;241m=\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(torch, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbfloat16\u001b[39m\u001b[38;5;124m'\u001b[39m), cache_enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Forward pass through the layers up to layer 2\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtok_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[0;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m](x)\n",
      "File \u001b[1;32mc:\\Users\\devan\\anaconda3\\envs\\flashstu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\devan\\anaconda3\\envs\\flashstu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\devan\\anaconda3\\envs\\flashstu\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\devan\\anaconda3\\envs\\flashstu\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "mean_list = []\n",
    "var_list = []\n",
    "\n",
    "model.eval()\n",
    "val_loader.reset()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm.tqdm(val_loader):\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device).to(torch.bfloat16), targets.to(device).to(torch.bfloat16)\n",
    "        with autocast(device_type=device.type, dtype=getattr(torch, 'bfloat16'), cache_enabled=True):\n",
    "        # Forward pass through the layers up to layer 2\n",
    "            x = model.tok_emb(inputs)\n",
    "            x = model.dropout(x)\n",
    "            x = model.layers[0](x)\n",
    "            x = model.layers[1](x)\n",
    "        \n",
    "        # Collect inputs to STU layer 2\n",
    "        stu_inputs = x.clone().detach().cpu().numpy()\n",
    "        mean_list.append(np.mean(stu_inputs))\n",
    "        var_list.append(np.var(stu_inputs))\n",
    "\n",
    "mean_stu_layer_2 = np.mean(mean_list)\n",
    "var_stu_layer_2 = np.mean(var_list)\n",
    "\n",
    "print(f\"Mean of inputs to STU layer 2: {mean_stu_layer_2}\")\n",
    "print(f\"Variance of inputs to STU layer 2: {var_stu_layer_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
