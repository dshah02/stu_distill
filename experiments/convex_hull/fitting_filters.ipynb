{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../../src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to import FlashFFTConv: No module named 'flashfftconv'. Falling back to PyTorch implementation.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from stu import STU\n",
    "import time\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lds import LDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hankel(seq_len: int, use_hankel_L: bool = False) -> np.ndarray:\n",
    "    entries = np.arange(1, seq_len + 1, dtype=np.float64)\n",
    "    i_plus_j = entries[:, None] + entries[None, :]\n",
    "\n",
    "    if use_hankel_L:\n",
    "        sgn = (-1.0) ** (i_plus_j - 2.0) + 1.0\n",
    "        denom = (i_plus_j + 3.0) * (i_plus_j - 1.0) * (i_plus_j + 1.0)\n",
    "        Z = sgn * (8.0 / denom)\n",
    "    elif not use_hankel_L:\n",
    "        Z = 2.0 / (i_plus_j**3 - i_plus_j)\n",
    "    else:\n",
    "        raise ValueError(\"use_hankel_L must be a boolean\")\n",
    "\n",
    "    return Z\n",
    "\n",
    "def get_spectral_filters(\n",
    "    seq_len: int,\n",
    "    K: int,\n",
    "    use_hankel_L: bool = False,\n",
    "    device: torch.device = None,\n",
    "    dtype: torch.dtype = torch.bfloat16,\n",
    ") -> torch.Tensor:\n",
    "    # assert torch.cuda.is_available(), \"CUDA is required.\"\n",
    "    Z = get_hankel(seq_len, use_hankel_L)\n",
    "    sigma, phi = np.linalg.eigh(Z)\n",
    "    sigma_k, phi_k = sigma[-K:], phi[:, -K:]\n",
    "    phi_k *= sigma_k ** 0.25\n",
    "    filters = torch.from_numpy(phi_k)\n",
    "    return filters.to(device=device, dtype=dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_i = 0\n",
    "state_dim = 10000\n",
    "batch_size = 2\n",
    "epochs = 4000\n",
    "seq_len = 512\n",
    "kx = 5\n",
    "lr = 0.0001\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seq_len = 4096\n",
    "num_eigh = 30\n",
    "use_hankel_L  = True\n",
    "phi= get_spectral_filters(seq_len = seq_len, K = num_eigh,  use_hankel_L= use_hankel_L,\n",
    "                                device  = device,  dtype = torch.float32)\n",
    "\n",
    "stu_config = {\n",
    "    \"num_eigh\": num_eigh,\n",
    "    \"use_hankel_L\": True,\n",
    "    \"torch_dtype\": torch.float32,\n",
    "    \"d_in\": 1,\n",
    "    \"d_out\": 1,\n",
    "    \"seq_len\": seq_len,\n",
    "    \"k_u\": 0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LDS model\n",
    "lds = LDS(state_dim, 768, 768, kx).to(device)\n",
    "optimizer = torch.optim.Adam(lds.parameters(), lr=lr)\n",
    "\n",
    "# Training\n",
    "lds_loss_values = []\n",
    "\n",
    "best_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basic_stu import STU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_0 = STU(stu_config, phi).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_0.M_phi_minus.data = torch.zeros_like(phi_0.M_phi_minus)\n",
    "phi_0.M_phi_plus.data = torch.zeros_like(phi_0.M_phi_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_0.M_phi_plus[0][0][0].data = torch.tensor(1.0, requires_grad= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/flashstu/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([2, 4096, 1])) that is different to the input size (torch.Size([2, 4096, 768])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.006508288439363241\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    inputs = torch.randn(batch_size, seq_len, 768).to(device).to(torch.bfloat16)\n",
    "    stu_outputs = phi_0(inputs).to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = lds.compute_loss(inputs.to(stu_outputs.dtype), stu_outputs)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(lds.parameters(), max_norm=1)\n",
    "    lds_loss_values.append(loss.item())\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        lds.A.data.clamp_(max=1, min=-1)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flashstu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
