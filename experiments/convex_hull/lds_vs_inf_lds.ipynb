{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "57286df0-cd3d-4abc-b9b8-d7b3ff7e8560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-by-step generation and comparison:\n",
      "Step 0 | y_infer: tensor([[-1.3543]], grad_fn=<AddBackward0>) \n",
      "         y_base: tensor([[-1.3543]], grad_fn=<SliceBackward0>) \n",
      "         max diff = 0.0000e+00\n",
      "Step 1 | y_infer: tensor([[-1.1029]], grad_fn=<AddBackward0>) \n",
      "         y_base: tensor([[-1.1029]], grad_fn=<SliceBackward0>) \n",
      "         max diff = 2.3842e-07\n",
      "Step 2 | y_infer: tensor([[1.6247]], grad_fn=<AddBackward0>) \n",
      "         y_base: tensor([[1.6247]], grad_fn=<SliceBackward0>) \n",
      "         max diff = 0.0000e+00\n",
      "Step 3 | y_infer: tensor([[-0.9242]], grad_fn=<AddBackward0>) \n",
      "         y_base: tensor([[-0.9242]], grad_fn=<SliceBackward0>) \n",
      "         max diff = 0.0000e+00\n",
      "Step 4 | y_infer: tensor([[-2.6577]], grad_fn=<AddBackward0>) \n",
      "         y_base: tensor([[-2.6577]], grad_fn=<SliceBackward0>) \n",
      "         max diff = 0.0000e+00\n",
      "Step 5 | y_infer: tensor([[2.0841]], grad_fn=<AddBackward0>) \n",
      "         y_base: tensor([[2.0841]], grad_fn=<SliceBackward0>) \n",
      "         max diff = 0.0000e+00\n",
      "Step 6 | y_infer: tensor([[0.6803]], grad_fn=<AddBackward0>) \n",
      "         y_base: tensor([[0.6803]], grad_fn=<SliceBackward0>) \n",
      "         max diff = 2.3842e-07\n",
      "Step 7 | y_infer: tensor([[-4.6943]], grad_fn=<AddBackward0>) \n",
      "         y_base: tensor([[-4.6943]], grad_fn=<SliceBackward0>) \n",
      "         max diff = 4.7684e-07\n",
      "Step 8 | y_infer: tensor([[1.3302]], grad_fn=<AddBackward0>) \n",
      "         y_base: tensor([[1.3302]], grad_fn=<SliceBackward0>) \n",
      "         max diff = 4.7684e-07\n",
      "Step 9 | y_infer: tensor([[3.8402]], grad_fn=<AddBackward0>) \n",
      "         y_base: tensor([[3.8402]], grad_fn=<SliceBackward0>) \n",
      "         max diff = 0.0000e+00\n",
      "\n",
      "Timing Summary:\n",
      "Total time for inference model over 10 steps: 0.001639 seconds\n",
      "Total time for base model over 10 steps: 0.005290 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Adjust the path to locate your modules.\n",
    "sys.path.append(os.path.abspath(\"../../src\"))\n",
    "\n",
    "from lds import LDS as LDSBase\n",
    "from inference_lds import LDS as LDSInference\n",
    "\n",
    "def initialize_params(model):\n",
    "    \"\"\"\n",
    "    Initialize model parameters to nonzero values.\n",
    "    Here we simply initialize each parameter with a random normal distribution.\n",
    "    \"\"\"\n",
    "    model.A.data.copy_(torch.randn_like(model.A))\n",
    "    model.B.data.copy_(torch.randn_like(model.B))\n",
    "    model.C.data.copy_(torch.randn_like(model.C))\n",
    "    model.M.data.copy_(torch.randn_like(model.M))\n",
    "    model.h0.data.copy_(torch.randn_like(model.h0))\n",
    "\n",
    "def test_regressive_equivalence():\n",
    "    torch.manual_seed(55)\n",
    "\n",
    "    # Hyperparameters.\n",
    "    state_dim = 4\n",
    "    input_dim = 1    # Must equal output_dim for autoregressive generation (output fed as next input)\n",
    "    output_dim = 1\n",
    "    kx = 1         # AR order (number of taps)\n",
    "    steps = 10      # Number of autoregressive generation steps.\n",
    "    bsz = 1        # Batch size.\n",
    "\n",
    "    # Instantiate models.\n",
    "    # LDSInference is optimized for iterative generation.\n",
    "    # LDSBase computes a full forward pass over the entire sequence.\n",
    "    model_infer = LDSInference(state_dim, input_dim, output_dim, kx=kx)\n",
    "    model_base = LDSBase(state_dim, input_dim, output_dim, kx=kx)\n",
    "\n",
    "    # Initialize parameters on the inference model.\n",
    "    initialize_params(model_infer)\n",
    "    \n",
    "    # Force both models to have identical parameters.\n",
    "    model_base.A.data = model_infer.A.data.clone()\n",
    "    model_base.B.data = model_infer.B.data.clone()\n",
    "    model_base.C.data = model_infer.C.data.clone()\n",
    "    model_base.M.data = model_infer.M.data.clone()\n",
    "    model_base.h0.data = model_infer.h0.data.clone()\n",
    "\n",
    "    # Reset the inference model's state.\n",
    "    model_infer.reset_state(batch_size=bsz)\n",
    "\n",
    "    # Create a growing input sequence for the base model.\n",
    "    x_init = torch.randn(bsz, input_dim)\n",
    "    inputs_base = x_init.unsqueeze(1)  # Shape: [bsz, 1, input_dim]\n",
    "\n",
    "    inference_time_total = 0.0\n",
    "    base_time_total = 0.0\n",
    "\n",
    "    print(\"Step-by-step generation and comparison:\")\n",
    "\n",
    "    for t in range(steps):\n",
    "        # Measure inference model time.\n",
    "        start_infer = time.perf_counter()\n",
    "        y_t_infer = model_infer.next_step(x_init)  # Shape: [bsz, output_dim]\n",
    "        end_infer = time.perf_counter()\n",
    "        inference_time_total += (end_infer - start_infer)\n",
    "\n",
    "        # Measure base model time.\n",
    "        start_base = time.perf_counter()\n",
    "        y_base_all = model_base(inputs_base)\n",
    "        y_t_base = y_base_all[:, -1, :]  # Extract the last output.\n",
    "        end_base = time.perf_counter()\n",
    "        base_time_total += (end_base - start_base)\n",
    "\n",
    "        # Compare the outputs.\n",
    "        diff = (y_t_infer - y_t_base).abs().max().item()\n",
    "        print(f\"Step {t} | y_infer: {y_t_infer} \\n         y_base: {y_t_base} \\n         max diff = {diff:.4e}\")\n",
    "\n",
    "        # Append the new output to inputs_base for the next iteration.\n",
    "        inputs_base = torch.cat([inputs_base, y_t_infer.unsqueeze(1)], dim=1)\n",
    "        # Use the new inference output as the next input.\n",
    "        x_init = y_t_infer\n",
    "\n",
    "    print(\"\\nTiming Summary:\")\n",
    "    print(f\"Total time for inference model over {steps} steps: {inference_time_total:.6f} seconds\")\n",
    "    print(f\"Total time for base model over {steps} steps: {base_time_total:.6f} seconds\")\n",
    "\n",
    "\n",
    "test_regressive_equivalence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6a04708f-4f2d-4313-bc46-023480cf0766",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 100\n",
    "input_dim = 1    # Must equal output_dim for autoregressive generation (output fed as next input)\n",
    "output_dim = 1\n",
    "kx = 1         # AR order (number of taps)\n",
    "seq_len = 100      # Number of autoregressive generation steps.\n",
    "bsz = 1 \n",
    "\n",
    "model_base = LDSBase(state_dim, input_dim, output_dim, kx=kx)\n",
    "model_infer = LDSInference(state_dim, input_dim, output_dim, kx=kx)\n",
    "\n",
    "\n",
    "# Initialize parameters on the inference model.\n",
    "initialize_params(model_infer)\n",
    "\n",
    "# M = torch.zeros_like(model_base.M.data)\n",
    "# model_base.M.data = M\n",
    "# Force both models to have identical parameters.\n",
    "model_infer.A.data = model_base.A.data.clone()\n",
    "model_infer.B.data = model_base.B.data.clone()\n",
    "model_infer.C.data = model_base.C.data.clone()\n",
    "model_infer.M.data = model_base.M.data.clone()\n",
    "model_infer.h0.data = model_base.h0.data.clone()\n",
    "\n",
    "# Reset the inference model's state.\n",
    "model_infer.reset_state(batch_size=bsz)\n",
    "\n",
    "# Create a growing input sequence for the base model.\n",
    "x_init = torch.randn(bsz, seq_len ,input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "62885f8e-a5a1-4cfa-8b51-03020e333e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_out = model_infer(x_init)\n",
    "lds_out = model_base(x_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a4f69a4a-a246-4c49-ab39-d206bd89dd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.6340e-16, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "F.mse_loss(inf_out, lds_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2a12df",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d9ac477e-8728-4988-a98b-840185a0fe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_infer.reset_state(1)\n",
    "outputs = []\n",
    "for t in range(seq_len):\n",
    "    x_t = x_init[:, t, :]\n",
    "    y_t = model_infer.next_step(x_t)\n",
    "    outputs.append(y_t.unsqueeze(1))\n",
    "y_inf_out = torch.cat(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a26b65a7-5f13-46de-96d0-ab0520f6078f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.6340e-16, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(y_inf_out, lds_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cda214-f867-45e8-8d6f-1d356f7fd106",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env [~/.conda/envs/torch-env/]",
   "language": "python",
   "name": "conda_torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
