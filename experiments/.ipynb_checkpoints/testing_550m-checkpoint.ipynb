{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f9aec6-9c5f-4963-9c2f-07bedf39d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64cf4453-1788-444c-b270-de42224f90d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ds6237/.conda/envs/torch-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Validation loss: 2.8789, trained on ~59.7B FineWeb-Edu tokens.\n",
    "This is a base model and can frequently hallucinate.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "from torch import nn\n",
    "\n",
    "from safetensors import safe_open\n",
    "\n",
    "from model_550m import FlashSTU, FlashSTUConfig, get_spectral_filters\n",
    "\n",
    "\n",
    "CHECKPOINT_PATH = \"../models/model_step-114000.safetensors\"\n",
    "CONFIG_PATH = \"../models/config_2-7b.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bbcc1b9-4ebe-4ac9-a3ce-79dca34de58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_compile(model: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Apply torch.compile to each layer. This makes compilation efficient\n",
    "    due to repeated structure. Alternatively, one can just compile the whole model.\n",
    "    \"\"\"\n",
    "    print(f\"Compiling each {model.__class__.__name__} layer with torch.compile...\")\n",
    "    start = time.perf_counter()\n",
    "    for idx, layer in model.layers.named_children():\n",
    "        compiled_layer = torch.compile(layer, mode=\"max-autotune\", fullgraph=True)\n",
    "        model.layers.register_module(idx, compiled_layer)\n",
    "    end = time.perf_counter()\n",
    "    print(f\"Finished compiling each {model.__class__.__name__} layer in {end - start:.4f} seconds.\")\n",
    "\n",
    "\n",
    "def load_stu_model(config_path: str, checkpoint_path: str, device: torch.device):\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config_data = json.load(f)\n",
    "\n",
    "    dim = config_data[\"dim\"]\n",
    "    num_heads = config_data[\"num_heads\"]\n",
    "    num_layers = config_data[\"num_layers\"]\n",
    "    num_eigh = config_data[\"num_eigh\"]\n",
    "    seq_len = config_data[\"seq_len\"]\n",
    "    use_hankel_L = config_data[\"use_hankel_L\"]\n",
    "    window_size = config_data[\"window_size\"]\n",
    "    vocab_size = config_data[\"vocab_size\"]\n",
    "    mlp_scale = config_data[\"mlp_scale\"]\n",
    "    bias = config_data[\"bias\"]\n",
    "    dropout = config_data[\"dropout\"]\n",
    "    softcap = config_data[\"softcap\"]\n",
    "    theta = config_data[\"theta\"]\n",
    "    torch_compile = config_data[\"torch_compile\"]\n",
    "    torch_dtype = getattr(torch, config_data[\"torch_dtype\"])\n",
    "\n",
    "    model_config = FlashSTUConfig(\n",
    "        dim=dim,\n",
    "        num_heads=num_heads,\n",
    "        num_layers=num_layers,\n",
    "        seq_len=seq_len,\n",
    "        window_size=window_size,\n",
    "        vocab_size=vocab_size,\n",
    "        mlp_scale=mlp_scale,\n",
    "        bias=bias,\n",
    "        dropout=dropout,\n",
    "        softcap=softcap,\n",
    "        theta=theta,\n",
    "        torch_dtype=torch_dtype,\n",
    "    )\n",
    "\n",
    "    spectral_filters = get_spectral_filters(seq_len, num_eigh, use_hankel_L, device, torch_dtype)\n",
    "    model = FlashSTU(model_config, spectral_filters)\n",
    "    model = model.to(device=device, dtype=torch_dtype)\n",
    "\n",
    "    print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "    state_dict = {}\n",
    "    start_time = time()\n",
    "\n",
    "    if checkpoint_path.endswith(\".safetensors\"):\n",
    "        with safe_open(checkpoint_path, framework=\"pt\", device=device.type) as f:\n",
    "            for k in f.keys():\n",
    "                state_dict[k] = f.get_tensor(k)\n",
    "    elif checkpoint_path.endswith(\".pt\"):\n",
    "        state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported checkpoint format: {checkpoint_path}\")\n",
    "    print(f\"Checkpoint loaded in {time() - start_time:.2f} seconds.\")\n",
    "\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    print(\"Model weights loaded successfully!\")\n",
    "\n",
    "    if torch_compile:\n",
    "        model = apply_compile(model)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return model, config_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43c051ad-0d01-4b01-9715-f5f6b3959d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    num_return_sequences=1,\n",
    "    max_length=512,\n",
    "    device=\"cuda\",\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate text from the given prompt using top-k sampling.\n",
    "\n",
    "    Args:\n",
    "        model: The FlashSTU model instance.\n",
    "        tokenizer: The tokenizer used for encoding/decoding.\n",
    "        prompt (str): Input prompt text.\n",
    "        num_return_sequences (int): How many sequences to return.\n",
    "        max_length (int): Maximum length of generated tokens.\n",
    "        device: torch device.\n",
    "        temperature (float): Sampling temperature. Higher = more random.\n",
    "        top_k (int): Top-K sampling parameter.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of generated text sequences.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Encode prompt tokens.\n",
    "    tokens = torch.tensor(\n",
    "        [tokenizer.encode(prompt, allowed_special={\"<|endoftext|>\"})],\n",
    "        device=device,\n",
    "    )\n",
    "    seq_len = tokens.shape[1]\n",
    "    tokens = tokens.repeat(num_return_sequences, 1)\n",
    "    \n",
    "    input_pos = torch.arange(seq_len, device=device)\n",
    "\n",
    "    \n",
    "    sample_rng = torch.Generator(device=device)\n",
    "    sample_rng.manual_seed(1746)\n",
    "\n",
    "    eos_token_id = tokenizer.encode(\n",
    "        \"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}\n",
    "    )[0]\n",
    "    cur_token = seq_len\n",
    "    with torch.no_grad():\n",
    "        for idx in range(max_length - tokens.size(1)):\n",
    "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                # Fwd pass. Inspect logits here.\n",
    "                if idx != 0:\n",
    "                    logits = model(tokens[:, -1:], input_pos = input_pos)     # shape: [batch, 1, vocab]\n",
    "                else:\n",
    "                    logits = model(tokens, input_pos = input_pos)     # shape: [batch, seq, vocab]\n",
    "                logits = logits[:, -1, :]  # last token logits\n",
    "\n",
    "                # Apply temperature scaling.\n",
    "                if temperature > 0:\n",
    "                    logits = logits / temperature\n",
    "\n",
    "            # Compute probabilities.\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Top-K sampling.\n",
    "            top_k_probs, top_k_indices = torch.topk(probs, top_k, dim=-1)\n",
    "            ix = torch.multinomial(top_k_probs, 1, generator=sample_rng)\n",
    "            next_token = torch.gather(top_k_indices, -1, ix)\n",
    "\n",
    "            # Append next token.\n",
    "            tokens = torch.cat((tokens, next_token), dim=1)\n",
    "            input_pos = torch.tensor([cur_token]).to(device)\n",
    "            cur_token +=1 \n",
    "            # Stop if EOS token is generated.\n",
    "            if (next_token == eos_token_id).any():\n",
    "                break\n",
    "\n",
    "    # Decode all sequences.\n",
    "    generated_sequences = []\n",
    "    for i in range(num_return_sequences):\n",
    "        decoded = tokenizer.decode(tokens[i].tolist())\n",
    "        generated_sequences.append(decoded)\n",
    "\n",
    "    return generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c729eacd-b0f4-4464-b578-897dc212ca38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tiktoken.load import load_tiktoken_bpe\n",
    "bpe_path = \"../models/o200k_base.tiktoken\"\n",
    "bpe_dict = load_tiktoken_bpe(bpe_path)\n",
    "\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    name=\"o200k_base\",  # Name of the encoding\n",
    "    pat_str=r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+\"\"\",\n",
    "    mergeable_ranks=bpe_dict,\n",
    "    special_tokens={\n",
    "        \"<|endoftext|>\": 199999,  # Custom special token example (modify as needed)\n",
    "        \"<|endofprompt|>\": 200018,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88070375-8af1-4a23-90b0-d0f70644c191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameter Count: 550.31M\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ds6237/.conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1326: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at /opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/Copy.cpp:308.)\n",
      "  return t.to(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from ../models/model_step-114000.safetensors...\n",
      "Checkpoint loaded in 0.24 seconds.\n",
      "Model weights loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Load model and config.\n",
    "model, config_data = load_stu_model(CONFIG_PATH, CHECKPOINT_PATH, device)\n",
    "# tokenizer = tiktoken.get_encoding(\"o200k_base\") #need to cache this for offline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d95e05c-8320-4061-85c8-3a62b9fee437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (press enter with no text to finish):  hi all\n",
      "Enter a prompt (press enter with no text to finish):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for prompt 1: hi all\n",
      "\n",
      "Prompt: hi all\n",
      "Generated Text: hi all these times.\n",
      "A. From the first few days of their stay in the village the little children were very happy. They had a wonderful time playing with their toys, and listening to their mother and father read the stories. The little children started to ask questions in the village about everything they did in the village. The little children could not understand what they were doing at first. But the little children were very happy to talk about their thoughts and feelings.\n",
      "B. From the second day onwards, the little children were very happy to read, write and talk about their thoughts and feelings. They were very happy to play with their toys and talk to their mother and father. They were very happy to play with their friends. But the little children could not understand why the little children could not understand what they were doing at first.\n",
      "C. From the third day onwards, the little children were very happy to talk about their thoughts and feelings. They were very happy to talk to their mother and father. They were very happy to read their stories and discuss with the little children. They were very happy to talk with their friends.\n",
      "D. From the fourth day onwards, the little children were very happy to talk about their thoughts and feelings. They were very happy to talk to their friends. They were very happy to talk with their friends. They were very happy to talk with their friends.\n",
      "E. From the fifth day onwards, the little children were very happy to talk about their thoughts and feelings. They were very happy to talk with their friends. They were very happy to talk to their friends.\n",
      "4. From the sixth day onwards, the little children were very happy to talk about their thoughts and feelings. They were very happy to talk with their friends. They were very happy to talk with their friends.\n",
      "5. From the seventh day onwards, the little children were very happy to talk about their thoughts and feelings. They were very happy to talk with their friends. They were very happy to talk with their friends. They were very happy to talk with their friends.\n",
      "6. From the eighth day onwards, the little children were very happy to talk about their thoughts and feelings. They were very happy to talk with their friends. They were very happy to talk with their friends. They were very happy to talk with their friends.\n",
      "7. From the ninth day onwards, the little children were very happy to talk about their thoughts and feelings. They were very happy to talk with their friends. They were very\n",
      "\n",
      "Tokens per second: 70.35\n"
     ]
    }
   ],
   "source": [
    "# Collect prompt(s) from user.\n",
    "prompts = []\n",
    "while True:\n",
    "    prompt = input(\"Enter a prompt (press enter with no text to finish): \")\n",
    "    if not prompt:\n",
    "        break\n",
    "    prompts.append(prompt.strip())\n",
    "\n",
    "    if len(prompts) == 0:\n",
    "        print(\"No prompts provided. Exiting.\")\n",
    "        break\n",
    "    \n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# BASE SETTINGS:\n",
    "BASE_TEMPERATURE = 0.7  # Increase for more randomness.\n",
    "BASE_TOP_K = 50         # Limit sampling to the top k tokens.\n",
    "MAX_LENGTH = 500        # Maximum number of tokens to generate.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "total_tokens = 0\n",
    "start_time = time()\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"Generating text for prompt {i}: {prompt}\")\n",
    "    model.setup_caches(batch_size = 1)\n",
    "    generated_texts = generate_text(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        num_return_sequences=1,\n",
    "        max_length=MAX_LENGTH,\n",
    "        device=device,\n",
    "        temperature=BASE_TEMPERATURE,\n",
    "        top_k=BASE_TOP_K,\n",
    "    )\n",
    "    for gen_text in generated_texts:\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(f\"Generated Text: {gen_text}\\n\")\n",
    "        total_tokens += len(\n",
    "            tokenizer.encode(gen_text, allowed_special={\"<|endoftext|>\"})\n",
    "        )\n",
    "end_time = time()\n",
    "tokens_per_second = total_tokens / (end_time - start_time)\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "629d4aa7-0948-45c3-856d-1b9ebc29a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33266bc3-0590-48cc-8d8b-5ac0c3e046cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(\"./convex_hull\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37089918-7635-401a-9e9e-21f56d9650e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 03:24:50,607 - INFO - Found 1 shards for split val\n"
     ]
    }
   ],
   "source": [
    "from torch.amp import autocast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import tqdm \n",
    "\n",
    "from dataloader import DataLoader\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    bsz=1,\n",
    "    seq_len= 128, #USUALLY 512\n",
    "    dataset='../fineweb-edu', \n",
    "    split=\"val\", \n",
    "    main_process=True,\n",
    ")\n",
    "\n",
    "def evaluate(model, val_steps = 5):\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    val_loss = 0.0\n",
    "    torch_dtype = getattr(torch, 'bfloat16')\n",
    "    # val_steps = 5 # Abitrarily set to reduce long evaluations, >20 typically used\n",
    "    model.eval()\n",
    "    val_loader.reset()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in tqdm.tqdm(zip(range(val_steps), val_loader, strict=False)):\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            if torch_dtype != torch.float32:\n",
    "                with autocast(device_type=device.type, dtype=torch_dtype, cache_enabled=True):\n",
    "                    preds = model(inputs)\n",
    "            else:\n",
    "                preds = model(inputs)\n",
    "\n",
    "            loss = loss_fn(preds.flatten(0, 1), targets.flatten(0, 1))\n",
    "            loss = loss / val_steps\n",
    "            val_loss += loss.detach().float()\n",
    "    return(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc6ae288-0ee5-4719-8fd4-1a10c1f5f5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_caches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb66849b-1e17-4bef-a699-c5bf73eba431",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:04, 11.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.3545, device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, val_steps = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66ab57d6-201a-4e2b-8005-101a47b8c2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ds6237/stu_distill/experiments/convex_hull/full_fast_stu.py:110: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "from full_fast_stu import FullFastSTU\n",
    "\n",
    "for idx in range(0, 12, 2):\n",
    "    stu_layer = copy.deepcopy(model.layers[idx].stu)\n",
    "    fast_layer = FullFastSTU(stu_layer.cuda(), \"../experiments/convex_hull/best_phi_lds.pt\").cuda()\n",
    "    model.layers[idx].stu = fast_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb820d0a-06ea-40ea-bfe7-f630ff4c8086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for prompt 1: hi all\n",
      "\n",
      "Prompt: hi all\n",
      "Generated Text: hi all had the power to have shown the power to the power from the power to meet their needs of the need of the world. In the greatest danger of the need for the ability to the possibility that the fact that the world and to be 12 hours until their own company. They can be used to the U.S. The Sun's effect of the\n",
      "-using a 10% of the people of the same way for the whole of the 5, the amount of the 12 months ago that the same amount of the right to find the use of the same thing to the same as the possibility of a certain aspects of 2. The World Bank of 2001, and to the same time the first five to the same. The United Nations and, which they have to the same as well known as the need to the same period of the first step to ensure that the idea of the two types of the same thing that they were in the need to the same.\n",
      "There is the use of the first and the same for some other animals to the same is not be a totality in the fact that they came to the state.\n",
      "The most of the same time of the most of the right to the same, it to the best possible.\n",
      "|8. The American Academy of a good, and 12 years. It is not only in the first, and that the 4, and it is not the \"anxiety and the same time, in the number of the use of these two different from the same as it as an important to be an easy to have received a combination of the two kinds of the best way.\n",
      "The last few months, to the state of the same in the same. In the same as a strong, and the first of the ability to the time, and the same, and the term \"the other aspects of the right to be a variety of the original version of the most of the world. That is 2, and in the state that is the same as the same, with several times that the same as the most important to the first year 2. The National Geographic Information Technology. The Sun, and other countries of the following year. What is to be a very long-term\n",
      "-which we can be the same day, but the \"to the name of the second order to the first part of many years, and will help them to the state of the process of that they had to the largest of an important\n",
      "\n",
      "Tokens per second: 43.40\n"
     ]
    }
   ],
   "source": [
    "total_tokens = 0\n",
    "start_time = time()\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"Generating text for prompt {i}: {prompt}\")\n",
    "    model.setup_caches(batch_size = 1)\n",
    "    generated_texts = generate_text(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        num_return_sequences=1,\n",
    "        max_length=MAX_LENGTH,\n",
    "        device=device,\n",
    "        temperature=BASE_TEMPERATURE,\n",
    "        top_k=BASE_TOP_K,\n",
    "    )\n",
    "    for gen_text in generated_texts:\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(f\"Generated Text: {gen_text}\\n\")\n",
    "        total_tokens += len(\n",
    "            tokenizer.encode(gen_text, allowed_special={\"<|endoftext|>\"})\n",
    "        )\n",
    "end_time = time()\n",
    "tokens_per_second = total_tokens / (end_time - start_time)\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ded471-5123-423e-99fb-0dbe89f5db7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "443a9501-7811-4789-87d4-139467da9ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [02:47,  3.34s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.3530, device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, val_steps = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c4ebacf-7a15-48d9-acf8-ecbc694b3b6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LDS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m LDS\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LDS' is not defined"
     ]
    }
   ],
   "source": [
    "LDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f722285-400d-4bda-ab2c-c2124bf48cff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env [~/.conda/envs/torch-env/]",
   "language": "python",
   "name": "conda_torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
