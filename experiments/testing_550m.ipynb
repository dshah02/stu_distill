{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f9aec6-9c5f-4963-9c2f-07bedf39d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import tqdm\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "sys.path.append(os.path.abspath(\"./convex_hull\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64cf4453-1788-444c-b270-de42224f90d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf6006/.conda/envs/clean_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Validation loss: 2.8789, trained on ~59.7B FineWeb-Edu tokens.\n",
    "This is a base model and can frequently hallucinate.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "from torch import nn\n",
    "\n",
    "from safetensors import safe_open\n",
    "\n",
    "from model_550m import FlashSTU, FlashSTUConfig, get_spectral_filters\n",
    "\n",
    "\n",
    "CHECKPOINT_PATH = \"../models/model_step-114000.safetensors\"\n",
    "CONFIG_PATH = \"../models/config_2-7b.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bbcc1b9-4ebe-4ac9-a3ce-79dca34de58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_compile(model: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Apply torch.compile to each layer. This makes compilation efficient\n",
    "    due to repeated structure. Alternatively, one can just compile the whole model.\n",
    "    \"\"\"\n",
    "    print(f\"Compiling each {model.__class__.__name__} layer with torch.compile...\")\n",
    "    #start = time.perf_counter()\n",
    "    for idx, layer in model.layers.named_children():\n",
    "        compiled_layer = torch.compile(layer, mode=\"max-autotune\", fullgraph=True)\n",
    "        model.layers.register_module(idx, compiled_layer)\n",
    "    #end = time.perf_counter()\n",
    "    #print(f\"Finished compiling each {model.__class__.__name__} layer in {end - start:.4f} seconds.\")\n",
    "\n",
    "\n",
    "def load_stu_model(config_path: str, checkpoint_path: str, device: torch.device):\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config_data = json.load(f)\n",
    "\n",
    "    dim = config_data[\"dim\"]\n",
    "    num_heads = config_data[\"num_heads\"]\n",
    "    num_layers = config_data[\"num_layers\"]\n",
    "    num_eigh = config_data[\"num_eigh\"]\n",
    "    seq_len = config_data[\"seq_len\"]\n",
    "    use_hankel_L = config_data[\"use_hankel_L\"]\n",
    "    window_size = config_data[\"window_size\"]\n",
    "    vocab_size = config_data[\"vocab_size\"]\n",
    "    mlp_scale = config_data[\"mlp_scale\"]\n",
    "    bias = config_data[\"bias\"]\n",
    "    dropout = config_data[\"dropout\"]\n",
    "    softcap = config_data[\"softcap\"]\n",
    "    theta = config_data[\"theta\"]\n",
    "    torch_compile = config_data[\"torch_compile\"]\n",
    "    torch_dtype = getattr(torch, config_data[\"torch_dtype\"])\n",
    "\n",
    "    model_config = FlashSTUConfig(\n",
    "        dim=dim,\n",
    "        num_heads=num_heads,\n",
    "        num_layers=num_layers,\n",
    "        seq_len=seq_len,\n",
    "        window_size=window_size,\n",
    "        vocab_size=vocab_size,\n",
    "        mlp_scale=mlp_scale,\n",
    "        bias=bias,\n",
    "        dropout=dropout,\n",
    "        softcap=softcap,\n",
    "        theta=theta,\n",
    "        torch_dtype=torch_dtype,\n",
    "    )\n",
    "\n",
    "    spectral_filters = get_spectral_filters(seq_len, num_eigh, use_hankel_L, device, torch_dtype)\n",
    "    model = FlashSTU(model_config, spectral_filters)\n",
    "    model = model.to(device=device, dtype=torch_dtype)\n",
    "\n",
    "    print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "    state_dict = {}\n",
    "    start_time = time()\n",
    "\n",
    "    if checkpoint_path.endswith(\".safetensors\"):\n",
    "        with safe_open(checkpoint_path, framework=\"pt\", device=device.type) as f:\n",
    "            for k in f.keys():\n",
    "                state_dict[k] = f.get_tensor(k)\n",
    "    elif checkpoint_path.endswith(\".pt\"):\n",
    "        state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported checkpoint format: {checkpoint_path}\")\n",
    "    print(f\"Checkpoint loaded in {time() - start_time:.2f} seconds.\")\n",
    "\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    print(\"Model weights loaded successfully!\")\n",
    "\n",
    "    if torch_compile:\n",
    "        model = apply_compile(model)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return model, config_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43c051ad-0d01-4b01-9715-f5f6b3959d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    num_return_sequences=1,\n",
    "    max_length=512,\n",
    "    device=\"cuda\",\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    cache = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate text from the given prompt using top-k sampling.\n",
    "\n",
    "    Args:\n",
    "        model: The FlashSTU model instance.\n",
    "        tokenizer: The tokenizer used for encoding/decoding.\n",
    "        prompt (str): Input prompt text.\n",
    "        num_return_sequences (int): How many sequences to return.\n",
    "        max_length (int): Maximum length of generated tokens.\n",
    "        device: torch device.\n",
    "        temperature (float): Sampling temperature. Higher = more random.\n",
    "        top_k (int): Top-K sampling parameter.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of generated text sequences.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Encode prompt tokens.\n",
    "    tokens = torch.tensor(\n",
    "        [tokenizer.encode(prompt, allowed_special={\"<|endoftext|>\"})],\n",
    "        device=device,\n",
    "    )\n",
    "    seq_len = tokens.shape[1]\n",
    "    tokens = tokens.repeat(num_return_sequences, 1)\n",
    "    \n",
    "    input_pos = torch.arange(seq_len, device=device)\n",
    "\n",
    "    \n",
    "    sample_rng = torch.Generator(device=device)\n",
    "    sample_rng.manual_seed(1746)\n",
    "\n",
    "    eos_token_id = tokenizer.encode(\n",
    "        \"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}\n",
    "    )[0]\n",
    "    cur_token = seq_len\n",
    "    with torch.no_grad():\n",
    "        for idx in tqdm.tqdm(range(max_length - tokens.size(1))):\n",
    "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                # Fwd pass. Inspect logits here.\n",
    "                if not cache:\n",
    "                    logits = model(tokens)\n",
    "                elif idx != 0:\n",
    "                    logits = model(tokens[:, -1:], input_pos = input_pos)     # shape: [batch, 1, vocab]\n",
    "                else:\n",
    "                    logits = model(tokens, input_pos = input_pos)     # shape: [batch, seq, vocab]\n",
    "                logits = logits[:, -1, :]  # last token logits\n",
    "\n",
    "                # Apply temperature scaling.\n",
    "                if temperature > 0:\n",
    "                    logits = logits / temperature\n",
    "\n",
    "            # Compute probabilities.\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Top-K sampling.\n",
    "            top_k_probs, top_k_indices = torch.topk(probs, top_k, dim=-1)\n",
    "            ix = torch.multinomial(top_k_probs, 1, generator=sample_rng)\n",
    "            next_token = torch.gather(top_k_indices, -1, ix)\n",
    "\n",
    "            # Append next token.\n",
    "            tokens = torch.cat((tokens, next_token), dim=1)\n",
    "            input_pos = torch.tensor([cur_token]).to(device)\n",
    "            cur_token +=1 \n",
    "            # Stop if EOS token is generated.\n",
    "            # if (next_token == eos_token_id).any():\n",
    "                # break\n",
    "\n",
    "    # Decode all sequences.\n",
    "    generated_sequences = []\n",
    "    for i in range(num_return_sequences):\n",
    "        decoded = tokenizer.decode(tokens[i].tolist())\n",
    "        generated_sequences.append(decoded)\n",
    "\n",
    "    return generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c729eacd-b0f4-4464-b578-897dc212ca38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tiktoken.load import load_tiktoken_bpe\n",
    "bpe_path = \"../models/o200k_base.tiktoken\"\n",
    "bpe_dict = load_tiktoken_bpe(bpe_path)\n",
    "\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    name=\"o200k_base\",  # Name of the encoding\n",
    "    pat_str=r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+\"\"\",\n",
    "    mergeable_ranks=bpe_dict,\n",
    "    special_tokens={\n",
    "        \"<|endoftext|>\": 199999,  # Custom special token example (modify as needed)\n",
    "        \"<|endofprompt|>\": 200018,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88070375-8af1-4a23-90b0-d0f70644c191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameter Count: 550.31M\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf6006/.conda/envs/clean_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1329: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at /pytorch/aten/src/ATen/native/Copy.cpp:308.)\n",
      "  return t.to(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from ../models/model_step-114000.safetensors...\n",
      "Checkpoint loaded in 0.37 seconds.\n",
      "Model weights loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Load model and config.\n",
    "model, config_data = load_stu_model(CONFIG_PATH, CHECKPOINT_PATH, device)\n",
    "# tokenizer = tiktoken.get_encoding(\"o200k_base\") #need to cache this for offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcd2dd73-e402-444e-a0d3-29e4c7b33a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import autocast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import tqdm \n",
    "\n",
    "from dataloader import DataLoader\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    bsz=1,\n",
    "    seq_len= 128,\n",
    "    dataset='../fineweb-edu', \n",
    "    split=\"val\", \n",
    "    main_process=True,\n",
    ")\n",
    "\n",
    "def evaluate(model, val_steps = 5):\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    val_loss = 0.0\n",
    "    torch_dtype = getattr(torch, 'bfloat16')\n",
    "    # val_steps = 5 # Abitrarily set to reduce long evaluations, >20 typically used\n",
    "    model.eval()\n",
    "    val_loader.reset()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in tqdm.tqdm(zip(range(val_steps), val_loader, strict=False)):\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            if torch_dtype != torch.float32:\n",
    "                with autocast(device_type=device.type, dtype=torch_dtype, cache_enabled=True):\n",
    "                    preds = model(inputs)\n",
    "            else:\n",
    "                preds = model(inputs)\n",
    "\n",
    "            loss = loss_fn(preds.flatten(0, 1), targets.flatten(0, 1))\n",
    "            loss = loss / val_steps\n",
    "            val_loss += loss.detach().float()\n",
    "    return(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d95e05c-8320-4061-85c8-3a62b9fee437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for prompt 1: Harry potter is famous for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 994/994 [00:13<00:00, 75.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Harry potter is famous for\n",
      "Generated Text: Harry potter is famous for his potter's wheel, which is a device for controlling the flow of water from a potter's wheel to a potter's wheel. Potters' wheels are operated in the potter's wheel by a handle. Potters' wheels are arranged for turning the potter's wheel in the potter's wheel. If the potter's wheel is turning in the potter's wheel, the potter's wheel turns toward the potter's wheel. If the potter's wheel is turning in the potter's wheel, the potter's wheel turns away from the potter's wheel.\n",
      "The potter's wheel has a handle, called a potter's wheel handle. The handle is located in the bottom of the potter's wheel. The handle is connected to the potter's wheel handle. The handle is connected to a handlebar. The handlebar is attached to the potter's wheel handle. The potter's wheel also has a handle that is connected to a handlebar. The handlebar is attached to the potter's wheel handle. The handlebar is attached to the handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The handlebar is attached to the handlebar. The handlebar is connected to the handlebar. The handlebar and handlebar are connected to the potter's wheel handle. The potter's wheel has a handlebar that is connected to a handlebar. The handlebar is connected to the potter's wheel handle. The potter's wheel has a handlebar that is connected to a handlebar. The handlebar is connected to the handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The handlebar is connected to the handlebar. The potter's wheel has a handlebar that is connected to the handlebar. The handlebar is connected to the handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The handlebar is connected to the handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The handlebar is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel has a handlebar that is connected to a handlebar. The potter's wheel\n",
      "\n",
      "Tokens per second: 75.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Collect prompt(s) from user.\n",
    "# prompts = []\n",
    "# while True:\n",
    "#     prompt = input(\"Enter a prompt (press enter with no text to finish): \")\n",
    "#     if not prompt:\n",
    "#         break\n",
    "#     prompts.append(prompt.strip())\n",
    "\n",
    "#     if len(prompts) == 0:\n",
    "#         print(\"No prompts provided. Exiting.\")\n",
    "#         break\n",
    "\n",
    "prompts = ['Harry potter is famous for']\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# BASE SETTINGS:\n",
    "BASE_TEMPERATURE = 0.7  # Increase for more randomness.\n",
    "BASE_TOP_K = 50         # Limit sampling to the top k tokens.\n",
    "MAX_LENGTH = 1000        # Maximum number of tokens to generate.\n",
    "# -------------------------------------------------------------------\n",
    "def generate_and_time(model, prompts, cache = True):\n",
    "    total_tokens = 0\n",
    "    start_time = time()\n",
    "    \n",
    "    for i, prompt in enumerate(prompts, 1):\n",
    "        print(f\"Generating text for prompt {i}: {prompt}\")\n",
    "        if cache:\n",
    "            model.setup_caches(batch_size = 1)\n",
    "        if not cache and model.caches_are_enabled():\n",
    "            model.reset_caches()\n",
    "            \n",
    "        generated_texts = generate_text(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            prompt,\n",
    "            num_return_sequences=1,\n",
    "            max_length=MAX_LENGTH,\n",
    "            device=device,\n",
    "            temperature=BASE_TEMPERATURE,\n",
    "            top_k=BASE_TOP_K,\n",
    "            cache = cache\n",
    "        )\n",
    "        for gen_text in generated_texts:\n",
    "            print(f\"\\nPrompt: {prompt}\")\n",
    "            print(f\"Generated Text: {gen_text}\\n\")\n",
    "            total_tokens += len(\n",
    "                tokenizer.encode(gen_text, allowed_special={\"<|endoftext|>\"})\n",
    "            )\n",
    "        \n",
    "        if cache:\n",
    "            model.reset_caches()\n",
    "    \n",
    "    end_time = time()\n",
    "    tokens_per_second = total_tokens / (end_time - start_time)\n",
    "    print(f\"Tokens per second: {tokens_per_second:.2f}\")\n",
    "generate_and_time(model, prompts, True)\n",
    "\n",
    "#note that when stu is entirely removed, the speed is 145.89 it.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bb5728f-d4ba-4007-97fb-5df1b5ab1bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:04, 11.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.3540, device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, val_steps = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9dd1089-bd32-444e-96d1-b716d84d198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from full_fast_stu import FullFastSTU\n",
    "# from full_fast_accel import FullFastSTU\n",
    "import copy\n",
    "\n",
    "stu_before = []\n",
    "stu_after = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66ab57d6-201a-4e2b-8005-101a47b8c2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(0, 12, 2):\n",
    "    stu_layer = copy.deepcopy(model.layers[idx].stu)\n",
    "    fast_layer = FullFastSTU(stu_layer.cuda(), \"../experiments/convex_hull/fit_filters_205/250_phi_lds_float32.pt\").cuda()\n",
    "    model.layers[idx].stu = torch.compile(fast_layer)\n",
    "\n",
    "    stu_before.append(stu_layer)\n",
    "    stu_after.append(fast_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce1ada22-3ea0-4a72-b4f3-cf0f72fb0404",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:08,  5.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.3542, device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, val_steps = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5713d29-36bf-42dd-bf92-719a64aba6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for prompt 1: Harry potter is famous for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 994/994 [00:09<00:00, 101.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Harry potter is famous for\n",
      "Generated Text: Harry potter is famous for his potter's wheel, which is a device for controlling the flow of water from a potter's wheel to a potter's wheel. Potters' wheels are connected to the potter's wheel by a chain. Potters' wheels are found to be very useful because they make a great deal of money. Potters' wheels are made of metal which makes them durable and resistant to corrosion. Potters' wheels are extremely useful because they can be used to control the flow of water from a potter's wheel to a potter's wheel. Potters' wheels are also very useful because they are very reliable and easy to use. Potters' wheels are very useful because they can be used to control the flow of water from a potter's wheel to a potter's wheel. Potters' wheels are also very useful because they can be used to control the flow of water from a potter's wheel to a potter's wheel.\n",
      "Potters' wheels are very useful because they can be used to control the flow of water from a potter's wheel to a potter's wheel. Potters' wheels are also very useful because they can be used to control the flow of water from a potter's wheel to a potter's wheel. Potters' wheels are very useful because they can be used to control the flow of water from a potter's wheel to a potter's wheel. Potters' wheels are also very useful because they can be used to control the flow of water from a potter's wheel to a potter's wheel. Potters' wheels are also very useful because they can be used to control the flow of water from a potter's wheel to a potter's wheel. Potters' wheels are also very useful because they can be used to control the flow of water from a potter's wheel to a potter's wheel. Potters' wheels are also very useful because they can be used to control the flow of water from a potter's wheel to a potter's wheel. Potters' wheels are also very useful because they can be used to control the flow of water from a potter's wheel to a potter's wheel. Potters' wheels are also very useful because they can be used to control the flow of water from a potter's wheel to a potter's wheel. Potters' wheels are also very useful because they can be used to control the flow of water from a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter's wheel to a potter\n",
      "\n",
      "Tokens per second: 101.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_ = generate_and_time(model, prompts, cache = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ce7ef0-dffc-4ee7-aa3c-3a00b600684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_time(model, prompts, cache = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f0b53c-611c-4ede-9c32-fd53abfd0584",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "gaps = []\n",
    "with torch.no_grad():\n",
    "    for i in range(6):\n",
    "        x = torch.randn(1,100,896).cuda()\n",
    "        input_pos = torch.arange(100, device=device)\n",
    "        \n",
    "        stu_out = stu_before[i](x.bfloat16(), input_pos)\n",
    "        lds_out = stu_after[i](x.bfloat16(), input_pos)\n",
    "        print(F.l1_loss(stu_out, lds_out))\n",
    "        gaps.append((stu_out - lds_out).abs().cpu().detach().float().flatten())\n",
    "        #F1 of approx 0.005 between stu with lds filters and lds. That's pretty high error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43968ab9-e845-460a-abd8-9a80e67dc369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_ = plt.hist(gaps, bins = 100, log = True)\n",
    "# plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6568999-5e74-49e7-a91c-85e748194833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eefaaa0-cdee-4ee6-9d9e-6fa906d7e617",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_env [~/.conda/envs/clean_env/]",
   "language": "python",
   "name": "conda_clean_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
