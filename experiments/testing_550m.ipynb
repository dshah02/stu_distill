{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f9aec6-9c5f-4963-9c2f-07bedf39d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import tqdm\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "sys.path.append(os.path.abspath(\"./convex_hull\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64cf4453-1788-444c-b270-de42224f90d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ds6237/.conda/envs/torch-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Validation loss: 2.8789, trained on ~59.7B FineWeb-Edu tokens.\n",
    "This is a base model and can frequently hallucinate.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "from torch import nn\n",
    "\n",
    "from safetensors import safe_open\n",
    "\n",
    "from model_550m import FlashSTU, FlashSTUConfig, get_spectral_filters\n",
    "\n",
    "\n",
    "CHECKPOINT_PATH = \"../models/model_step-114000.safetensors\"\n",
    "CONFIG_PATH = \"../models/config_2-7b.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bbcc1b9-4ebe-4ac9-a3ce-79dca34de58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_compile(model: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Apply torch.compile to each layer. This makes compilation efficient\n",
    "    due to repeated structure. Alternatively, one can just compile the whole model.\n",
    "    \"\"\"\n",
    "    print(f\"Compiling each {model.__class__.__name__} layer with torch.compile...\")\n",
    "    start = time.perf_counter()\n",
    "    for idx, layer in model.layers.named_children():\n",
    "        compiled_layer = torch.compile(layer, mode=\"max-autotune\", fullgraph=True)\n",
    "        model.layers.register_module(idx, compiled_layer)\n",
    "    end = time.perf_counter()\n",
    "    print(f\"Finished compiling each {model.__class__.__name__} layer in {end - start:.4f} seconds.\")\n",
    "\n",
    "\n",
    "def load_stu_model(config_path: str, checkpoint_path: str, device: torch.device):\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config_data = json.load(f)\n",
    "\n",
    "    dim = config_data[\"dim\"]\n",
    "    num_heads = config_data[\"num_heads\"]\n",
    "    num_layers = config_data[\"num_layers\"]\n",
    "    num_eigh = config_data[\"num_eigh\"]\n",
    "    seq_len = config_data[\"seq_len\"]\n",
    "    use_hankel_L = config_data[\"use_hankel_L\"]\n",
    "    window_size = config_data[\"window_size\"]\n",
    "    vocab_size = config_data[\"vocab_size\"]\n",
    "    mlp_scale = config_data[\"mlp_scale\"]\n",
    "    bias = config_data[\"bias\"]\n",
    "    dropout = config_data[\"dropout\"]\n",
    "    softcap = config_data[\"softcap\"]\n",
    "    theta = config_data[\"theta\"]\n",
    "    torch_compile = config_data[\"torch_compile\"]\n",
    "    torch_dtype = getattr(torch, config_data[\"torch_dtype\"])\n",
    "\n",
    "    model_config = FlashSTUConfig(\n",
    "        dim=dim,\n",
    "        num_heads=num_heads,\n",
    "        num_layers=num_layers,\n",
    "        seq_len=seq_len,\n",
    "        window_size=window_size,\n",
    "        vocab_size=vocab_size,\n",
    "        mlp_scale=mlp_scale,\n",
    "        bias=bias,\n",
    "        dropout=dropout,\n",
    "        softcap=softcap,\n",
    "        theta=theta,\n",
    "        torch_dtype=torch_dtype,\n",
    "    )\n",
    "\n",
    "    spectral_filters = get_spectral_filters(seq_len, num_eigh, use_hankel_L, device, torch_dtype)\n",
    "    model = FlashSTU(model_config, spectral_filters)\n",
    "    model = model.to(device=device, dtype=torch_dtype)\n",
    "\n",
    "    print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "    state_dict = {}\n",
    "    start_time = time()\n",
    "\n",
    "    if checkpoint_path.endswith(\".safetensors\"):\n",
    "        with safe_open(checkpoint_path, framework=\"pt\", device=device.type) as f:\n",
    "            for k in f.keys():\n",
    "                state_dict[k] = f.get_tensor(k)\n",
    "    elif checkpoint_path.endswith(\".pt\"):\n",
    "        state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported checkpoint format: {checkpoint_path}\")\n",
    "    print(f\"Checkpoint loaded in {time() - start_time:.2f} seconds.\")\n",
    "\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    print(\"Model weights loaded successfully!\")\n",
    "\n",
    "    if torch_compile:\n",
    "        model = apply_compile(model)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return model, config_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43c051ad-0d01-4b01-9715-f5f6b3959d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    num_return_sequences=1,\n",
    "    max_length=512,\n",
    "    device=\"cuda\",\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    cache = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate text from the given prompt using top-k sampling.\n",
    "\n",
    "    Args:\n",
    "        model: The FlashSTU model instance.\n",
    "        tokenizer: The tokenizer used for encoding/decoding.\n",
    "        prompt (str): Input prompt text.\n",
    "        num_return_sequences (int): How many sequences to return.\n",
    "        max_length (int): Maximum length of generated tokens.\n",
    "        device: torch device.\n",
    "        temperature (float): Sampling temperature. Higher = more random.\n",
    "        top_k (int): Top-K sampling parameter.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of generated text sequences.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Encode prompt tokens.\n",
    "    tokens = torch.tensor(\n",
    "        [tokenizer.encode(prompt, allowed_special={\"<|endoftext|>\"})],\n",
    "        device=device,\n",
    "    )\n",
    "    seq_len = tokens.shape[1]\n",
    "    tokens = tokens.repeat(num_return_sequences, 1)\n",
    "    \n",
    "    input_pos = torch.arange(seq_len, device=device)\n",
    "\n",
    "    \n",
    "    sample_rng = torch.Generator(device=device)\n",
    "    sample_rng.manual_seed(1746)\n",
    "\n",
    "    eos_token_id = tokenizer.encode(\n",
    "        \"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}\n",
    "    )[0]\n",
    "    cur_token = seq_len\n",
    "    with torch.no_grad():\n",
    "        for idx in tqdm.tqdm(range(max_length - tokens.size(1))):\n",
    "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                # Fwd pass. Inspect logits here.\n",
    "                if not cache:\n",
    "                    logits = model(tokens)\n",
    "                elif idx != 0:\n",
    "                    logits = model(tokens[:, -1:], input_pos = input_pos)     # shape: [batch, 1, vocab]\n",
    "                else:\n",
    "                    logits = model(tokens, input_pos = input_pos)     # shape: [batch, seq, vocab]\n",
    "                logits = logits[:, -1, :]  # last token logits\n",
    "\n",
    "                # Apply temperature scaling.\n",
    "                if temperature > 0:\n",
    "                    logits = logits / temperature\n",
    "\n",
    "            # Compute probabilities.\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Top-K sampling.\n",
    "            top_k_probs, top_k_indices = torch.topk(probs, top_k, dim=-1)\n",
    "            ix = torch.multinomial(top_k_probs, 1, generator=sample_rng)\n",
    "            next_token = torch.gather(top_k_indices, -1, ix)\n",
    "\n",
    "            # Append next token.\n",
    "            tokens = torch.cat((tokens, next_token), dim=1)\n",
    "            input_pos = torch.tensor([cur_token]).to(device)\n",
    "            cur_token +=1 \n",
    "            # Stop if EOS token is generated.\n",
    "            # if (next_token == eos_token_id).any():\n",
    "                # break\n",
    "\n",
    "    # Decode all sequences.\n",
    "    generated_sequences = []\n",
    "    for i in range(num_return_sequences):\n",
    "        decoded = tokenizer.decode(tokens[i].tolist())\n",
    "        generated_sequences.append(decoded)\n",
    "\n",
    "    return generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c729eacd-b0f4-4464-b578-897dc212ca38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tiktoken.load import load_tiktoken_bpe\n",
    "bpe_path = \"../models/o200k_base.tiktoken\"\n",
    "bpe_dict = load_tiktoken_bpe(bpe_path)\n",
    "\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    name=\"o200k_base\",  # Name of the encoding\n",
    "    pat_str=r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+\"\"\",\n",
    "    mergeable_ranks=bpe_dict,\n",
    "    special_tokens={\n",
    "        \"<|endoftext|>\": 199999,  # Custom special token example (modify as needed)\n",
    "        \"<|endofprompt|>\": 200018,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88070375-8af1-4a23-90b0-d0f70644c191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameter Count: 550.31M\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ds6237/.conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1326: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at /opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/Copy.cpp:308.)\n",
      "  return t.to(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from ../models/model_step-114000.safetensors...\n",
      "Checkpoint loaded in 0.22 seconds.\n",
      "Model weights loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Load model and config.\n",
    "model, config_data = load_stu_model(CONFIG_PATH, CHECKPOINT_PATH, device)\n",
    "# tokenizer = tiktoken.get_encoding(\"o200k_base\") #need to cache this for offline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcd2dd73-e402-444e-a0d3-29e4c7b33a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 02:05:59,437 - INFO - Found 1 shards for split val\n"
     ]
    }
   ],
   "source": [
    "from torch.amp import autocast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import tqdm \n",
    "\n",
    "from dataloader import DataLoader\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    bsz=1,\n",
    "    seq_len= 128,\n",
    "    dataset='../fineweb-edu', \n",
    "    split=\"val\", \n",
    "    main_process=True,\n",
    ")\n",
    "\n",
    "def evaluate(model, val_steps = 5):\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    val_loss = 0.0\n",
    "    torch_dtype = getattr(torch, 'bfloat16')\n",
    "    # val_steps = 5 # Abitrarily set to reduce long evaluations, >20 typically used\n",
    "    model.eval()\n",
    "    val_loader.reset()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in tqdm.tqdm(zip(range(val_steps), val_loader, strict=False)):\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            if torch_dtype != torch.float32:\n",
    "                with autocast(device_type=device.type, dtype=torch_dtype, cache_enabled=True):\n",
    "                    preds = model(inputs)\n",
    "            else:\n",
    "                preds = model(inputs)\n",
    "\n",
    "            loss = loss_fn(preds.flatten(0, 1), targets.flatten(0, 1))\n",
    "            loss = loss / val_steps\n",
    "            val_loss += loss.detach().float()\n",
    "    return(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d95e05c-8320-4061-85c8-3a62b9fee437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for prompt 1: Harry potter is famous for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 994/994 [00:13<00:00, 71.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Harry potter is famous for\n",
      "Generated Text: Harry potter is famous for his potter's wheel, which is a simple machine in which the potter can draw the potter's wheel and then turn the potter's wheel to turn the potter's wheel to make the potter's wheel turn. However, Harry potter is also famous for his pottery which is made of clay, clay tiles, and clay tile. The process is a very complicated process that requires great skill and imagination.\n",
      "Potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel, potter's wheel\n",
      "\n",
      "Tokens per second: 71.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Collect prompt(s) from user.\n",
    "# prompts = []\n",
    "# while True:\n",
    "#     prompt = input(\"Enter a prompt (press enter with no text to finish): \")\n",
    "#     if not prompt:\n",
    "#         break\n",
    "#     prompts.append(prompt.strip())\n",
    "\n",
    "#     if len(prompts) == 0:\n",
    "#         print(\"No prompts provided. Exiting.\")\n",
    "#         break\n",
    "\n",
    "prompts = ['Harry potter is famous for']\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# BASE SETTINGS:\n",
    "BASE_TEMPERATURE = 0.7  # Increase for more randomness.\n",
    "BASE_TOP_K = 50         # Limit sampling to the top k tokens.\n",
    "MAX_LENGTH = 1000        # Maximum number of tokens to generate.\n",
    "# -------------------------------------------------------------------\n",
    "def generate_and_time(model, prompts, cache = True):\n",
    "    total_tokens = 0\n",
    "    start_time = time()\n",
    "    \n",
    "    for i, prompt in enumerate(prompts, 1):\n",
    "        print(f\"Generating text for prompt {i}: {prompt}\")\n",
    "        if cache:\n",
    "            model.setup_caches(batch_size = 1)\n",
    "        if not cache and model.caches_are_enabled():\n",
    "            model.reset_caches()\n",
    "            \n",
    "        generated_texts = generate_text(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            prompt,\n",
    "            num_return_sequences=1,\n",
    "            max_length=MAX_LENGTH,\n",
    "            device=device,\n",
    "            temperature=BASE_TEMPERATURE,\n",
    "            top_k=BASE_TOP_K,\n",
    "            cache = cache\n",
    "        )\n",
    "        for gen_text in generated_texts:\n",
    "            print(f\"\\nPrompt: {prompt}\")\n",
    "            print(f\"Generated Text: {gen_text}\\n\")\n",
    "            total_tokens += len(\n",
    "                tokenizer.encode(gen_text, allowed_special={\"<|endoftext|>\"})\n",
    "            )\n",
    "        \n",
    "        if cache:\n",
    "            model.reset_caches()\n",
    "    \n",
    "    end_time = time()\n",
    "    tokens_per_second = total_tokens / (end_time - start_time)\n",
    "    print(f\"Tokens per second: {tokens_per_second:.2f}\")\n",
    "generate_and_time(model, prompts, True)\n",
    "\n",
    "#note that when stu is entirely removed, the speed is 145.89 it.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bb5728f-d4ba-4007-97fb-5df1b5ab1bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:04, 11.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.3545, device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, val_steps = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9dd1089-bd32-444e-96d1-b716d84d198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from full_fast_stu import FullFastSTU\n",
    "# from full_fast_accel import FullFastSTU\n",
    "import copy\n",
    "\n",
    "stu_before = []\n",
    "stu_after = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66ab57d6-201a-4e2b-8005-101a47b8c2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ds6237/stu_distill/experiments/convex_hull/full_fast_stu.py:104: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "for idx in range(0, 12, 2):\n",
    "    stu_layer = copy.deepcopy(model.layers[idx].stu)\n",
    "    fast_layer = FullFastSTU(stu_layer.cuda(), \"../experiments/convex_hull/fit_filters_205/250_phi_lds_float32.pt\").cuda()\n",
    "    model.layers[idx].stu = fast_layer\n",
    "\n",
    "    stu_before.append(stu_layer)\n",
    "    stu_after.append(fast_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce1ada22-3ea0-4a72-b4f3-cf0f72fb0404",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:05,  8.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.3552, device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, val_steps = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5713d29-36bf-42dd-bf92-719a64aba6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for prompt 1: Harry potter is famous for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 994/994 [00:11<00:00, 89.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Harry potter is famous for\n",
      "Generated Text: Harry potter is famous for his potter's wheel, which is a simple machine in which the potter can draw the potter's wheel and then turn the potter's wheel to turn the potter's wheel to make the potter's wheel turn. However, Harry potter is also famous for his pottery which is made of clay, clay tiles, and clay tile. The term potter's wheel was also used to describe the \"first potter\" and the first potter's wheel to have a handle.\n",
      "Early potters, such as Joseph and David, did not have a handle. Instead, they used a long handle to turn the handle of the potter's wheel to make the potter's wheel turn. These pots were used for making clay pots and other types of pottery. Potters used the handle to turn the handle of the potter's wheel to make the potter's wheel turn. Potters used the handle to turn the potter's wheel by turning the handle and turning the potter's wheel by turning the handle. This is sometimes called the \"first potter's wheel\".\n",
      "As early as the late 1700s, potters used the handle to turn the potter's wheel. This was a simple tool that was used to make the potter's wheel turn. The potter's wheel was made up of two parts: the handle and the handle. The handle was made of wood and was used to hold the potter's wheel and handle. The handle was made of wood and was used to hold the potter's wheel and handle. The handle was made of wood and was used to hold the potter's wheel and handle. The handle was made of wood and was used to hold the potter's wheel and handle. The handle was made of wood and was used to hold the potter's wheel and handle. The handle was made of wood and was used to hold the potter's wheel and handle. The handle was made of wood and was used to hold the potter's wheel and handle. The handle was made of metal and was used to hold the potter's wheel and handle. The handle was made of metal and was used to hold the potter's wheel and handle. The handle was made of metal and was used to hold the potter's wheel and handle. The handle was made of metal and was used to hold the potter's wheel and handle. The handle was made of wood and was used to hold the potter's wheel and handle. The potter's wheel was made of wood and was used to hold the potter's wheel and handle. The potter's wheel was made of wood and was used to hold the potter's wheel and handle. The potter's wheel was made of wood and was used to hold the potter's wheel and handle. The potter's wheel was made of steel and was used to hold the potter's wheel and handle. The potter's wheel was made of wood and was used to hold the potter's wheel and handle. The potter's wheel was made of wood and was used to hold the potter's wheel and handle. The potter's wheel was made of wood and was used to hold the potter's wheel and handle. The potter's wheel was made of wood and was used to hold the potter's wheel and handle. The potter's wheel was made of wood and was used to hold the potter's wheel and handle. The potter's wheel was made of wood and was used to hold the potter's wheel and handle. The potter's wheel was made of wood and was used to hold the potter's wheel and handle. The potter's wheel was made of wood and was used to hold the potter's wheel and handle. The potter's wheel was made of wood and was used to hold the potter's wheel and handle. The potter's wheel was made of wood and was used to hold the potter's wheel and handle. The potter's wheel was made of wood and was used to hold the potter's wheel and handle. The potter's wheel was made of wood and was used to hold the potter's wheel and handle. The potter's wheel was made of wood and was used to hold the potter's wheel and handle. The potter's wheel was made of wood and was used to hold the potter's wheel and handle. The potter's wheel was made of iron and was used to hold the potter's wheel and handle. The potter's wheel was made of wood and was used to hold the potter's wheel and handle. The potter's wheel was made of wood and was used to hold the potter's wheel and handle. The potter's wheel was made of wood and was used to hold the potter's wheel and handle. The potter's\n",
      "\n",
      "Tokens per second: 89.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_ = generate_and_time(model, prompts, cache = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62ce7ef0-dffc-4ee7-aa3c-3a00b600684b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for prompt 1: Harry potter is famous for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 69/994 [00:05<01:14, 12.48it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_and_time(model, prompts, cache = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4f0b53c-611c-4ede-9c32-fd53abfd0584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0006, device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor(0.0021, device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor(0.0037, device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor(0.0045, device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor(0.0040, device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor(0.0042, device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "gaps = []\n",
    "with torch.no_grad():\n",
    "    for i in range(6):\n",
    "        x = torch.randn(1,100,896).cuda()\n",
    "        input_pos = torch.arange(100, device=device)\n",
    "        \n",
    "        stu_out = stu_before[i](x.bfloat16(), input_pos)\n",
    "        lds_out = stu_after[i](x.bfloat16(), input_pos)\n",
    "        print(F.l1_loss(stu_out, lds_out))\n",
    "        gaps.append((stu_out - lds_out).abs().cpu().detach().float().flatten())\n",
    "        #F1 of approx 0.005 between stu with lds filters and lds. That's pretty high error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43968ab9-e845-460a-abd8-9a80e67dc369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAGdCAYAAADJ6dNTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAgTklEQVR4nO3df0yd5f3/8dcplENqCg7Rs2Ipq9/MAmul6wEVtLMs2+lwrdFmkcUEcWmXEVgsI8ZImmxd58Qla9Pt00NNnZHuD5V0Tlz2IetI3EojcxaExY0u2g0HtSChmz0FJ+jp/f1j6/l4eg5tD5wf133u5yO5/7jv+zrX/T5Xr8jL+9w/XJZlWQIAADDEklQXAAAA8EmEEwAAYBTCCQAAMArhBAAAGIVwAgAAjEI4AQAARiGcAAAAoxBOAACAUTJTXUCsLly4oDNnzmj58uVyuVypLgcAAFwFy7J0/vx5FRQUaMmSy58bsV04OXPmjAoLC1NdBgAAWICxsTGtXLnysm1sF06WL18u6T9fLicnJ8XVAACAqxEIBFRYWBj6O345tgsnF3/KycnJIZwAAGAzV3NJBhfEAgAAoxBOAACAUQgnAADAKCkJJ5mZmVq/fr3Wr1+vHTt2pKIEAABgqJRcEHvttddqaGgoFYcGAACG42cdAABglJjDSW9vr7Zu3aqCggK5XC51dXVFtGlvb9fq1auVnZ0tr9er48ePh+0PBALyer268847dezYsQUXDwAA0k/M4WRmZkZlZWU6cOBA1P2dnZ1qbm7Wrl27NDg4qI0bN6qmpkajo6OhNu+8844GBgb01FNP6cEHH1QgEFj4NwAAAGnFZVmWteAPu1x66aWXdO+994a23XbbbdqwYYMOHjwY2lZSUqJ7771XbW1tEX3U1NToBz/4gcrLy6MeY3Z2VrOzs6H1i0+YO3fuHA9hAwDAJgKBgHJzc6/q73dcrzmZm5vTwMCAfD5f2Hafz6e+vj5J0r/+9a9Q2Dh9+rSGh4d10003zdtnW1ubcnNzQwvv1QEAIL3FNZxMTU0pGAzK4/GEbfd4PJqYmJAknTx5UuXl5SorK9OWLVv0k5/8RHl5efP22draqnPnzoWWsbGxeJYMAAAMk5BbiS99br5lWaFtVVVVevPNN6+6L7fbLbfbHdf6AACAueJ65iQ/P18ZGRmhsyQXTU5ORpxNiZXf71dpaakqKioW1Q8AADBbXMNJVlaWvF6venp6wrb39PSoqqpqUX03NTVpeHhYJ06cWFQ/AADAbDH/rDM9Pa1Tp06F1kdGRjQ0NKS8vDytWrVKLS0tqqurU3l5uSorK3Xo0CGNjo6qoaEhroUDAID0FHM46e/vV3V1dWi9paVFklRfX6+Ojg7V1tbq7Nmz2rNnj8bHx7V27Vp1d3erqKgoflUDAIC0tajnnCST3++X3+9XMBjUW2+9xXNOAACwkViec2KbcHJRLF8OAACYIWUPYQMAAFgswgkAADCKbcIJzzkBAMAZuOYEAAAkHNecLMbu3FRXAACAoxFOAACAUWwTTrjmBAAAZ7BNOEn2u3X8Da8k5TgAACCcbcIJAABwBsJJFCeLS1JdAgAAjkU4AQAARiGcAAAAo9gmnHC3DgAAzmCbcJLsu3UAAEBq2CacAAAAZyCcAAAAoxBOAACAUQgnAADAKLYJJ9ytAwCAM9gmnHC3DgAAzmCbcAIAAJyBcAIAAIxCOAEAAEYhnAAAAKMQTgAAgFEIJwAAwCiEEwAAYBTbhBMewgYAgDPYJpzwEDYAAJzBNuEEAAA4A+EEAAAYhXACAACMQjgBAABGIZwAAACjEE4AAIBRCCcAAMAohBMAAGAUwgkAADAK4QQAABjFNuGEd+sAAOAMtgknvFsHAABnsE04AQAAzkA4AQAARiGcAAAAoxBOAACAUQgnAADAKIQTAABgFMIJAAAwCuEEAAAYhXACAACMQji5CusOr0t1CQAAOAbhBAAAGIVwAgAAjJKycPLBBx+oqKhIjzzySKpKAAAABkpZOPnhD3+o2267LVWHX5C9tVvm3cd1KQAAxEdKwsnbb7+tv/71r7r77rtTcfgFOVlckuoSAABwhJjDSW9vr7Zu3aqCggK5XC51dXVFtGlvb9fq1auVnZ0tr9er48ePh+1/5JFH1NbWtuCiAQBA+oo5nMzMzKisrEwHDhyIur+zs1PNzc3atWuXBgcHtXHjRtXU1Gh0dFSS9PLLL+vmm2/WzTffvLjKDXW5n34AAMCVZcb6gZqaGtXU1My7f9++fdq+fbt27NghSdq/f7+OHj2qgwcPqq2tTa+99ppeeOEFHTlyRNPT0/roo4+Uk5Oj7373u1H7m52d1ezsbGg9EAjEWnLSnCwukcr+X6rLAADA1uJ6zcnc3JwGBgbk8/nCtvt8PvX19UmS2traNDY2pnfeeUc//vGP9c1vfnPeYHKxfW5ubmgpLCyMZ8kAAMAwcQ0nU1NTCgaD8ng8Yds9Ho8mJiYW1Gdra6vOnTsXWsbGxuJRKgAAMFTMP+tcDZfLFbZuWVbENkl66KGHrtiX2+2W2+2OV2kAAMBwcT1zkp+fr4yMjIizJJOTkxFnU2Ll9/tVWlqqioqKRfUDAADMFtdwkpWVJa/Xq56enrDtPT09qqqqWlTfTU1NGh4e1okTJxbVDwAAMFvMP+tMT0/r1KlTofWRkRENDQ0pLy9Pq1atUktLi+rq6lReXq7KykodOnRIo6OjamhoiGvhAAAgPcUcTvr7+1VdXR1ab2lpkSTV19ero6NDtbW1Onv2rPbs2aPx8XGtXbtW3d3dKioqil/VAAAgbcUcTjZt2iTLsi7bprGxUY2NjQsuKhq/3y+/369gMBjXfgEAgFlS9uK/WHHNCQAAzmCbcAIAAJyBcAIAAIxim3DCc04AAHAG24QTrjkBAMAZbBNO0tW6w+tSXQIAAEYhnAAAAKPYJpyk8zUn/oZXUl0CAADGsE044ZoTAACcwTbhBAAAOAPhBAAAGIVwAgAAjEI4AQAARrFNOEnnu3UAAMD/sU044W4dAACcwTbhBAAAOAPhBAAAGIVwAgAAjEI4AQAARrFNOOFuHQAAnME24YS7dQAAcAbbhBMAAOAMhBMAAGAUwgkAADAK4QQAABiFcAIAAIxCOAEAAEYhnAAAAKPYJpzwEDYAAJzBNuGEh7ABAOAMtgknAADAGQgnAADAKIQTAABgFMIJAAAwCuEEAAAYhXDiMP6GV1JdAgAAl0U4AQAARiGcXMFnHvvfVJcAAICjEE4AAIBRCCcAAMAotgknvFsHAABnsE044d06AAA4g23CCQAAcAbCCQAAMArhBAAAGIVwAgAAjEI4AQAARiGcAAAAoxBOAACAUQgnDrLu8LpUlwAAwBURTlJpd26qKwAAwDiEEwAAYBTCCQAAMArhBAAAGCXp4eT8+fOqqKjQ+vXrtW7dOj399NPJLgEAABgsM9kHXLZsmY4dO6Zly5bpgw8+0Nq1a7Vt2zZdd911yS4FAAAYKOlnTjIyMrRs2TJJ0ocffqhgMCjLspJdBgAAMFTM4aS3t1dbt25VQUGBXC6Xurq6Itq0t7dr9erVys7Oltfr1fHjx8P2v//++yorK9PKlSv16KOPKj8/f8FfAAAApJeYw8nMzIzKysp04MCBqPs7OzvV3NysXbt2aXBwUBs3blRNTY1GR0dDba699lr96U9/0sjIiJ577jm99957C/8GAAAgrcQcTmpqavT4449r27ZtUffv27dP27dv144dO1RSUqL9+/ersLBQBw8ejGjr8Xh0yy23qLe3d97jzc7OKhAIhC0AACB9xfWak7m5OQ0MDMjn84Vt9/l86uvrkyS99957oYARCATU29urNWvWzNtnW1ubcnNzQ0thYWE8S14Uf8MrqS4BAIC0E9dwMjU1pWAwKI/HE7bd4/FoYmJCknT69Gl94QtfUFlZme688059+9vf1i233DJvn62trTp37lxoGRsbi2fJC3ayuCTVJQAAkJYSciuxy+UKW7csK7TN6/VqaGjoqvtyu91yu93xLA8AABgsrmdO8vPzlZGRETpLctHk5GTE2ZRY+f1+lZaWqqKiYlH9AAAAs8U1nGRlZcnr9aqnpydse09Pj6qqqhbVd1NTk4aHh3XixIlF9QMAAMwW888609PTOnXqVGh9ZGREQ0NDysvL06pVq9TS0qK6ujqVl5ersrJShw4d0ujoqBoaGuJaOAAASE8xh5P+/n5VV1eH1ltaWiRJ9fX16ujoUG1trc6ePas9e/ZofHxca9euVXd3t4qKiuJXNQAASFsxh5NNmzZd8XHzjY2NamxsXHBR0fj9fvn9fgWDwbj2CwAAzJL0d+ssFNecAADgDLYJJwAAwBkIJwAAwCi2CSc85wQAAGewTTjhmhMAAJzBNuEEAAA4A+EEAAAYxTbhhGtOAABwBtuEE645AQDAGWwTTgAAgDMQTgAAgFEIJwAAwCiEEwAAYBTbhBPu1gEAwBlsE06Multnd27Y6mce+98UFQIAQPqxTTixK3/DK6kuAQAAWyGcIC2dLC5JdQkAgAUinAAAAKMQTgAAgFFsE07seLfOusPrUl0CAAC2Y5twYtTdOgAAIGFsE06AWHGnFADYE+EEAAAYhXACAACMQjgBAABGIZwAAACjEE4AAIBRCCcAAMAotgkndnwIGwAAiJ1twgkPYQMAwBlsE04AAIAzEE4AAIBRCCcAAMAohBMAAGAUwgkAADAK4QQAABiFcAIAAIxCOAEAAEYhnAAAAKMQTgAAgFFsE054tw4AAM5gm3DCu3UAAHAG24QTAADgDIQTAABgFMIJAAAwCuEEAAAYhXBiUyeLS1JdAgAACUE4AQAARiGcAAAAoxBOAACAUQgnAADAKIQTAABgFMIJAAAwCuEEAAAYJenhZGxsTJs2bVJpaaluueUWHTlyJNklAAAAg2Um/YCZmdq/f7/Wr1+vyclJbdiwQXfffbeuueaaZJcCAAAMlPRwsmLFCq1YsUKSdMMNNygvL0///Oc/CScAAEDSAn7W6e3t1datW1VQUCCXy6Wurq6INu3t7Vq9erWys7Pl9Xp1/PjxqH319/frwoULKiwsjLlwAACQnmIOJzMzMyorK9OBAwei7u/s7FRzc7N27dqlwcFBbdy4UTU1NRodHQ1rd/bsWT344IM6dOjQwioHAABpKeafdWpqalRTUzPv/n379mn79u3asWOHJGn//v06evSoDh48qLa2NknS7Oys7rvvPrW2tqqqquqyx5udndXs7GxoPRAIxFoyAACwkbjerTM3N6eBgQH5fL6w7T6fT319fZIky7L00EMP6Ytf/KLq6uqu2GdbW5tyc3NDixN+AkrIG4d358a/TwAAEiCu4WRqakrBYFAejydsu8fj0cTEhCTp1VdfVWdnp7q6urR+/XqtX79eb7755rx9tra26ty5c6FlbGwsniUnDmEAAIAFScjdOi6XK2zdsqzQtjvvvFMXLly46r7cbrfcbndc60ulk8UlKvnryVSXAQCAseJ65iQ/P18ZGRmhsyQXTU5ORpxNiZXf71dpaakqKioW1Q8AADBbXMNJVlaWvF6venp6wrb39PRc8cLXK2lqatLw8LBOnDixqH4AAIDZYv5ZZ3p6WqdOnQqtj4yMaGhoSHl5eVq1apVaWlpUV1en8vJyVVZW6tChQxodHVVDQ0NcCwcAAOkp5nDS39+v6urq0HpLS4skqb6+Xh0dHaqtrdXZs2e1Z88ejY+Pa+3ateru7lZRUVH8qgYAAGkr5nCyadMmWZZ12TaNjY1qbGxccFHR+P1++f1+BYPBuPYLAADMkvS3Ei8U15wAAOAMtgknAADAGQgnAADAKLYJJzznBAAAZ7BNOOGak0h7a7ekugQAAOLONuEEAAA4A+EEAAAYxTbhJB2uOVl3eF2qSwAAwHi2CSdccwIAgDPYJpwAAABnIJwAAACjEE4AAIBRCCcAAMAotgkn6XC3DgAAuDLbhBPu1gEAwBlsE04AAIAzEE5S4GreicN7cwAATkU4AWAsf8MrqS4BQAoQTgAAgFFsE064WwcAAGewTTjhbh0AAJzBNuEEAAA4A+EEAAAYhXACAACMQjgBAABGIZwAAACjEE4AAIBRCCcAAMAotgknPIQNAABnsE044SFsAAA4g23CCQAAcAbCCQAAMArhBAAAGIVwYph1h9elugQAAFKKcAIAAIxCOAEAAEYhnAAAAKMQTgAAgFEIJwAAwCiEEwAAYBTbhBPerQM4y8niklSXACBFbBNOeLcOAADOYJtwAgAAnIFwAgAAjEI4AQAARiGcIGH8Da+kugQAgA0RTgAAgFEIJwAAwCiEEwAAYBTCCQAAMArhBAm1t3ZLqksAANgM4QQAABiFcAIAAIxCOMEV8QI2AEAyEU4AAIBRUhJO7rvvPn3qU5/S1772tVQcHgAARGHKk71TEk4efvhh/fznP0/FoQEAgOFSEk6qq6u1fPnyVBwaAAAYLuZw0tvbq61bt6qgoEAul0tdXV0Rbdrb27V69WplZ2fL6/Xq+PHj8agVV2DK6TgAABYj5nAyMzOjsrIyHThwIOr+zs5ONTc3a9euXRocHNTGjRtVU1Oj0dHRBRU4OzurQCAQtgAAgPQVczipqanR448/rm3btkXdv2/fPm3fvl07duxQSUmJ9u/fr8LCQh08eHBBBba1tSk3Nze0FBYWLqgfuws7K7I7N3WFAACQYHG95mRubk4DAwPy+Xxh230+n/r6+hbUZ2trq86dOxdaxsbG4lEqAAAwVGY8O5uamlIwGJTH4wnb7vF4NDExEVrfvHmz3njjDc3MzGjlypV66aWXVFFREbVPt9stt9sdzzIBAIDB4hpOLnK5XGHrlmWFbTt69GgiDgsAANJAXH/Wyc/PV0ZGRthZEkmanJyMOJsSK7/fr9LS0nnPsAAAgPQQ13CSlZUlr9ernp6esO09PT2qqqpaVN9NTU0aHh7WiRMnFtUPAAAwW8w/60xPT+vUqVOh9ZGREQ0NDSkvL0+rVq1SS0uL6urqVF5ersrKSh06dEijo6NqaGiIa+EAACA9xRxO+vv7VV1dHVpvaWmRJNXX16ujo0O1tbU6e/as9uzZo/Hxca1du1bd3d0qKipaVKF+v19+v1/BYHBR/QAAALPFHE42bdoky7Iu26axsVGNjY0LLiqapqYmNTU1KRAIKDeX53wAAJCuUvJuHQAAgPkQTgAAgFFsE064lRgAAGewTTjhVmIAAJzBNuEEAAA4A+EEAAAYhXACAACMYptwwgWx9rLu8LpUlwAAsCnbhBMuiAUAwBlsE04AAIAzEE4AAIBRCCcAAMAotgknXBCbPP6GV1JdwuLsTu8XQ9r+3wcArsA24YQLYgEAcAbbhBMAAOAMhBMAAGAUwgkAADAK4QQAABiFcAIAAIxim3DCrcQAACTe3totqS7BPuGEW4kBAHAG24QTAADgDIQTAABgFMIJAAAwCuEEAAAYhXACAACMQjgBAABGsU044TknsTtZXLLgz5pwn7ud+BteSXUJAJA2bBNOeM4JAADOYJtwAgAAnIFwAgAAjEI4AQAARiGcAAAAoxBOAACAUQgnAADAKIQTAABgFMIJAAAwCuEEAAAYhXACAACMYptw4th36+zO1brD667Y7GraRLOY9+8kCu+pAQBns0044d06AAA4g23CCQAAcAbCCQAAMArhBAAAGIVwAgAAjEI4AQAARiGcAAAAoxBOAACAUQgnAADAKIQTAABgFMIJAAAwCuEEAAAYhXACAACMQjgBAABGSUk4+fWvf601a9bos5/9rH72s5+logQAAGCozGQf8OOPP1ZLS4t+97vfKScnRxs2bNC2bduUl5eX7FIAAICBkn7m5PXXX9fnPvc53XjjjVq+fLnuvvtuHT16NNllAAAAQ8UcTnp7e7V161YVFBTI5XKpq6srok17e7tWr16t7Oxseb1eHT9+PLTvzJkzuvHGG0PrK1eu1Lvvvruw6gEAQNqJOZzMzMyorKxMBw4ciLq/s7NTzc3N2rVrlwYHB7Vx40bV1NRodHRUkmRZVsRnXC7XvMebnZ1VIBAIWwAAQPqKOZzU1NTo8ccf17Zt26Lu37dvn7Zv364dO3aopKRE+/fvV2FhoQ4ePChJuvHGG8POlJw+fVorVqyY93htbW3Kzc0NLYWFhbGW7Gh7a7ckpZ+TxSULPtYnP3eyuGRBfcRLqo9vV/GaZwAgxfmak7m5OQ0MDMjn84Vt9/l86uvrkyTdeuut+vOf/6x3331X58+fV3d3tzZv3jxvn62trTp37lxoGRsbi2fJAADAMHG9W2dqakrBYFAejydsu8fj0cTExH8OmJmpvXv3qrq6WhcuXNCjjz6q6667bt4+3W633G53PMsEAAAGS8itxJdeQ2JZVti2e+65R/fcc08iDg0AAGwurj/r5OfnKyMjI3SW5KLJycmIsymx8vv9Ki0tVUVFxaL6AQAAZotrOMnKypLX61VPT0/Y9p6eHlVVVS2q76amJg0PD+vEiROL6gcAAJgt5p91pqenderUqdD6yMiIhoaGlJeXp1WrVqmlpUV1dXUqLy9XZWWlDh06pNHRUTU0NMS1cAAAkJ5iDif9/f2qrq4Orbe0tEiS6uvr1dHRodraWp09e1Z79uzR+Pi41q5dq+7ubhUVFS2qUL/fL7/fr2AwuKh+AACA2WIOJ5s2bYr6ILVPamxsVGNj44KLiqapqUlNTU0KBALKzc2Na98AAMAcKXkrMQAAwHwIJwAAwCi2CSfcSgwAgDPYJpxwKzEAAM5gm3ACAACcgXACAACMQjgBAABGSciL/xLh4kPYPv74Y0lSIBBIzIFmLU0Hg/r33IwCgYAuzH6g4L+Dmg4G9eFHH/3nuFfZJvjv/2sjKW5tLh4/1Oa/nwtrI125n/+2uXQs/z03o9lP9H3psaKZr57ZK3xOUtjnPjmuC3bJv08soo3H1Vh0zYYe62pdaX4sRFzmAoCYXO1/txfiYp9XelaaJLmsq2llkNOnT6uwsDDVZQAAgAUYGxvTypUrL9vGduHkwoULOnPmjJYvXy6XyxW3fgOBgAoLCzU2NqacnJy49YtIjHVyMM7JwTgnD2OdHIkaZ8uydP78eRUUFGjJkstfVWKbn3UuWrJkyRUT12Lk5OQw6ZOEsU4Oxjk5GOfkYayTIxHjfLWvn+GCWAAAYBTCCQAAMArh5L/cbre+973vye12p7qUtMdYJwfjnByMc/Iw1slhwjjb7oJYAACQ3jhzAgAAjEI4AQAARiGcAAAAoxBOAACAUdI6nLS3t2v16tXKzs6W1+vV8ePHL9v+2LFj8nq9ys7O1k033aSnnnoqos2LL76o0tJSud1ulZaW6qWXXkpU+bYR73Hu6OiQy+WKWD788MNEfg3jxTLO4+PjeuCBB7RmzRotWbJEzc3NUdsxn6OL91gzp6OLZZx/+ctf6stf/rKuv/565eTkqLKyUkePHo1ox5yOFO9xTsp8ttLUCy+8YC1dutR6+umnreHhYWvnzp3WNddcY/3jH/+I2v7vf/+7tWzZMmvnzp3W8PCw9fTTT1tLly61fvGLX4Ta9PX1WRkZGdYTTzxhnTx50nriiSeszMxM67XXXkvW1zJOIsb52WeftXJycqzx8fGwxcliHeeRkRHr4Ycftg4fPmytX7/e2rlzZ0Qb5nN0iRhr5nSkWMd5586d1o9+9CPr9ddft9566y2rtbXVWrp0qfXGG2+E2jCnIyVinJMxn9M2nNx6661WQ0ND2Lbi4mLrsccei9r+0UcftYqLi8O2fetb37Juv/320Pr9999vfeUrXwlrs3nzZuvrX/96nKq2n0SM87PPPmvl5ubGvVY7i3WcP+muu+6K+geT+RxdIsaaOR1pMeN8UWlpqfX9738/tM6cjpSIcU7GfE7Ln3Xm5uY0MDAgn88Xtt3n86mvry/qZ/7whz9EtN+8ebP6+/v10UcfXbbNfH2mu0SNsyRNT0+rqKhIK1eu1JYtWzQ4OBj/L2ATCxnnq8F8jpSosZaY058Uj3G+cOGCzp8/r7y8vNA25nS4RI2zlPj5nJbhZGpqSsFgUB6PJ2y7x+PRxMRE1M9MTExEbf/xxx9ramrqsm3m6zPdJWqci4uL1dHRoV/96ld6/vnnlZ2drTvuuENvv/12Yr6I4RYyzleD+RwpUWPNnA4Xj3Heu3evZmZmdP/994e2MafDJWqckzGfbfdW4li4XK6wdcuyIrZdqf2l22Pt0wniPc633367br/99tD+O+64Qxs2bND//M//6Kc//Wm8yradRMw95nN08R4X5nR0Cx3n559/Xrt379bLL7+sG264IS59prN4j3My5nNahpP8/HxlZGREJMPJycmIBHnRpz/96ajtMzMzdd111122zXx9prtEjfOllixZooqKCsf+X+ZCxvlqMJ8jJWqsL8WcXvg4d3Z2avv27Tpy5Ii+9KUvhe1jTodL1DhfKhHzOS1/1snKypLX61VPT0/Y9p6eHlVVVUX9TGVlZUT73/72tyovL9fSpUsv22a+PtNdosb5UpZlaWhoSCtWrIhP4TazkHG+GsznSIka60sxpxc2zs8//7weeughPffcc/rqV78asZ85HS5R43yphMznhF5um0IXb5965plnrOHhYau5udm65pprrHfeeceyLMt67LHHrLq6ulD7i7e4fuc737GGh4etZ555JuIW11dffdXKyMiwnnzySevkyZPWk08+yW1qCRjn3bt3W7/5zW+sv/3tb9bg4KD1jW98w8rMzLT++Mc/Jv37mSLWcbYsyxocHLQGBwctr9drPfDAA9bg4KD1l7/8JbSf+RxdIsaaOR0p1nF+7rnnrMzMTMvv94fdvvr++++H2jCnIyVinJMxn9M2nFiWZfn9fquoqMjKysqyNmzYYB07diy0r76+3rrrrrvC2v/+97+3Pv/5z1tZWVnWZz7zGevgwYMRfR45csRas2aNtXTpUqu4uNh68cUXE/01jBfvcW5ubrZWrVplZWVlWddff73l8/msvr6+ZHwVo8U6zpIilqKiorA2zOfo4j3WzOnoYhnnu+66K+o419fXh/XJnI4U73FOxnx2WdZ/r0YEAAAwQFpecwIAAOyLcAIAAIxCOAEAAEYhnAAAAKMQTgAAgFEIJwAAwCiEEwAAYBTCCQAAMArhBAAAGIVwAgAAjEI4AQAARiGcAAAAo/x/GvLYSb0wjpoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_ = plt.hist(gaps, bins = 100, log = True)\n",
    "# plt.xscale('log')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env [~/.conda/envs/torch-env/]",
   "language": "python",
   "name": "conda_torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
