{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2f9aec6-9c5f-4963-9c2f-07bedf39d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64cf4453-1788-444c-b270-de42224f90d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Validation loss: 2.8789, trained on ~59.7B FineWeb-Edu tokens.\n",
    "This is a base model and can frequently hallucinate.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "from torch import nn\n",
    "\n",
    "from safetensors import safe_open\n",
    "\n",
    "from model_550m import FlashSTU, FlashSTUConfig, get_spectral_filters\n",
    "\n",
    "\n",
    "CHECKPOINT_PATH = \"../models/model_step-114000.safetensors\"\n",
    "CONFIG_PATH = \"../models/config_2-7b.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bbcc1b9-4ebe-4ac9-a3ce-79dca34de58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_compile(model: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Apply torch.compile to each layer. This makes compilation efficient\n",
    "    due to repeated structure. Alternatively, one can just compile the whole model.\n",
    "    \"\"\"\n",
    "    print(f\"Compiling each {model.__class__.__name__} layer with torch.compile...\")\n",
    "    start = time.perf_counter()\n",
    "    for idx, layer in model.layers.named_children():\n",
    "        compiled_layer = torch.compile(layer, mode=\"max-autotune\", fullgraph=True)\n",
    "        model.layers.register_module(idx, compiled_layer)\n",
    "    end = time.perf_counter()\n",
    "    print(f\"Finished compiling each {model.__class__.__name__} layer in {end - start:.4f} seconds.\")\n",
    "\n",
    "\n",
    "def load_stu_model(config_path: str, checkpoint_path: str, device: torch.device):\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config_data = json.load(f)\n",
    "\n",
    "    dim = config_data[\"dim\"]\n",
    "    num_heads = config_data[\"num_heads\"]\n",
    "    num_layers = config_data[\"num_layers\"]\n",
    "    num_eigh = config_data[\"num_eigh\"]\n",
    "    seq_len = config_data[\"seq_len\"]\n",
    "    use_hankel_L = config_data[\"use_hankel_L\"]\n",
    "    window_size = config_data[\"window_size\"]\n",
    "    vocab_size = config_data[\"vocab_size\"]\n",
    "    mlp_scale = config_data[\"mlp_scale\"]\n",
    "    bias = config_data[\"bias\"]\n",
    "    dropout = config_data[\"dropout\"]\n",
    "    softcap = config_data[\"softcap\"]\n",
    "    theta = config_data[\"theta\"]\n",
    "    torch_compile = config_data[\"torch_compile\"]\n",
    "    torch_dtype = getattr(torch, config_data[\"torch_dtype\"])\n",
    "\n",
    "    model_config = FlashSTUConfig(\n",
    "        dim=dim,\n",
    "        num_heads=num_heads,\n",
    "        num_layers=num_layers,\n",
    "        seq_len=seq_len,\n",
    "        window_size=window_size,\n",
    "        vocab_size=vocab_size,\n",
    "        mlp_scale=mlp_scale,\n",
    "        bias=bias,\n",
    "        dropout=dropout,\n",
    "        softcap=softcap,\n",
    "        theta=theta,\n",
    "        torch_dtype=torch_dtype,\n",
    "    )\n",
    "\n",
    "    spectral_filters = get_spectral_filters(seq_len, num_eigh, use_hankel_L, device, torch_dtype)\n",
    "    model = FlashSTU(model_config, spectral_filters)\n",
    "    model = model.to(device=device, dtype=torch_dtype)\n",
    "\n",
    "    print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "    state_dict = {}\n",
    "    start_time = time()\n",
    "\n",
    "    if checkpoint_path.endswith(\".safetensors\"):\n",
    "        with safe_open(checkpoint_path, framework=\"pt\", device=device.type) as f:\n",
    "            for k in f.keys():\n",
    "                state_dict[k] = f.get_tensor(k)\n",
    "    elif checkpoint_path.endswith(\".pt\"):\n",
    "        state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported checkpoint format: {checkpoint_path}\")\n",
    "    print(f\"Checkpoint loaded in {time() - start_time:.2f} seconds.\")\n",
    "\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    print(\"Model weights loaded successfully!\")\n",
    "\n",
    "    if torch_compile:\n",
    "        model = apply_compile(model)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return model, config_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43c051ad-0d01-4b01-9715-f5f6b3959d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    num_return_sequences=1,\n",
    "    max_length=512,\n",
    "    device=\"cuda\",\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate text from the given prompt using top-k sampling.\n",
    "\n",
    "    Args:\n",
    "        model: The FlashSTU model instance.\n",
    "        tokenizer: The tokenizer used for encoding/decoding.\n",
    "        prompt (str): Input prompt text.\n",
    "        num_return_sequences (int): How many sequences to return.\n",
    "        max_length (int): Maximum length of generated tokens.\n",
    "        device: torch device.\n",
    "        temperature (float): Sampling temperature. Higher = more random.\n",
    "        top_k (int): Top-K sampling parameter.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of generated text sequences.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Encode prompt tokens.\n",
    "    tokens = torch.tensor(\n",
    "        [tokenizer.encode(prompt, allowed_special={\"<|endoftext|>\"})],\n",
    "        device=device,\n",
    "    )\n",
    "    tokens = tokens.repeat(num_return_sequences, 1)\n",
    "\n",
    "    sample_rng = torch.Generator(device=device)\n",
    "    sample_rng.manual_seed(1746)\n",
    "\n",
    "    eos_token_id = tokenizer.encode(\n",
    "        \"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}\n",
    "    )[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - tokens.size(1)):\n",
    "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                # Fwd pass. Inspect logits here.\n",
    "                logits = model(tokens)     # shape: [batch, seq, vocab]\n",
    "                logits = logits[:, -1, :]  # last token logits\n",
    "\n",
    "                # Apply temperature scaling.\n",
    "                if temperature > 0:\n",
    "                    logits = logits / temperature\n",
    "\n",
    "            # Compute probabilities.\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Top-K sampling.\n",
    "            top_k_probs, top_k_indices = torch.topk(probs, top_k, dim=-1)\n",
    "            ix = torch.multinomial(top_k_probs, 1, generator=sample_rng)\n",
    "            next_token = torch.gather(top_k_indices, -1, ix)\n",
    "\n",
    "            # Append next token.\n",
    "            tokens = torch.cat((tokens, next_token), dim=1)\n",
    "\n",
    "            # Stop if EOS token is generated.\n",
    "            if (next_token == eos_token_id).any():\n",
    "                break\n",
    "\n",
    "    # Decode all sequences.\n",
    "    generated_sequences = []\n",
    "    for i in range(num_return_sequences):\n",
    "        decoded = tokenizer.decode(tokens[i].tolist())\n",
    "        generated_sequences.append(decoded)\n",
    "\n",
    "    return generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c729eacd-b0f4-4464-b578-897dc212ca38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tiktoken.load import load_tiktoken_bpe\n",
    "bpe_path = \"../models/o200k_base.tiktoken\"\n",
    "bpe_dict = load_tiktoken_bpe(bpe_path)\n",
    "\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    name=\"o200k_base\",  # Name of the encoding\n",
    "    pat_str=r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+\"\"\",\n",
    "    mergeable_ranks=bpe_dict,\n",
    "    special_tokens={\n",
    "        \"<|endoftext|>\": 199999,  # Custom special token example (modify as needed)\n",
    "        \"<|endofprompt|>\": 200018,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88070375-8af1-4a23-90b0-d0f70644c191",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'STU' object has no attribute 'M_filters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load model and config.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model, config_data \u001b[38;5;241m=\u001b[39m load_stu_model(CONFIG_PATH, CHECKPOINT_PATH, device)\n",
      "Cell \u001b[0;32mIn[14], line 51\u001b[0m, in \u001b[0;36mload_stu_model\u001b[0;34m(config_path, checkpoint_path, device)\u001b[0m\n\u001b[1;32m     35\u001b[0m model_config \u001b[38;5;241m=\u001b[39m FlashSTUConfig(\n\u001b[1;32m     36\u001b[0m     dim\u001b[38;5;241m=\u001b[39mdim,\n\u001b[1;32m     37\u001b[0m     num_heads\u001b[38;5;241m=\u001b[39mnum_heads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     50\u001b[0m spectral_filters \u001b[38;5;241m=\u001b[39m get_spectral_filters(seq_len, num_eigh, use_hankel_L, device, torch_dtype)\n\u001b[0;32m---> 51\u001b[0m model \u001b[38;5;241m=\u001b[39m FlashSTU(model_config, spectral_filters)\n\u001b[1;32m     52\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch_dtype)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading checkpoint from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/stu_distill/src/model_550m.py:505\u001b[0m, in \u001b[0;36mFlashSTU.__init__\u001b[0;34m(self, config, filters)\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtok_emb\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mweight\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mdim \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m--> 505\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_weights)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Parameter Count: \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124mM\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_num_params() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1e6\u001b[39m,))\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1029\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \n\u001b[1;32m    995\u001b[0m \u001b[38;5;124;03mTypical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \n\u001b[1;32m   1027\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 1029\u001b[0m     module\u001b[38;5;241m.\u001b[39mapply(fn)\n\u001b[1;32m   1030\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1029\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \n\u001b[1;32m    995\u001b[0m \u001b[38;5;124;03mTypical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \n\u001b[1;32m   1027\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 1029\u001b[0m     module\u001b[38;5;241m.\u001b[39mapply(fn)\n\u001b[1;32m   1030\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1029\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \n\u001b[1;32m    995\u001b[0m \u001b[38;5;124;03mTypical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \n\u001b[1;32m   1027\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 1029\u001b[0m     module\u001b[38;5;241m.\u001b[39mapply(fn)\n\u001b[1;32m   1030\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1030\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m   1029\u001b[0m     module\u001b[38;5;241m.\u001b[39mapply(fn)\n\u001b[0;32m-> 1030\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/stu_distill/src/model_550m.py:546\u001b[0m, in \u001b[0;36mFlashSTU._init_weights\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_approx:\n\u001b[1;32m    545\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mxavier_normal_(module\u001b[38;5;241m.\u001b[39mM_inputs)\n\u001b[0;32m--> 546\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mxavier_normal_(module\u001b[38;5;241m.\u001b[39mM_filters)\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    548\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mxavier_normal_(module\u001b[38;5;241m.\u001b[39mM_phi_plus)\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'STU' object has no attribute 'M_filters'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Load model and config.\n",
    "model, config_data = load_stu_model(CONFIG_PATH, CHECKPOINT_PATH, device)\n",
    "# tokenizer = tiktoken.get_encoding(\"o200k_base\") #need to cache this for offline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d95e05c-8320-4061-85c8-3a62b9fee437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect prompt(s) from user.\n",
    "prompts = []\n",
    "while True:\n",
    "    prompt = input(\"Enter a prompt (press enter with no text to finish): \")\n",
    "    if not prompt:\n",
    "        break\n",
    "    prompts.append(prompt.strip())\n",
    "\n",
    "    if len(prompts) == 0:\n",
    "        print(\"No prompts provided. Exiting.\")\n",
    "        break\n",
    "    \n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# BASE SETTINGS:\n",
    "BASE_TEMPERATURE = 0.7  # Increase for more randomness.\n",
    "BASE_TOP_K = 50         # Limit sampling to the top k tokens.\n",
    "MAX_LENGTH = 512        # Maximum number of tokens to generate.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "total_tokens = 0\n",
    "start_time = time()\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"Generating text for prompt {i}: {prompt}\")\n",
    "    generated_texts = generate_text(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        num_return_sequences=1,\n",
    "        max_length=MAX_LENGTH,\n",
    "        device=device,\n",
    "        temperature=BASE_TEMPERATURE,\n",
    "        top_k=BASE_TOP_K,\n",
    "    )\n",
    "    for gen_text in generated_texts:\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(f\"Generated Text: {gen_text}\\n\")\n",
    "        total_tokens += len(\n",
    "            tokenizer.encode(gen_text, allowed_special={\"<|endoftext|>\"})\n",
    "        )\n",
    "end_time = time()\n",
    "tokens_per_second = total_tokens / (end_time - start_time)\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfaa8e32-7563-4f05-9397-41189dde1282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "for i in range(0, 12, 2):\n",
    "    stu_layer = copy.deepcopy(model.layers[i].stu)\n",
    "    # stu_layer.stu_filters = stu_layer.stu_filters.to(torch.float)\n",
    "    # stu_layer.M_inputs = stu_layer.M_inputs.to(torch.float)\n",
    "    # torch.save(stu_layer.state_dict(), f\"../stu_layers/stu_layer_{i}_60Btokens_param.pt\")\n",
    "    torch.save(stu_layer, f\"../stu_layers/stu_layer_{i}_550m_param_full.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37089918-7635-401a-9e9e-21f56d9650e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 03:40:50,004 - INFO - Found 1 shards for split val\n"
     ]
    }
   ],
   "source": [
    "from torch.amp import autocast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import tqdm \n",
    "\n",
    "from dataloader import DataLoader\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    bsz=1,\n",
    "    seq_len=512, \n",
    "    dataset='../fineweb-edu', \n",
    "    split=\"val\", \n",
    "    main_process=True,\n",
    ")\n",
    "\n",
    "def evaluate(model, val_steps = 5):\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    val_loss = 0.0\n",
    "    torch_dtype = getattr(torch, 'bfloat16')\n",
    "    # val_steps = 5 # Abitrarily set to reduce long evaluations, >20 typically used\n",
    "    model.eval()\n",
    "    val_loader.reset()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in tqdm.tqdm(zip(range(val_steps), val_loader, strict=False)):\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            if torch_dtype != torch.float32:\n",
    "                with autocast(device_type=device.type, dtype=torch_dtype, cache_enabled=True):\n",
    "                    preds = model(inputs)\n",
    "            else:\n",
    "                preds = model(inputs)\n",
    "\n",
    "            loss = loss_fn(preds.flatten(0, 1), targets.flatten(0, 1))\n",
    "            loss = loss / val_steps\n",
    "            val_loss += loss.detach().float()\n",
    "    return(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb66849b-1e17-4bef-a699-c5bf73eba431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:05,  9.68it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.9617, device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, val_steps = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c530ca28-ef04-473b-8717-3298726184bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_684723/4049127919.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f\"lds_layer_{layer}_10k_5.pth\")\n"
     ]
    }
   ],
   "source": [
    "from lds import LDS\n",
    "\n",
    "lds_layers = []\n",
    "for layer in [0,2,4,6,8,10]:\n",
    "    lds = LDS(10000, 896, 896, 5).to(device)\n",
    "    state_dict = torch.load(f\"lds_layer_{layer}_10k_5.pth\")\n",
    "    lds.load_state_dict(state_dict)\n",
    "    lds_layers.append(lds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "136aa8e2-3f4f-4161-9e6b-b992c720c9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    model.layers[i].stu = lds_layers[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ed6f2b38-5172-40ca-a3c7-a653555957f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:07,  6.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.9656, device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, val_steps = 50)\n",
    "\n",
    "#layer 1 is a success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a3a0148e-7b24-40b5-8a86-30df710ff08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlashSTU(\n",
       "  (tok_emb): Embedding(200064, 896)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): STULayer(\n",
       "      (stu_norm): RMSNorm((896,), eps=None, elementwise_affine=True)\n",
       "      (stu): LDS()\n",
       "      (mlp_norm): RMSNorm((896,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=896, out_features=10752, bias=False)\n",
       "        (up_proj): Linear(in_features=896, out_features=10752, bias=False)\n",
       "        (down_proj): Linear(in_features=10752, out_features=896, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): AttentionLayer(\n",
       "      (attn_norm): RMSNorm((896,), eps=None, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Linear(in_features=896, out_features=2688, bias=False)\n",
       "        (c_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (mlp_norm): RMSNorm((896,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=896, out_features=10752, bias=False)\n",
       "        (up_proj): Linear(in_features=896, out_features=10752, bias=False)\n",
       "        (down_proj): Linear(in_features=10752, out_features=896, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): STULayer(\n",
       "      (stu_norm): RMSNorm((896,), eps=None, elementwise_affine=True)\n",
       "      (stu): STU(\n",
       "        (flash_fft): FlashFFTConv()\n",
       "      )\n",
       "      (mlp_norm): RMSNorm((896,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=896, out_features=10752, bias=False)\n",
       "        (up_proj): Linear(in_features=896, out_features=10752, bias=False)\n",
       "        (down_proj): Linear(in_features=10752, out_features=896, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): AttentionLayer(\n",
       "      (attn_norm): RMSNorm((896,), eps=None, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Linear(in_features=896, out_features=2688, bias=False)\n",
       "        (c_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (mlp_norm): RMSNorm((896,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=896, out_features=10752, bias=False)\n",
       "        (up_proj): Linear(in_features=896, out_features=10752, bias=False)\n",
       "        (down_proj): Linear(in_features=10752, out_features=896, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): STULayer(\n",
       "      (stu_norm): RMSNorm((896,), eps=None, elementwise_affine=True)\n",
       "      (stu): STU(\n",
       "        (flash_fft): FlashFFTConv()\n",
       "      )\n",
       "      (mlp_norm): RMSNorm((896,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=896, out_features=10752, bias=False)\n",
       "        (up_proj): Linear(in_features=896, out_features=10752, bias=False)\n",
       "        (down_proj): Linear(in_features=10752, out_features=896, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): AttentionLayer(\n",
       "      (attn_norm): RMSNorm((896,), eps=None, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Linear(in_features=896, out_features=2688, bias=False)\n",
       "        (c_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (mlp_norm): RMSNorm((896,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=896, out_features=10752, bias=False)\n",
       "        (up_proj): Linear(in_features=896, out_features=10752, bias=False)\n",
       "        (down_proj): Linear(in_features=10752, out_features=896, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): STULayer(\n",
       "      (stu_norm): RMSNorm((896,), eps=None, elementwise_affine=True)\n",
       "      (stu): STU(\n",
       "        (flash_fft): FlashFFTConv()\n",
       "      )\n",
       "      (mlp_norm): RMSNorm((896,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=896, out_features=10752, bias=False)\n",
       "        (up_proj): Linear(in_features=896, out_features=10752, bias=False)\n",
       "        (down_proj): Linear(in_features=10752, out_features=896, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): AttentionLayer(\n",
       "      (attn_norm): RMSNorm((896,), eps=None, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Linear(in_features=896, out_features=2688, bias=False)\n",
       "        (c_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (mlp_norm): RMSNorm((896,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=896, out_features=10752, bias=False)\n",
       "        (up_proj): Linear(in_features=896, out_features=10752, bias=False)\n",
       "        (down_proj): Linear(in_features=10752, out_features=896, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): STULayer(\n",
       "      (stu_norm): RMSNorm((896,), eps=None, elementwise_affine=True)\n",
       "      (stu): STU(\n",
       "        (flash_fft): FlashFFTConv()\n",
       "      )\n",
       "      (mlp_norm): RMSNorm((896,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=896, out_features=10752, bias=False)\n",
       "        (up_proj): Linear(in_features=896, out_features=10752, bias=False)\n",
       "        (down_proj): Linear(in_features=10752, out_features=896, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): AttentionLayer(\n",
       "      (attn_norm): RMSNorm((896,), eps=None, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Linear(in_features=896, out_features=2688, bias=False)\n",
       "        (c_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (mlp_norm): RMSNorm((896,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=896, out_features=10752, bias=False)\n",
       "        (up_proj): Linear(in_features=896, out_features=10752, bias=False)\n",
       "        (down_proj): Linear(in_features=10752, out_features=896, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): STULayer(\n",
       "      (stu_norm): RMSNorm((896,), eps=None, elementwise_affine=True)\n",
       "      (stu): STU(\n",
       "        (flash_fft): FlashFFTConv()\n",
       "      )\n",
       "      (mlp_norm): RMSNorm((896,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=896, out_features=10752, bias=False)\n",
       "        (up_proj): Linear(in_features=896, out_features=10752, bias=False)\n",
       "        (down_proj): Linear(in_features=10752, out_features=896, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): AttentionLayer(\n",
       "      (attn_norm): RMSNorm((896,), eps=None, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Linear(in_features=896, out_features=2688, bias=False)\n",
       "        (c_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (mlp_norm): RMSNorm((896,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=896, out_features=10752, bias=False)\n",
       "        (up_proj): Linear(in_features=896, out_features=10752, bias=False)\n",
       "        (down_proj): Linear(in_features=10752, out_features=896, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm((896,), eps=None, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=896, out_features=200064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d94e0-d556-4860-832b-20d524d90e52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env [~/.conda/envs/torch-env/]",
   "language": "python",
   "name": "conda_torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
