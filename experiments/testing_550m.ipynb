{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f9aec6-9c5f-4963-9c2f-07bedf39d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64cf4453-1788-444c-b270-de42224f90d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ds6237/.conda/envs/torch-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Validation loss: 2.8789, trained on ~59.7B FineWeb-Edu tokens.\n",
    "This is a base model and can frequently hallucinate.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "from torch import nn\n",
    "\n",
    "from safetensors import safe_open\n",
    "\n",
    "from model_550m import FlashSTU, FlashSTUConfig, get_spectral_filters\n",
    "\n",
    "\n",
    "CHECKPOINT_PATH = \"../models/model_step-114000.safetensors\"\n",
    "CONFIG_PATH = \"../models/config_2-7b.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bbcc1b9-4ebe-4ac9-a3ce-79dca34de58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_compile(model: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Apply torch.compile to each layer. This makes compilation efficient\n",
    "    due to repeated structure. Alternatively, one can just compile the whole model.\n",
    "    \"\"\"\n",
    "    print(f\"Compiling each {model.__class__.__name__} layer with torch.compile...\")\n",
    "    start = time.perf_counter()\n",
    "    for idx, layer in model.layers.named_children():\n",
    "        compiled_layer = torch.compile(layer, mode=\"max-autotune\", fullgraph=True)\n",
    "        model.layers.register_module(idx, compiled_layer)\n",
    "    end = time.perf_counter()\n",
    "    print(f\"Finished compiling each {model.__class__.__name__} layer in {end - start:.4f} seconds.\")\n",
    "\n",
    "\n",
    "def load_stu_model(config_path: str, checkpoint_path: str, device: torch.device):\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config_data = json.load(f)\n",
    "\n",
    "    dim = config_data[\"dim\"]\n",
    "    num_heads = config_data[\"num_heads\"]\n",
    "    num_layers = config_data[\"num_layers\"]\n",
    "    num_eigh = config_data[\"num_eigh\"]\n",
    "    seq_len = config_data[\"seq_len\"]\n",
    "    use_hankel_L = config_data[\"use_hankel_L\"]\n",
    "    window_size = config_data[\"window_size\"]\n",
    "    vocab_size = config_data[\"vocab_size\"]\n",
    "    mlp_scale = config_data[\"mlp_scale\"]\n",
    "    bias = config_data[\"bias\"]\n",
    "    dropout = config_data[\"dropout\"]\n",
    "    softcap = config_data[\"softcap\"]\n",
    "    theta = config_data[\"theta\"]\n",
    "    torch_compile = config_data[\"torch_compile\"]\n",
    "    torch_dtype = getattr(torch, config_data[\"torch_dtype\"])\n",
    "\n",
    "    model_config = FlashSTUConfig(\n",
    "        dim=dim,\n",
    "        num_heads=num_heads,\n",
    "        num_layers=num_layers,\n",
    "        seq_len=seq_len,\n",
    "        window_size=window_size,\n",
    "        vocab_size=vocab_size,\n",
    "        mlp_scale=mlp_scale,\n",
    "        bias=bias,\n",
    "        dropout=dropout,\n",
    "        softcap=softcap,\n",
    "        theta=theta,\n",
    "        torch_dtype=torch_dtype,\n",
    "    )\n",
    "\n",
    "    spectral_filters = get_spectral_filters(seq_len, num_eigh, use_hankel_L, device, torch_dtype)\n",
    "    model = FlashSTU(model_config, spectral_filters)\n",
    "    model = model.to(device=device, dtype=torch_dtype)\n",
    "\n",
    "    print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "    state_dict = {}\n",
    "    start_time = time()\n",
    "\n",
    "    if checkpoint_path.endswith(\".safetensors\"):\n",
    "        with safe_open(checkpoint_path, framework=\"pt\", device=device.type) as f:\n",
    "            for k in f.keys():\n",
    "                state_dict[k] = f.get_tensor(k)\n",
    "    elif checkpoint_path.endswith(\".pt\"):\n",
    "        state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported checkpoint format: {checkpoint_path}\")\n",
    "    print(f\"Checkpoint loaded in {time() - start_time:.2f} seconds.\")\n",
    "\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    print(\"Model weights loaded successfully!\")\n",
    "\n",
    "    if torch_compile:\n",
    "        model = apply_compile(model)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return model, config_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43c051ad-0d01-4b01-9715-f5f6b3959d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    num_return_sequences=1,\n",
    "    max_length=512,\n",
    "    device=\"cuda\",\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate text from the given prompt using top-k sampling.\n",
    "\n",
    "    Args:\n",
    "        model: The FlashSTU model instance.\n",
    "        tokenizer: The tokenizer used for encoding/decoding.\n",
    "        prompt (str): Input prompt text.\n",
    "        num_return_sequences (int): How many sequences to return.\n",
    "        max_length (int): Maximum length of generated tokens.\n",
    "        device: torch device.\n",
    "        temperature (float): Sampling temperature. Higher = more random.\n",
    "        top_k (int): Top-K sampling parameter.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of generated text sequences.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Encode prompt tokens.\n",
    "    tokens = torch.tensor(\n",
    "        [tokenizer.encode(prompt, allowed_special={\"<|endoftext|>\"})],\n",
    "        device=device,\n",
    "    )\n",
    "    tokens = tokens.repeat(num_return_sequences, 1)\n",
    "\n",
    "    sample_rng = torch.Generator(device=device)\n",
    "    sample_rng.manual_seed(1746)\n",
    "\n",
    "    eos_token_id = tokenizer.encode(\n",
    "        \"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}\n",
    "    )[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - tokens.size(1)):\n",
    "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                # Fwd pass. Inspect logits here.\n",
    "                logits = model(tokens)     # shape: [batch, seq, vocab]\n",
    "                logits = logits[:, -1, :]  # last token logits\n",
    "\n",
    "                # Apply temperature scaling.\n",
    "                if temperature > 0:\n",
    "                    logits = logits / temperature\n",
    "\n",
    "            # Compute probabilities.\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Top-K sampling.\n",
    "            top_k_probs, top_k_indices = torch.topk(probs, top_k, dim=-1)\n",
    "            ix = torch.multinomial(top_k_probs, 1, generator=sample_rng)\n",
    "            next_token = torch.gather(top_k_indices, -1, ix)\n",
    "\n",
    "            # Append next token.\n",
    "            tokens = torch.cat((tokens, next_token), dim=1)\n",
    "\n",
    "            # Stop if EOS token is generated.\n",
    "            if (next_token == eos_token_id).any():\n",
    "                break\n",
    "\n",
    "    # Decode all sequences.\n",
    "    generated_sequences = []\n",
    "    for i in range(num_return_sequences):\n",
    "        decoded = tokenizer.decode(tokens[i].tolist())\n",
    "        generated_sequences.append(decoded)\n",
    "\n",
    "    return generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c729eacd-b0f4-4464-b578-897dc212ca38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tiktoken.load import load_tiktoken_bpe\n",
    "bpe_path = \"../models/o200k_base.tiktoken\"\n",
    "bpe_dict = load_tiktoken_bpe(bpe_path)\n",
    "\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    name=\"o200k_base\",  # Name of the encoding\n",
    "    pat_str=r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+\"\"\",\n",
    "    mergeable_ranks=bpe_dict,\n",
    "    special_tokens={\n",
    "        \"<|endoftext|>\": 199999,  # Custom special token example (modify as needed)\n",
    "        \"<|endofprompt|>\": 200018,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88070375-8af1-4a23-90b0-d0f70644c191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameter Count: 550.31M\n",
      "\n",
      "Loading checkpoint from ../models/model_step-114000.safetensors...\n",
      "Checkpoint loaded in 0.23 seconds.\n",
      "Model weights loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Load model and config.\n",
    "model, config_data = load_stu_model(CONFIG_PATH, CHECKPOINT_PATH, device)\n",
    "# tokenizer = tiktoken.get_encoding(\"o200k_base\") #need to cache this for offline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d95e05c-8320-4061-85c8-3a62b9fee437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (press enter with no text to finish):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens per second: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Collect prompt(s) from user.\n",
    "prompts = []\n",
    "while True:\n",
    "    prompt = input(\"Enter a prompt (press enter with no text to finish): \")\n",
    "    if not prompt:\n",
    "        break\n",
    "    prompts.append(prompt.strip())\n",
    "\n",
    "    if len(prompts) == 0:\n",
    "        print(\"No prompts provided. Exiting.\")\n",
    "        break\n",
    "    \n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# BASE SETTINGS:\n",
    "BASE_TEMPERATURE = 0.7  # Increase for more randomness.\n",
    "BASE_TOP_K = 50         # Limit sampling to the top k tokens.\n",
    "MAX_LENGTH = 512        # Maximum number of tokens to generate.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "total_tokens = 0\n",
    "start_time = time()\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"Generating text for prompt {i}: {prompt}\")\n",
    "    generated_texts = generate_text(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        num_return_sequences=1,\n",
    "        max_length=MAX_LENGTH,\n",
    "        device=device,\n",
    "        temperature=BASE_TEMPERATURE,\n",
    "        top_k=BASE_TOP_K,\n",
    "    )\n",
    "    for gen_text in generated_texts:\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(f\"Generated Text: {gen_text}\\n\")\n",
    "        total_tokens += len(\n",
    "            tokenizer.encode(gen_text, allowed_special={\"<|endoftext|>\"})\n",
    "        )\n",
    "end_time = time()\n",
    "tokens_per_second = total_tokens / (end_time - start_time)\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "629d4aa7-0948-45c3-856d-1b9ebc29a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33266bc3-0590-48cc-8d8b-5ac0c3e046cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(\"./convex_hull\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37089918-7635-401a-9e9e-21f56d9650e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import autocast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import tqdm \n",
    "\n",
    "from dataloader import DataLoader\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    bsz=1,\n",
    "    seq_len= 256, #USUALLY 512\n",
    "    dataset='../fineweb-edu', \n",
    "    split=\"val\", \n",
    "    main_process=True,\n",
    ")\n",
    "\n",
    "def evaluate(model, val_steps = 5):\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    val_loss = 0.0\n",
    "    torch_dtype = getattr(torch, 'bfloat16')\n",
    "    # val_steps = 5 # Abitrarily set to reduce long evaluations, >20 typically used\n",
    "    model.eval()\n",
    "    val_loader.reset()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in tqdm.tqdm(zip(range(val_steps), val_loader, strict=False)):\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            if torch_dtype != torch.float32:\n",
    "                with autocast(device_type=device.type, dtype=torch_dtype, cache_enabled=True):\n",
    "                    preds = model(inputs)\n",
    "            else:\n",
    "                preds = model(inputs)\n",
    "\n",
    "            loss = loss_fn(preds.flatten(0, 1), targets.flatten(0, 1))\n",
    "            loss = loss / val_steps\n",
    "            val_loss += loss.detach().float()\n",
    "    return(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb66849b-1e17-4bef-a699-c5bf73eba431",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:04, 11.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.3545, device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, val_steps = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66ab57d6-201a-4e2b-8005-101a47b8c2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from full_fast_stu import lds_phi, FullFastSTU\n",
    "\n",
    "for idx in range(0, 12, 2):\n",
    "    stu_layer = copy.deepcopy(model.layers[idx].stu)\n",
    "    fast_layer = FullFastSTU(stu_layer.cuda(), lds_phi.cuda()).cuda()\n",
    "    model.layers[idx].stu = fast_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "443a9501-7811-4789-87d4-139467da9ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [02:38,  3.16s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.3530, device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, val_steps = 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env [~/.conda/envs/torch-env/]",
   "language": "python",
   "name": "conda_torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
