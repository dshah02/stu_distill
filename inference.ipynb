{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\devan\\anaconda3\\envs\\flashstu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to import FlashFFTConv: No module named 'flashfftconv'. Falling back to PyTorch implementation.\n",
      "Unable to import Triton-based MLP: No module named 'liger_kernel'. Falling back to vanilla SwiGLU MLP instead.\n",
      "Unable to import Triton-based RMSNorm: No module named 'liger_kernel'. Falling back to PyTorch implementation.\n",
      "Unable to import Triton-based MLP: No module named 'liger_kernel'. Falling back to vanilla SwiGLU MLP instead.\n",
      "Unable to import Triton-based RMSNorm: No module named 'liger_kernel'. Falling back to PyTorch implementation.\n",
      "Unable to import Triton-based RMSNorm: No module named 'liger_kernel'. Falling back to PyTorch implementation.\n",
      "Using device: cuda\n",
      "Loading the checkpoint...\n",
      "Successfully loaded the checkpoint in 1.35 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "import logging\n",
    "import json\n",
    "from time import time\n",
    "from safetensors import safe_open\n",
    "from model import FlashSTU\n",
    "from config import FlashSTUConfig\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "def get_hankel(seq_len: int, use_hankel_L: bool = False) -> np.ndarray:\n",
    "    entries = np.arange(1, seq_len + 1, dtype=np.float64)\n",
    "    i_plus_j = entries[:, None] + entries[None, :]\n",
    "\n",
    "    if use_hankel_L:\n",
    "        sgn = (-1.0) ** (i_plus_j - 2.0) + 1.0\n",
    "        denom = (i_plus_j + 3.0) * (i_plus_j - 1.0) * (i_plus_j + 1.0)\n",
    "        Z = sgn * (8.0 / denom)\n",
    "    elif not use_hankel_L:\n",
    "        Z = 2.0 / (i_plus_j**3 - i_plus_j)\n",
    "    else:\n",
    "        raise ValueError(\"use_hankel_L must be a boolean\")\n",
    "\n",
    "    return Z\n",
    "\n",
    "def get_spectral_filters(\n",
    "    seq_len: int, \n",
    "    K: int, \n",
    "    use_hankel_L: bool = False, \n",
    "    device: torch.device = None,\n",
    "    dtype: torch.dtype = torch.bfloat16,\n",
    ") -> torch.Tensor:\n",
    "    assert torch.cuda.is_available(), \"CUDA is required.\"\n",
    "    Z = get_hankel(seq_len, use_hankel_L)\n",
    "    sigma, phi = np.linalg.eigh(Z)\n",
    "    sigma_k, phi_k = sigma[-K:], phi[:, -K:]\n",
    "    phi_k *= sigma_k ** 0.25\n",
    "    filters = torch.from_numpy(phi_k)\n",
    "    return filters.to(device=device, dtype=dtype)\n",
    "\n",
    "# Load the checkpoint\n",
    "print(\"Loading the checkpoint...\")\n",
    "start_time = time()\n",
    "state_dict = {}\n",
    "with safe_open(\n",
    "    \"model_19073.safetensors\",\n",
    "    framework=\"pt\",\n",
    "    device=\"cuda\",\n",
    ") as f:\n",
    "    for k in f.keys():\n",
    "        state_dict[k] = f.get_tensor(k)\n",
    "\n",
    "print(f\"Successfully loaded the checkpoint in {time() - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameter Count: 426.28M\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set precision for matrix multiplication\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# Load model configurations from JSON file\n",
    "with open(\"config.json\", \"r\") as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "# Extract model configurations\n",
    "n_embd = config[\"n_embd\"]\n",
    "n_heads = config[\"n_heads\"]\n",
    "n_layers = config[\"n_layers\"]\n",
    "seq_len = config[\"seq_len\"]\n",
    "window_size = config[\"window_size\"]\n",
    "vocab_size = config[\"vocab_size\"]\n",
    "mlp_scale = config[\"mlp_scale\"]\n",
    "bias = config[\"bias\"]\n",
    "dropout = config[\"dropout\"]\n",
    "num_eigh = config[\"num_eigh\"]\n",
    "use_hankel_L = config[\"use_hankel_L\"]\n",
    "use_flash_fft = config[\"use_flash_fft\"]\n",
    "use_approx = config[\"use_approx\"]\n",
    "use_attn = config[\"use_attn\"]\n",
    "softcap = config[\"softcap\"]\n",
    "\n",
    "# Model setup\n",
    "config = FlashSTUConfig(\n",
    "    n_embd=n_embd,\n",
    "    n_heads=n_heads,\n",
    "    n_layers=n_layers,\n",
    "    seq_len=seq_len,\n",
    "    window_size=window_size,\n",
    "    vocab_size=vocab_size,\n",
    "    mlp_scale=mlp_scale,\n",
    "    bias=bias,\n",
    "    dropout=dropout,\n",
    "    num_eigh=num_eigh,\n",
    "    use_hankel_L=use_hankel_L,\n",
    "    use_flash_fft=use_flash_fft,\n",
    "    use_approx=use_approx,\n",
    "    use_attn=use_attn,\n",
    "    softcap=softcap,\n",
    "    torch_dtype=getattr(torch, config[\"torch_dtype\"]),\n",
    ")\n",
    "phi = get_spectral_filters(seq_len, num_eigh, use_hankel_L, device, torch.float32)\n",
    "model = FlashSTU(config, phi)\n",
    "\n",
    "# Load state dictionary into the model\n",
    "model.load_state_dict(state_dict, strict = True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Prepare tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"o200k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "def generate_text(\n",
    "    model, tokenizer, prompt, num_return_sequences=4, max_length=1024, device=\"cuda\", temperature=1.0, top_k=50\n",
    "):\n",
    "    model.eval()\n",
    "    tokens = torch.tensor([tokenizer.encode(prompt, allowed_special={\"<|endoftext|>\"})], device=device)\n",
    "    tokens = tokens.repeat(num_return_sequences, 1)\n",
    "\n",
    "    sample_rng = torch.Generator(device=device)\n",
    "    sample_rng.manual_seed(1337)\n",
    "\n",
    "    eos_token_id = tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm.tqdm(range(max_length - tokens.size(1))):\n",
    "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                logits = model(tokens)\n",
    "                logits = logits[:, -1, :]  # Get logits for the last token\n",
    "\n",
    "                # Apply temperature scaling if temperature > 0\n",
    "                if temperature > 0:\n",
    "                    logits = logits / temperature\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)  # Compute probabilities\n",
    "\n",
    "            # Top-K sampling: set all probabilities outside the top K to 0\n",
    "            top_k_probs, top_k_indices = torch.topk(probs, top_k, dim=-1)\n",
    "            ix = torch.multinomial(top_k_probs, 1, generator=sample_rng)\n",
    "            next_token = torch.gather(top_k_indices, -1, ix)\n",
    "            tokens = torch.cat((tokens, next_token), dim=1)\n",
    "\n",
    "            # Break if EOS token is generated\n",
    "            if (next_token == eos_token_id).any():\n",
    "                break\n",
    "\n",
    "    generated_sequences = []\n",
    "    for i in range(num_return_sequences):\n",
    "        decoded = tokenizer.decode(tokens[i].tolist())\n",
    "        generated_sequences.append(decoded)\n",
    "\n",
    "    return generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating text for prompt: 'The future of artificial intelligence is'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:06<00:00,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: The future of artificial intelligence is also a bit controversial in the political area. The idea of artificial intelligence is actually a part of people who might not have a clear idea of how can we make this technology come to life. People that already have computers can\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"The future of artificial intelligence is\",\n",
    "    # \"In the year 2050, the world will\",\n",
    "    # \"The most important scientific discovery of the 21st century is\",\n",
    "    # \"If I could change one thing about the education system, it would be\",\n",
    "    # \"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nGenerating text for prompt: '{prompt}'\\n\")\n",
    "    generated_texts = generate_text(model, tokenizer, prompt, num_return_sequences=1, max_length=50)\n",
    "    for i, text in enumerate(generated_texts):\n",
    "        print(f\"Sample {i + 1}: {text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "stu = copy.deepcopy(model.layers[0].stu)\n",
    "stu.phi = stu.phi.to(torch.bfloat16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, n_layers, 2):\n",
    "#     stu_layer = copy.deepcopy(model.layers[i].stu)\n",
    "#     stu_layer.phi = stu_layer.phi.to(torch.bfloat16)\n",
    "#     torch.save(stu_layer.state_dict(), f\"./stu_layer_{i}_500m_param.pt\")\n",
    "#     torch.save(stu_layer, f\"./stu_layer_{i}_500m_param_full.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the full STU layer\n",
    "# stu_layer_full = torch.load(\"./stu_layers/stu_layer_0_500m_param_full.pt\")\n",
    "# stu_layer_full.to(device)s\n",
    "\n",
    "# # Prepare some input data\n",
    "# inputs = torch.randn(5, 1024, 768).to(device).to(torch.bfloat16)\n",
    "\n",
    "# # Run the STU layer\n",
    "# outputs = stu_layer_full(inputs)\n",
    "# print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the saved STU layer\n",
    "# stu_larch.load(\"./stu_layer_0_500m_param.pt\"))\n",
    "# stu_layer.to(device)\n",
    "\n",
    "# # Prepare some input data\n",
    "# inputs = torch.randn(5, 1024, 768).to(device).to(torch.bfloat16)\n",
    "\n",
    "# # Run the STU layer\n",
    "# outputs = stu_layer(inputs)\n",
    "# print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "stu = copy.deepcopy(model.layers[0].stu)\n",
    "stu.phi = stu.phi.to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.randn(1, 1024, 768).to(device).to(torch.bfloat16)\n",
    "stu(inputs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def compute_ar_x_preds(w: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the auto-regressive component of a spectral SSM (PyTorch version),\n",
    "    allowing for a batch dimension in `x`.\n",
    "\n",
    "    Args:\n",
    "        w: A tensor of shape [d_out, d_in, k].\n",
    "        x: A tensor of shape [batch_size, l, d_in].\n",
    "\n",
    "    Returns:\n",
    "        A tensor of shape [batch_size, l, d_out].\n",
    "    \"\"\"\n",
    "    d_out, d_in, k = w.shape\n",
    "    b, l, d_in_x = x.shape\n",
    "    assert d_in == d_in_x, (\n",
    "        f\"Dimension mismatch: w.shape={w.shape}, x.shape={x.shape}\"\n",
    "    )\n",
    "\n",
    "    o = torch.einsum(\"oik,bli->bklo\", w, x)\n",
    "\n",
    "    for i in range(k):\n",
    "        # shape: [b, l, d_out]\n",
    "        o[:, i] = torch.roll(o[:, i], shifts=i, dims=1)\n",
    "\n",
    "    m = torch.triu(torch.ones(k, l, dtype=o.dtype, device=o.device))  # [k, l]\n",
    "    # shape: [k, l, 1] -> then repeat along d_out\n",
    "    m = m.unsqueeze(-1).repeat(1, 1, d_out)  # [k, l, d_out]\n",
    "\n",
    "    ar_x_preds = torch.sum(o * m, dim=1)  # now shape is [b, l, d_out]\n",
    "\n",
    "    return ar_x_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LDS(nn.Module):\n",
    "    def __init__(self, state_dim, input_dim, output_dim, kx=5):\n",
    "        super(LDS, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.kx = kx\n",
    "        self.h0 = nn.Parameter(torch.randn(state_dim))\n",
    "        init_A = torch.randn(state_dim)\n",
    "        self.A = nn.Parameter(init_A / torch.max(torch.abs(init_A)))\n",
    "        self.B = nn.Parameter(torch.randn(input_dim, state_dim) / input_dim)\n",
    "        self.C = nn.Parameter(torch.randn(state_dim, output_dim) / state_dim)\n",
    "        self.M = nn.Parameter(torch.randn(input_dim, output_dim, kx) / (output_dim))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        device = inputs.device\n",
    "        bsz, seq_len, _ = inputs.shape\n",
    "        h_t = self.h0.expand(bsz, self.state_dim).to(device)\n",
    "        A = self.A.flatten()\n",
    "        all_h_t = []\n",
    "        for t in range(seq_len):\n",
    "            u_t = inputs[:, t, :]\n",
    "            h_t = A * h_t + (u_t @ self.B)\n",
    "            all_h_t.append(h_t.unsqueeze(1))\n",
    "        all_h_t = torch.cat(all_h_t, dim=1)\n",
    "        lds_out = torch.matmul(all_h_t, self.C)\n",
    "\n",
    "        ar = compute_ar_x_preds(self.M, inputs)\n",
    "        return lds_out + ar\n",
    "\n",
    "    def compute_loss(self, inputs, targets):\n",
    "        mse_loss = nn.MSELoss()\n",
    "        outputs = self(inputs)\n",
    "        return mse_loss(outputs, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.10523165762424469\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 962.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 8.42 GiB is allocated by PyTorch, and 5.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(lds_epochs):\n\u001b[0;32m      9\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m4096\u001b[39m, \u001b[38;5;241m768\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[1;32m---> 10\u001b[0m     stu_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mstu\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m lds\u001b[38;5;241m.\u001b[39mcompute_loss(inputs\u001b[38;5;241m.\u001b[39mto(stu_outputs\u001b[38;5;241m.\u001b[39mdtype), stu_outputs)\n",
      "File \u001b[1;32mc:\\Users\\devan\\anaconda3\\envs\\flashstu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\devan\\anaconda3\\envs\\flashstu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\devan\\Downloads\\stu_lm_backup\\inference\\stu.py:60\u001b[0m, in \u001b[0;36mSTU.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     56\u001b[0m         spectral_plus, spectral_minus \u001b[38;5;241m=\u001b[39m flash_convolve(\n\u001b[0;32m     57\u001b[0m             x_proj, phi_proj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflash_fft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_approx\n\u001b[0;32m     58\u001b[0m         )\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m         spectral_plus, spectral_minus \u001b[38;5;241m=\u001b[39m \u001b[43mconvolve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m            \u001b[49m\u001b[43mx_proj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphi_proj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_approx\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Convolve inputs and filters,\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflash_fft:\n",
      "File \u001b[1;32mc:\\Users\\devan\\Downloads\\stu_lm_backup\\inference\\convolve.py:29\u001b[0m, in \u001b[0;36mconvolve\u001b[1;34m(u, v, n, use_approx)\u001b[0m\n\u001b[0;32m     27\u001b[0m U \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([u, u \u001b[38;5;241m*\u001b[39m sgn], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     28\u001b[0m U \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mrfft(U, n\u001b[38;5;241m=\u001b[39mn, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m U_conv \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mirfft\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[:, :seq_len]\n\u001b[0;32m     30\u001b[0m U_plus, U_minus \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munbind(U_conv, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     31\u001b[0m U_minus \u001b[38;5;241m=\u001b[39m U_minus \u001b[38;5;241m*\u001b[39m sgn\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 962.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 8.42 GiB is allocated by PyTorch, and 5.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "#remarkably, the approximation gets better and better as state_dim increases from  20 to ~500 (i.e 500 is 8x better than 50)\n",
    "state_dim = 1000 #@param\n",
    "lds =  LDS(state_dim, 768, 768, kx = 5).to(device)\n",
    "optimizer = torch.optim.Adam(lds.parameters(), lr = 0.002)\n",
    "lds_epochs = 2001\n",
    "lds_loss_values = []\n",
    "\n",
    "for epoch in range(lds_epochs):\n",
    "    inputs = torch.randn(10, 4096, 768).to(device).to(torch.bfloat16)\n",
    "    stu_outputs = stu(inputs).to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = lds.compute_loss(inputs.to(stu_outputs.dtype), stu_outputs)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(lds.parameters(), max_norm=1)\n",
    "    lds_loss_values.append(loss.item())\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        lds.A.data.clamp_(max=1, min = -1)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}') #Epoch 2000, Loss: 0.00030023115687072277 for normal LDS and  state dim  10000\n",
    "        #Epoch 2000, Loss: 0.001 for normal LDS and  state dim  1000\n",
    "        #0.0007 for 2000 epochs on LDS with 5 autoregressive heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lds.state_dict(), \"./lds_layer_new_2cap_0.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devan\\AppData\\Local\\Temp\\ipykernel_68564\\1833646201.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"./lds_trained/4512_0_80000_interim_lds_model_and_optimizer.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New LDS model created and initialized.\n"
     ]
    }
   ],
   "source": [
    "# Load the model and optimizer\n",
    "checkpoint = torch.load(\"./lds_trained/4512_0_80000_interim_lds_model_and_optimizer.pt\")\n",
    "del checkpoint['lds_state_dict']['M'] #this param doesn't correspond to current M\n",
    "lds_model = LDS(state_dim=80000, input_dim=768, output_dim=768, kx=0)\n",
    "lds_model.load_state_dict(checkpoint['lds_state_dict'], strict =  False)\n",
    "\n",
    "# Compute Cdiag(A)^iB for i in range(0, 10)\n",
    "C = lds_model.C\n",
    "A = lds_model.A\n",
    "B = lds_model.B\n",
    "\n",
    "\n",
    "\n",
    "M = torch.zeros((768, 768, 10), device=device)\n",
    "low_order_A = A.clone()\n",
    "low_order_A[(low_order_A > 0.7) | (low_order_A < -0.7)] = 0\n",
    "\n",
    "for i in range(10): #encompases low order terms or so\n",
    "    M[:, :, i] = (C.T  @ (B * low_order_A).T)\n",
    "\n",
    "# Create a new LDS with kx = 10 and M initialized to these values\n",
    "new_lds = LDS(state_dim=80000, input_dim=768, output_dim=768, kx=10)\n",
    "new_lds.M.data = M\n",
    "\n",
    "# Set all terms with low magnitude in A to 0\n",
    "threshold = 0.7  # Define a threshold for low magnitude\n",
    "A_new  = A.detach().clone()\n",
    "A_new[torch.abs(A_new) < threshold] = 0\n",
    "new_lds.A.data = A_new\n",
    "\n",
    "new_lds.B.data = B.clone()\n",
    "new_lds.C.data = C.clone()\n",
    "print(\"New LDS model created and initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m M \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(new_lds_kx10\u001b[38;5;241m.\u001b[39mM\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):  \u001b[38;5;66;03m# Encompasses low order terms or so\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mM\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m (new_lds_kx10\u001b[38;5;241m.\u001b[39mC\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m (new_lds_kx10\u001b[38;5;241m.\u001b[39mB \u001b[38;5;241m*\u001b[39m low_order_A)\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m     17\u001b[0m new_lds_kx10\u001b[38;5;241m.\u001b[39mM\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m M\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Set all terms with low magnitude in A to 0\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Create a new LDS with kx = 10\n",
    "new_lds_kx10 = LDS(state_dim=lds_model.state_dim, input_dim=lds_model.input_dim, output_dim=lds_model.output_dim, kx=50).to(device)\n",
    "\n",
    "# Copy parameters from the original LDS model\n",
    "new_lds_kx10.h0.data = lds_model.h0.data.clone()\n",
    "new_lds_kx10.B.data = lds_model.B.data.clone()\n",
    "new_lds_kx10.C.data = lds_model.C.data.clone()\n",
    "\n",
    "# Move low-order diagonal terms to the matrix connections\n",
    "low_order_A = lds_model.A.clone()\n",
    "low_order_A[(low_order_A > 0.7) | (low_order_A < -0.7)] = 0\n",
    "\n",
    "M = torch.zeros(new_lds_kx10.M.shape)\n",
    "for i in range(50):  # Encompasses low order terms or so\n",
    "    M[:, :, i] = (new_lds_kx10.C.T @ (new_lds_kx10.B * low_order_A).T)\n",
    "\n",
    "new_lds_kx10.M.data = M\n",
    "# Set all terms with low magnitude in A to 0\n",
    "threshold = 0.7  # Define a threshold for low magnitude\n",
    "A_new = lds_model.A.detach().clone()\n",
    "A_new[torch.abs(A_new) < threshold] = 0\n",
    "new_lds_kx10.A.data = torch.zeros(A_new.shape)\n",
    "\n",
    "lds_model.A.data = low_order_A.clone()\n",
    "\n",
    "print(\"New LDS model with kx=10 created and initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs from new_lds_kx10:\n",
      "tensor([[[-0.0320, -0.0954,  0.0884,  ..., -0.0486, -0.0227, -0.0624],\n",
      "         [ 0.0425, -0.3551,  0.1092,  ..., -0.0841,  0.0683, -0.2496],\n",
      "         [ 0.0507, -0.0788,  0.1489,  ...,  0.1220, -1.1290,  0.3742],\n",
      "         ...,\n",
      "         [-0.3814,  0.0917,  0.3059,  ...,  0.1473,  0.4253, -0.2302],\n",
      "         [-0.3391, -0.3873,  0.0998,  ...,  0.0743,  0.3013,  0.1995],\n",
      "         [-0.3856, -0.3221,  0.1163,  ...,  0.0706,  1.1418,  0.1554]]],\n",
      "       device='cuda:0')\n",
      "\n",
      "Outputs from lds_model:\n",
      "tensor([[[-0.0235, -0.0265,  0.0722,  ..., -0.1214, -0.0356, -0.0626],\n",
      "         [-0.0622, -0.3432,  0.1063,  ..., -0.0956,  0.0708, -0.2234],\n",
      "         [ 0.2258, -0.0204,  0.1506,  ...,  0.0499, -1.1362,  0.3839],\n",
      "         ...,\n",
      "         [ 0.0718,  0.0101,  0.3033,  ...,  0.1777,  0.4083, -0.2379],\n",
      "         [ 0.0305, -0.2475,  0.1030,  ...,  0.0160,  0.3109,  0.1906],\n",
      "         [-0.0814, -0.3260,  0.1206,  ...,  0.0711,  1.1825,  0.1430]]],\n",
      "       device='cuda:0')\n",
      "\n",
      "Are the outputs similar? 45052.77734375\n"
     ]
    }
   ],
   "source": [
    "# Prepare some input data\n",
    "inputs = torch.randn(1, 1024, 768).to(device).to(torch.float)\n",
    "\n",
    "# Run the input through new_lds_kx10\n",
    "new_lds_kx10.to(device).eval()\n",
    "with torch.no_grad():\n",
    "    new_lds_kx10_outputs = new_lds_kx10(inputs)\n",
    "\n",
    "# Run the input through lds_model\n",
    "lds_model.to(device).eval()\n",
    "with torch.no_grad():\n",
    "    lds_model_outputs = lds_model(inputs)\n",
    "\n",
    "# Compare the outputs\n",
    "print(\"Outputs from new_lds_kx10:\")\n",
    "print(new_lds_kx10_outputs)\n",
    "\n",
    "print(\"\\nOutputs from lds_model:\")\n",
    "print(lds_model_outputs)\n",
    "\n",
    "# Check if the outputs are similar\n",
    "similarity = torch.abs(new_lds_kx10_outputs - lds_model_outputs).mean()\n",
    "print(f\"\\nAre the outputs similar? {similarity}\")\n",
    "\n",
    "#I don't know what the issue is, perhaps the problem is h0 being non-zero? So test with h0 = 0 for both models and see if that works better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered LDS model created and initialized.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1041"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find indices where lds.A is not zero\n",
    "non_zero_indices = (new_lds.A != 0).nonzero(as_tuple=True)[0]\n",
    "\n",
    "# Filter B and C based on non-zero indices\n",
    "B_filtered = new_lds.B[:, non_zero_indices].clone()\n",
    "C_filtered = new_lds.C[non_zero_indices, :].clone()\n",
    "\n",
    "# Create a new LDS with the filtered B and C\n",
    "filtered_state_dim = len(non_zero_indices)\n",
    "new_lds_filtered = LDS(filtered_state_dim, new_lds.input_dim, new_lds.output_dim, kx=new_lds.kx).to(device)\n",
    "new_lds_filtered.B.data = B_filtered\n",
    "new_lds_filtered.C.data = C_filtered\n",
    "new_lds_filtered.A.data = new_lds.A[non_zero_indices].clone()\n",
    "\n",
    "print(\"Filtered LDS model created and initialized.\")\n",
    "filtered_state_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHHCAYAAABZbpmkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA87UlEQVR4nO3deVxUZf//8fcACqgs4gJSuEbuuaZplprc4ZJp2m2aGRppi2sulXepudyhZmYLZYui3rmU3WrdpaZpZgtpbllqKuZWClYGCCaxXL8/+jnfRkBlGJjx+Ho+HueRc53rnPlccxh4d+Y6Z2zGGCMAAACL8nJ3AQAAACWJsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsANcJWrWrKmBAwe6uwzLe+6551S7dm15e3uradOmhfYbOHCgatasecn9HTlyRDabTQsWLHBZjcDVhrADXIEWLFggm82mbdu2Fbi+Q4cOatSoUbGfZ/Xq1XrmmWeKvZ+rxbp16/T444/r5ptvVkJCgp599ll3l1Soxx9/XDabTffcc4+7SwFKnI+7CwBQOvbv3y8vr6L9/83q1asVHx9P4LlMGzdulJeXl+bNm6eyZcu6u5xCGWO0dOlS1axZU//73/905swZBQQEuLssoMRwZge4Svj6+qpMmTLuLqNIMjMz3V1CkZw6dUr+/v4eHXQkadOmTfrpp580f/585eTkaMWKFe4uCShRhB3gKnHhnJ3s7GxNnjxZkZGR8vPzU6VKldSuXTutX79e0l9zSuLj4yVJNpvNvpyXmZmpMWPGKCIiQr6+vqpbt65mzZolY4zD8/7xxx8aMWKEKleurICAAN155536+eefZbPZHM4YPfPMM7LZbNq7d6/uvfdeVaxYUe3atZMk7d69WwMHDlTt2rXl5+ensLAwPfDAA/rtt98cnuv8Pg4cOKD77rtPQUFBqlKliiZMmCBjjI4fP64ePXooMDBQYWFhev755y/rtcvJydHUqVNVp04d+fr6qmbNmvrXv/6lrKwsex+bzaaEhARlZmbaX6uizrNJTU3VwIEDFRQUpODgYMXExCg1NTVfv+TkZA0aNEjXXnutfH19Va1aNfXo0UNHjhy5rOdZvHixGjRooI4dOyoqKkqLFy8uUp3AlYaPsYArWFpamn799dd87dnZ2Zfc9plnnlFcXJwefPBBtWrVSunp6dq2bZt27Nihf/zjH3rooYd04sQJrV+/Xv/5z38ctjXG6M4779Snn36q2NhYNW3aVB9//LHGjRunn3/+WS+88IK978CBA/Xuu+9qwIABuummm/TZZ5+pW7duhdb1z3/+U5GRkXr22WftwWn9+vX68ccfNWjQIIWFhWnPnj164403tGfPHn399dcOIUyS7rnnHtWvX1/Tp0/XRx99pGnTpikkJESvv/66brvtNs2YMUOLFy/W2LFjdeONN+rWW2+96Gv14IMPauHChbr77rs1ZswYbdmyRXFxcdq3b59WrlwpSfrPf/6jN954Q1u3btVbb70lSWrbtu0lj8PfX9MePXroiy++0MMPP6z69etr5cqViomJyde3d+/e2rNnj4YPH66aNWvq1KlTWr9+vY4dO3bJSc9ZWVn673//qzFjxkiS+vXrp0GDBik5OVlhYWGXXS9wRTEArjgJCQlG0kWXhg0bOmxTo0YNExMTY3/cpEkT061bt4s+z9ChQ01BvyZWrVplJJlp06Y5tN99993GZrOZpKQkY4wx27dvN5LMqFGjHPoNHDjQSDKTJk2yt02aNMlIMv369cv3fGfPns3XtnTpUiPJbN68Od8+hgwZYm/Lyckx1157rbHZbGb69On29t9//934+/s7vCYF2bVrl5FkHnzwQYf2sWPHGklm48aN9raYmBhTvnz5i+7v731r1Khhf3z+NZ05c6ZD7bfccouRZBISEux1SzLPPffcZT3Phd577z0jyRw8eNAYY0x6errx8/MzL7zwglP7A64EfIwFXMHi4+O1fv36fMsNN9xwyW2Dg4O1Z88eHTx4sMjPu3r1anl7e2vEiBEO7WPGjJExRmvWrJEkrV27VpL06KOPOvQbPnx4oft++OGH87X5+/vb/33u3Dn9+uuvuummmyRJO3bsyNf/wQcftP/b29tbLVu2lDFGsbGx9vbg4GDVrVtXP/74Y6G1SH+NVZJGjx7t0H7+zMhHH3100e0v1+rVq+Xj46NHHnnEofYLX6vzc4I2bdqk33//vcjPs3jxYrVs2VLXXXedJCkgIEDdunXjoyxYGmEHuIK1atVKUVFR+ZaKFStectspU6YoNTVV119/vRo3bqxx48Zp9+7dl/W8R48eVXh4eL4reOrXr29ff/6/Xl5eqlWrlkO/839oC3JhX0k6ffq0Ro4cqdDQUPn7+6tKlSr2fmlpafn6V69e3eFxUFCQ/Pz8VLly5XztlwoM58dwYc1hYWEKDg62j7W4jh49qmrVqqlChQoO7XXr1nV47OvrqxkzZmjNmjUKDQ3VrbfeqpkzZyo5OfmSz5GamqrVq1erffv2SkpKsi8333yztm3bpgMHDrhkLICnIewAV6lbb71Vhw4d0vz589WoUSO99dZbat68uX2+ibv8/SzOeX369NGbb76phx9+WCtWrNC6devsZ43y8vLy9ff29r6sNkn5JlQX5sJ5Qe40atQoHThwQHFxcfLz89OECRNUv3597dy586LbLV++XFlZWXr++ecVGRlpX86fteLsDqyKsANcxUJCQjRo0CAtXbpUx48f1w033OBwhVRhf+Br1KihEydO6MyZMw7tP/zwg339+f/m5eXp8OHDDv2SkpIuu8bff/9dGzZs0JNPPqnJkyfrrrvu0j/+8Q/Vrl37svdRHOfHcOHHfSkpKUpNTbWP1RXPc/LkSWVkZDi079+/v8D+derU0ZgxY7Ru3Tp9//33+vPPPy95ddnixYvVqFEjLV++PN8SFRWlJUuWuGQsgKch7ABXqQsv265QoYKuu+46h8upy5cvL0n5Ln/u2rWrcnNz9corrzi0v/DCC7LZbOrSpYskKTo6WpL06quvOvR7+eWXL7vO82dkLjwDM2fOnMveR3F07dq1wOebPXu2JF30yrKiPk9OTo5ee+01e1tubm6+1+rs2bM6d+6cQ1udOnUUEBDgcOxOnjypH374wX5l3vHjx7V582b16dNHd999d75l0KBBSkpK0pYtW1wyHsCTcOk5cJVq0KCBOnTooBYtWigkJETbtm3Te++9p2HDhtn7tGjRQpI0YsQIRUdHy9vbW3379lX37t3VsWNHPfXUUzpy5IiaNGmidevW6f3339eoUaNUp04d+/a9e/fWnDlz9Ntvv9kvPT8/N+RyPhoKDAy0z0vJzs7WNddco3Xr1uU7W1RSmjRpopiYGL3xxhtKTU1V+/bttXXrVi1cuFA9e/ZUx44dXfI83bt3180336wnn3xSR44cUYMGDbRixYp8c5IOHDigTp06qU+fPmrQoIF8fHy0cuVKpaSkqG/fvvZ+48eP18KFC3X48GHVrFlTS5Yssd8yoCBdu3aVj4+PFi9erNatW7tkTICnIOwAV6kRI0bogw8+0Lp165SVlaUaNWpo2rRpGjdunL1Pr169NHz4cC1btkxvv/22jDHq27evvLy89MEHH2jixIl65513lJCQoJo1a+q5556zX6V03qJFixQWFqalS5dq5cqVioqK0jvvvKO6devKz8/vsmpdsmSJhg8frvj4eBljdPvtt2vNmjUKDw936WtSmLfeeku1a9fWggULtHLlSoWFhWn8+PGaNGmSy57j/Gs6atQovf3227LZbLrzzjv1/PPPq1mzZvZ+ERER6tevnzZs2KD//Oc/8vHxUb169fTuu++qd+/ehe5/8eLFql69upo0aVLg+uDgYLVr107vvPOOZs+eLR8f/jzAOmzmcmfnAYCL7Nq1S82aNdPbb7+t/v37u7scABbHnB0AJeqPP/7I1zZnzhx5eXld8s7FAOAKnKcEUKJmzpyp7du3q2PHjvLx8dGaNWu0Zs0aDRkyRBEREe4uD8BVgI+xAJSo9evXa/Lkydq7d68yMjJUvXp1DRgwQE899RTzQgCUCsIOAACwNObsAAAASyPsAAAAS+MDc/313TonTpxQQECAR33/DQAAKJwxRmfOnFF4eLi8vAo/f0PYkXTixAmuCgEA4Ap1/PhxXXvttYWuJ+xICggIkPTXixUYGOjmagAAwOVIT09XRESE/e94YQg7+r/v5wkMDCTsAABwhbnUFBQmKAMAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEvzcXcBVtf3jcR8bcuGtHFDJQAAXJ04swMAACyNsAMAACyNsAMAACyNsAMAACzNrWFn8+bN6t69u8LDw2Wz2bRq1apC+z788MOy2WyaM2eOQ/vp06fVv39/BQYGKjg4WLGxscrIyCjZwgEAwBXDrWEnMzNTTZo0UXx8/EX7rVy5Ul9//bXCw8Pzrevfv7/27Nmj9evX68MPP9TmzZs1ZMiQkioZAABcYdx66XmXLl3UpUuXi/b5+eefNXz4cH388cfq1q2bw7p9+/Zp7dq1+uabb9SyZUtJ0ssvv6yuXbtq1qxZBYYjAABQsi687Yq7b7ni0XN28vLyNGDAAI0bN04NGzbMtz4xMVHBwcH2oCNJUVFR8vLy0pYtW0qzVAAA4KE8+qaCM2bMkI+Pj0aMGFHg+uTkZFWtWtWhzcfHRyEhIUpOTi50v1lZWcrKyrI/Tk9Pd03BAADA43jsmZ3t27frxRdf1IIFC2Sz2Vy677i4OAUFBdmXiIgIl+4fAAB4Do8NO59//rlOnTql6tWry8fHRz4+Pjp69KjGjBmjmjVrSpLCwsJ06tQph+1ycnJ0+vRphYWFFbrv8ePHKy0tzb4cP368JIcCAADcyGM/xhowYICioqIc2qKjozVgwAANGjRIktSmTRulpqZq+/btatGihSRp48aNysvLU+vWrQvdt6+vr3x9fUuueAAA4DHcGnYyMjKUlJRkf3z48GHt2rVLISEhql69uipVquTQv0yZMgoLC1PdunUlSfXr11fnzp01ePBgzZ07V9nZ2Ro2bJj69u3LlVgAAECSmz/G2rZtm5o1a6ZmzZpJkkaPHq1mzZpp4sSJl72PxYsXq169eurUqZO6du2qdu3a6Y033iipkgEAwBXGrWd2OnToIGPMZfc/cuRIvraQkBAtWbLEhVUBAAAr8dgJygAAAK5A2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJbm1rCzefNmde/eXeHh4bLZbFq1apV9XXZ2tp544gk1btxY5cuXV3h4uO6//36dOHHCYR+nT59W//79FRgYqODgYMXGxiojI6OURwIAADyVW8NOZmammjRpovj4+Hzrzp49qx07dmjChAnasWOHVqxYof379+vOO+906Ne/f3/t2bNH69ev14cffqjNmzdryJAhpTUEAADg4Xzc+eRdunRRly5dClwXFBSk9evXO7S98soratWqlY4dO6bq1atr3759Wrt2rb755hu1bNlSkvTyyy+ra9eumjVrlsLDw0t8DAAAwLNdUXN20tLSZLPZFBwcLElKTExUcHCwPehIUlRUlLy8vLRly5ZC95OVlaX09HSHBQAAWNMVE3bOnTunJ554Qv369VNgYKAkKTk5WVWrVnXo5+Pjo5CQECUnJxe6r7i4OAUFBdmXiIiIEq0dAAC4zxURdrKzs9WnTx8ZY/Taa68Ve3/jx49XWlqafTl+/LgLqgQAAJ7IrXN2Lsf5oHP06FFt3LjRflZHksLCwnTq1CmH/jk5OTp9+rTCwsIK3aevr698fX1LrGYAAOA5PPrMzvmgc/DgQX3yySeqVKmSw/o2bdooNTVV27dvt7dt3LhReXl5at26dWmXCwAAPJBbz+xkZGQoKSnJ/vjw4cPatWuXQkJCVK1aNd19993asWOHPvzwQ+Xm5trn4YSEhKhs2bKqX7++OnfurMGDB2vu3LnKzs7WsGHD1LdvX67EAgAAktwcdrZt26aOHTvaH48ePVqSFBMTo2eeeUYffPCBJKlp06YO23366afq0KGDJGnx4sUaNmyYOnXqJC8vL/Xu3VsvvfRSqdQPAAA8n1vDTocOHWSMKXT9xdadFxISoiVLlriyLAAAYCEePWcHAACguAg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0jz+6yKsqO8biQ6Plw1p46ZKAACwPs7sAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAAS+OmggAAwGkX3ijXE3FmBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWJpbw87mzZvVvXt3hYeHy2azadWqVQ7rjTGaOHGiqlWrJn9/f0VFRengwYMOfU6fPq3+/fsrMDBQwcHBio2NVUZGRimOAgAAeDK3hp3MzEw1adJE8fHxBa6fOXOmXnrpJc2dO1dbtmxR+fLlFR0drXPnztn79O/fX3v27NH69ev14YcfavPmzRoyZEhpDQEAAHg4H3c+eZcuXdSlS5cC1xljNGfOHD399NPq0aOHJGnRokUKDQ3VqlWr1LdvX+3bt09r167VN998o5YtW0qSXn75ZXXt2lWzZs1SeHh4qY0FAAB4Jo+ds3P48GElJycrKirK3hYUFKTWrVsrMTFRkpSYmKjg4GB70JGkqKgoeXl5acuWLYXuOysrS+np6Q4LAACwJo8NO8nJyZKk0NBQh/bQ0FD7uuTkZFWtWtVhvY+Pj0JCQux9ChIXF6egoCD7EhER4eLqAQCAp/DYsFOSxo8fr7S0NPty/Phxd5cEAABKiMeGnbCwMElSSkqKQ3tKSop9XVhYmE6dOuWwPicnR6dPn7b3KYivr68CAwMdFgAAYE0eG3Zq1aqlsLAwbdiwwd6Wnp6uLVu2qE2bNpKkNm3aKDU1Vdu3b7f32bhxo/Ly8tS6detSrxkAAHget16NlZGRoaSkJPvjw4cPa9euXQoJCVH16tU1atQoTZs2TZGRkapVq5YmTJig8PBw9ezZU5JUv359de7cWYMHD9bcuXOVnZ2tYcOGqW/fvlyJBQAAJLk57Gzbtk0dO3a0Px49erQkKSYmRgsWLNDjjz+uzMxMDRkyRKmpqWrXrp3Wrl0rPz8/+zaLFy/WsGHD1KlTJ3l5eal379566aWXSn0sAADAM7k17HTo0EHGmELX22w2TZkyRVOmTCm0T0hIiJYsWVIS5QEAAAvw2Dk7AAAArkDYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAluZU2Pnxxx9dXQcAAECJcCrsXHfdderYsaPefvttnTt3ztU1AQAAuIxTYWfHjh264YYbNHr0aIWFhemhhx7S1q1bXV0bAABAsTkVdpo2baoXX3xRJ06c0Pz583Xy5Em1a9dOjRo10uzZs/XLL7+4uk4AAACnFGuCso+Pj3r16qXly5drxowZSkpK0tixYxUREaH7779fJ0+edFWdAAAATilW2Nm2bZseffRRVatWTbNnz9bYsWN16NAhrV+/XidOnFCPHj1cVScAAIBTfJzZaPbs2UpISND+/fvVtWtXLVq0SF27dpWX11/ZqVatWlqwYIFq1qzpyloBAACKzKmw89prr+mBBx7QwIEDVa1atQL7VK1aVfPmzStWcQAAAMXlVNg5ePDgJfuULVtWMTExzuweAADAZZyas5OQkKDly5fna1++fLkWLlxY7KIAAABcxamwExcXp8qVK+drr1q1qp599tliFwUAAOAqToWdY8eOqVatWvnaa9SooWPHjhW7KAAAAFdxKuxUrVpVu3fvztf+7bffqlKlSsUuCgAAwFWcCjv9+vXTiBEj9Omnnyo3N1e5ubnauHGjRo4cqb59+7q6RgAAAKc5dTXW1KlTdeTIEXXq1Ek+Pn/tIi8vT/fffz9zdgAAgEdxKuyULVtW77zzjqZOnapvv/1W/v7+aty4sWrUqOHq+gAAAIrFqbBz3vXXX6/rr7/eVbUAAAC4nFNhJzc3VwsWLNCGDRt06tQp5eXlOazfuHGjS4oDAAAoLqcmKI8cOVIjR45Ubm6uGjVqpCZNmjgsrpKbm6sJEyaoVq1a8vf3V506dTR16lQZY+x9jDGaOHGiqlWrJn9/f0VFRV3WHZ4BAMDVwakzO8uWLdO7776rrl27uroeBzNmzNBrr72mhQsXqmHDhtq2bZsGDRqkoKAgjRgxQpI0c+ZMvfTSS1q4cKFq1aqlCRMmKDo6Wnv37pWfn1+J1gcAADyf0xOUr7vuOlfXks9XX32lHj16qFu3bpKkmjVraunSpdq6daukv87qzJkzR08//bR69OghSVq0aJFCQ0O1atUqLoMHAADOfYw1ZswYvfjiiw4fJ5WEtm3basOGDTpw4ICkv25a+MUXX6hLly6SpMOHDys5OVlRUVH2bYKCgtS6dWslJiYWut+srCylp6c7LAAAwJqcOrPzxRdf6NNPP9WaNWvUsGFDlSlTxmH9ihUrXFLck08+qfT0dNWrV0/e3t7Kzc3Vv//9b/Xv31+SlJycLEkKDQ112C40NNS+riBxcXGaPHmyS2oEAACezamwExwcrLvuusvVteTz7rvvavHixVqyZIkaNmyoXbt2adSoUQoPD1dMTIzT+x0/frxGjx5tf5yenq6IiAhXlAwAADyMU2EnISHB1XUUaNy4cXryySftc28aN26so0ePKi4uTjExMQoLC5MkpaSkqFq1avbtUlJS1LRp00L36+vrK19f3xKtHQAAeAan5uxIUk5Ojj755BO9/vrrOnPmjCTpxIkTysjIcFlxZ8+elZeXY4ne3t72+/rUqlVLYWFh2rBhg319enq6tmzZojZt2risDgAAcOVy6szO0aNH1blzZx07dkxZWVn6xz/+oYCAAM2YMUNZWVmaO3euS4rr3r27/v3vf6t69epq2LChdu7cqdmzZ+uBBx6QJNlsNo0aNUrTpk1TZGSk/dLz8PBw9ezZ0yU1AACAK5tTYWfkyJFq2bKlvv32W1WqVMneftddd2nw4MEuK+7ll1/WhAkT9Oijj+rUqVMKDw/XQw89pIkTJ9r7PP7448rMzNSQIUOUmpqqdu3aae3atdxjBwAASHIy7Hz++ef66quvVLZsWYf2mjVr6ueff3ZJYZIUEBCgOXPmaM6cOYX2sdlsmjJliqZMmeKy5wUAANbh1JydvLw85ebm5mv/6aefFBAQUOyiAAAAXMWpsHP77bc7nG2x2WzKyMjQpEmTSvwrJAAAAIrCqY+xnn/+eUVHR6tBgwY6d+6c7r33Xh08eFCVK1fW0qVLXV0jAACA05wKO9dee62+/fZbLVu2TLt371ZGRoZiY2PVv39/+fv7u7pGAAAApzkVdiTJx8dH9913nytrAQAAcDmnws6iRYsuuv7+++93qhgAAABXc/o+O3+XnZ2ts2fPqmzZsipXrhxhBwAAeAynrsb6/fffHZaMjAzt379f7dq1Y4IyAADwKE5/N9aFIiMjNX369HxnfQAAANzJZWFH+mvS8okTJ1y5SwAAgGJxas7OBx984PDYGKOTJ0/qlVde0c033+ySwgAAAFzBqbBz4TeK22w2ValSRbfddpuef/55V9QFAADgEk6Fnby8PFfXAQAAUCJcOmcHAADA0zh1Zmf06NGX3Xf27NnOPAUAAIBLOBV2du7cqZ07dyo7O1t169aVJB04cEDe3t5q3ry5vZ/NZnNNlQAAAE5yKux0795dAQEBWrhwoSpWrCjprxsNDho0SLfccovGjBnj0iIBAACc5dScneeff15xcXH2oCNJFStW1LRp07gaCwAAeBSnwk56erp++eWXfO2//PKLzpw5U+yiAAAAXMWpsHPXXXdp0KBBWrFihX766Sf99NNP+u9//6vY2Fj16tXL1TUCAAA4zak5O3PnztXYsWN17733Kjs7+68d+fgoNjZWzz33nEsLBAAAKA6nwk65cuX06quv6rnnntOhQ4ckSXXq1FH58uVdWhwAAEBxFeumgidPntTJkycVGRmp8uXLyxjjqroAAABcwqmw89tvv6lTp066/vrr1bVrV508eVKSFBsby2XnAADAozgVdh577DGVKVNGx44dU7ly5ezt99xzj9auXeuy4gAAAIrLqTk769at08cff6xrr73WoT0yMlJHjx51SWEAAACu4NSZnczMTIczOuedPn1avr6+xS4KAADAVZwKO7fccosWLVpkf2yz2ZSXl6eZM2eqY8eOLisOAACguJz6GGvmzJnq1KmTtm3bpj///FOPP/649uzZo9OnT+vLL790dY0AAABOc+rMTqNGjXTgwAG1a9dOPXr0UGZmpnr16qWdO3eqTp06rq4RAADAaUU+s5Odna3OnTtr7ty5euqpp0qiJgAAAJcp8pmdMmXKaPfu3SVRCwAAgMs59THWfffdp3nz5rm6FgAAAJdzaoJyTk6O5s+fr08++UQtWrTI951Ys2fPdklxAAAAxVWksPPjjz+qZs2a+v7779W8eXNJ0oEDBxz62Gw211UHAABQTEUKO5GRkTp58qQ+/fRTSX99PcRLL72k0NDQEikOAACguIo0Z+fCbzVfs2aNMjMzXVoQAACAKzk1Qfm8C8MPAACApylS2LHZbPnm5JT0HJ2ff/5Z9913nypVqiR/f381btxY27Zts683xmjixImqVq2a/P39FRUVpYMHD5ZoTQAA4MpRpDk7xhgNHDjQ/mWf586d08MPP5zvaqwVK1a4pLjff/9dN998szp27Kg1a9aoSpUqOnjwoCpWrGjvM3PmTL300ktauHChatWqpQkTJig6Olp79+6Vn5+fS+oAAABXriKFnZiYGIfH9913n0uLudCMGTMUERGhhIQEe1utWrXs/zbGaM6cOXr66afVo0cPSdKiRYsUGhqqVatWqW/fviVaHwAA8HxFCjt/Dx2l4YMPPlB0dLT++c9/6rPPPtM111yjRx99VIMHD5YkHT58WMnJyYqKirJvExQUpNatWysxMbHQsJOVlaWsrCz74/T09JIdCAAAcJtiTVAuaT/++KNee+01RUZG6uOPP9YjjzyiESNGaOHChZKk5ORkScp36XtoaKh9XUHi4uIUFBRkXyIiIkpuEAAAwK08Ouzk5eWpefPmevbZZ9WsWTMNGTJEgwcP1ty5c4u13/HjxystLc2+HD9+3EUVAwAAT+PRYadatWpq0KCBQ1v9+vV17NgxSVJYWJgkKSUlxaFPSkqKfV1BfH19FRgY6LAAAABr8uiwc/PNN2v//v0ObQcOHFCNGjUk/TVZOSwsTBs2bLCvT09P15YtW9SmTZtSrRUAAHgmp74ItLQ89thjatu2rZ599ln16dNHW7du1RtvvKE33nhD0l/3+Bk1apSmTZumyMhI+6Xn4eHh6tmzp3uLBwAAHsGjw86NN96olStXavz48ZoyZYpq1aqlOXPmqH///vY+jz/+uDIzMzVkyBClpqaqXbt2Wrt2LffYAQAAkjw87EjSHXfcoTvuuKPQ9TabTVOmTNGUKVNKsSoAAHCl8PiwAwAAPEffNxLdXUKRefQEZQAAgOIi7AAAAEvjYywPUNApwWVDuHQeAABX4MwOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwtCsq7EyfPl02m02jRo2yt507d05Dhw5VpUqVVKFCBfXu3VspKSnuKxIAAHiUKybsfPPNN3r99dd1ww03OLQ/9thj+t///qfly5frs88+04kTJ9SrVy83VQkAADzNFRF2MjIy1L9/f7355puqWLGivT0tLU3z5s3T7Nmzddttt6lFixZKSEjQV199pa+//tqNFQMAAE9xRYSdoUOHqlu3boqKinJo3759u7Kzsx3a69Wrp+rVqysxMbHQ/WVlZSk9Pd1hAQAA1uTj7gIuZdmyZdqxY4e++eabfOuSk5NVtmxZBQcHO7SHhoYqOTm50H3GxcVp8uTJri4VAAB4II8+s3P8+HGNHDlSixcvlp+fn8v2O378eKWlpdmX48ePu2zfAADAs3h02Nm+fbtOnTql5s2by8fHRz4+Pvrss8/00ksvycfHR6Ghofrzzz+VmprqsF1KSorCwsIK3a+vr68CAwMdFgAAYE0e/TFWp06d9N133zm0DRo0SPXq1dMTTzyhiIgIlSlTRhs2bFDv3r0lSfv379exY8fUpk0bd5QMAAA8jEeHnYCAADVq1MihrXz58qpUqZK9PTY2VqNHj1ZISIgCAwM1fPhwtWnTRjfddJM7SgYAAB7Go8PO5XjhhRfk5eWl3r17KysrS9HR0Xr11VfdXRYAAPAQV1zY2bRpk8NjPz8/xcfHKz4+3j0FAQAAj+bRE5QBAACKi7ADAAAsjbADAAAs7YqbswMAgKfr+0b+ryxaNoRborgLZ3YAAIClEXYAAIClEXYAAIClEXYAAIClMUEZLnPhhDwm4wEAPAFndgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKURdgAAgKVxU0EAAEoBN151H87sAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAAS+M+OygxF95TQuK+EgCA0seZHQAAYGmc2fFQ3GkTAADX4MwOAACwNMIOAACwNMIOAACwNMIOAACwNCYoAwCAAhV0C5ErEWd2AACApRF2AACApRF2AACApRF2AACApRF2AACApXl02ImLi9ONN96ogIAAVa1aVT179tT+/fsd+pw7d05Dhw5VpUqVVKFCBfXu3VspKSluqhgAAHgaj770/LPPPtPQoUN14403KicnR//61790++23a+/evSpfvrwk6bHHHtNHH32k5cuXKygoSMOGDVOvXr305Zdfurl6AAAKV9Bl3XwPYsnw6LCzdu1ah8cLFixQ1apVtX37dt16661KS0vTvHnztGTJEt12222SpISEBNWvX19ff/21brrpJneUDQAAPIhHf4x1obS0NElSSEiIJGn79u3Kzs5WVFSUvU+9evVUvXp1JSYWfiOkrKwspaenOywAAMCarpiwk5eXp1GjRunmm29Wo0aNJEnJyckqW7asgoODHfqGhoYqOTm50H3FxcUpKCjIvkRERJRk6QAAwI2umLAzdOhQff/991q2bFmx9zV+/HilpaXZl+PHj7ugQgAA4Ik8es7OecOGDdOHH36ozZs369prr7W3h4WF6c8//1RqaqrD2Z2UlBSFhYUVuj9fX1/5+vqWZMkAAMBDePSZHWOMhg0bppUrV2rjxo2qVauWw/oWLVqoTJky2rBhg71t//79OnbsmNq0YUY7AADw8DM7Q4cO1ZIlS/T+++8rICDAPg8nKChI/v7+CgoKUmxsrEaPHq2QkBAFBgZq+PDhatOmDVdiAQAASR4edl577TVJUocOHRzaExISNHDgQEnSCy+8IC8vL/Xu3VtZWVmKjo7Wq6++WsqVXn0Kuj8EAACeyKPDjjHmkn38/PwUHx+v+Pj4UqgIAABcaTw67OD/OHunzQu34+6cAGB93J3ZkUdPUAYAACguwg4AALA0PsYCAOAqdDV91MWZHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGlMUAYAwEO46p5qcMSZHQAAYGmc2bnK8H8NAIDCWPV3PWd2AACApXFm5wrG914BAHBpnNkBAACWRtgBAACWxsdYAABcBaw6+fhycGYHAABYGmEHAABYGmEHAABYGmEHAABYGhOUcVVPWgMAT8fv6OLjzA4AALA0zuxYCOkfAID8OLMDAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAszTJhJz4+XjVr1pSfn59at26trVu3urskAADgASwRdt555x2NHj1akyZN0o4dO9SkSRNFR0fr1KlT7i4NAAC4mSXCzuzZszV48GANGjRIDRo00Ny5c1WuXDnNnz/f3aUBAAA3u+LDzp9//qnt27crKirK3ubl5aWoqCglJia6sTIAAOAJfNxdQHH9+uuvys3NVWhoqEN7aGiofvjhhwK3ycrKUlZWlv1xWlqaJCk9Pd3l9WX/kenyfV7JSuI1BgBPw+9+RyX1u//8fo0xF+13xYcdZ8TFxWny5Mn52iMiItxQzdVlxSh3VwAAKG0l/bv/zJkzCgoKKnT9FR92KleuLG9vb6WkpDi0p6SkKCwsrMBtxo8fr9GjR9sf5+Xl6fTp06pUqZJsNptL6kpPT1dERISOHz+uwMBAl+zT01h9jFYfn8QYrcDq45MYoxWU1PiMMTpz5ozCw8Mv2u+KDztly5ZVixYttGHDBvXs2VPSX+Flw4YNGjZsWIHb+Pr6ytfX16EtODi4ROoLDAy05A/u31l9jFYfn8QYrcDq45MYoxWUxPgudkbnvCs+7EjS6NGjFRMTo5YtW6pVq1aaM2eOMjMzNWjQIHeXBgAA3MwSYeeee+7RL7/8ookTJyo5OVlNmzbV2rVr801aBgAAVx9LhB1JGjZsWKEfW7mDr6+vJk2alO/jMiux+hitPj6JMVqB1ccnMUYrcPf4bOZS12sBAABcwa74mwoCAABcDGEHAABYGmEHAABYGmEHAABYGmHHSf/+97/Vtm1blStX7rJvSGiM0cSJE1WtWjX5+/srKipKBw8edOhz+vRp9e/fX4GBgQoODlZsbKwyMjJKYASXVtRajhw5IpvNVuCyfPlye7+C1i9btqw0hpSPM693hw4d8tX/8MMPO/Q5duyYunXrpnLlyqlq1aoaN26ccnJySnIoBSrq+E6fPq3hw4erbt268vf3V/Xq1TVixAj798ed585jGB8fr5o1a8rPz0+tW7fW1q1bL9p/+fLlqlevnvz8/NS4cWOtXr3aYf3lvC9LW1HG+Oabb+qWW25RxYoVVbFiRUVFReXrP3DgwHzHq3PnziU9jIsqyhgXLFiQr34/Pz+HPp52HIsyvoJ+p9hsNnXr1s3ex9OO4ebNm9W9e3eFh4fLZrNp1apVl9xm06ZNat68uXx9fXXddddpwYIF+foU9f192QycMnHiRDN79mwzevRoExQUdFnbTJ8+3QQFBZlVq1aZb7/91tx5552mVq1a5o8//rD36dy5s2nSpIn5+uuvzeeff26uu+46069fvxIaxcUVtZacnBxz8uRJh2Xy5MmmQoUK5syZM/Z+kkxCQoJDv7+/BqXJmde7ffv2ZvDgwQ71p6Wl2dfn5OSYRo0amaioKLNz506zevVqU7lyZTN+/PiSHk4+RR3fd999Z3r16mU++OADk5SUZDZs2GAiIyNN7969Hfq56xguW7bMlC1b1syfP9/s2bPHDB482AQHB5uUlJQC+3/55ZfG29vbzJw50+zdu9c8/fTTpkyZMua7776z97mc92VpKuoY7733XhMfH2927txp9u3bZwYOHGiCgoLMTz/9ZO8TExNjOnfu7HC8Tp8+XVpDyqeoY0xISDCBgYEO9ScnJzv08aTjWNTx/fbbbw5j+/777423t7dJSEiw9/G0Y7h69Wrz1FNPmRUrVhhJZuXKlRft/+OPP5py5cqZ0aNHm71795qXX37ZeHt7m7Vr19r7FPV1KwrCTjElJCRcVtjJy8szYWFh5rnnnrO3paamGl9fX7N06VJjjDF79+41ksw333xj77NmzRpjs9nMzz//7PLaL8ZVtTRt2tQ88MADDm2X88YoDc6OsX379mbkyJGFrl+9erXx8vJy+GX82muvmcDAQJOVleWS2i+Hq47hu+++a8qWLWuys7Ptbe46hq1atTJDhw61P87NzTXh4eEmLi6uwP59+vQx3bp1c2hr3bq1eeihh4wxl/e+LG1FHeOFcnJyTEBAgFm4cKG9LSYmxvTo0cPVpTqtqGO81O9ZTzuOxT2GL7zwggkICDAZGRn2Nk87hn93Ob8PHn/8cdOwYUOHtnvuucdER0fbHxf3dbsYPsYqJYcPH1ZycrKioqLsbUFBQWrdurUSExMlSYmJiQoODlbLli3tfaKiouTl5aUtW7aUar2uqGX79u3atWuXYmNj860bOnSoKleurFatWmn+/PkybrjdU3HGuHjxYlWuXFmNGjXS+PHjdfbsWYf9Nm7c2OEO3tHR0UpPT9eePXtcP5BCuOrnKS0tTYGBgfLxcbwHaWkfwz///FPbt293eA95eXkpKirK/h66UGJiokN/6a9jcb7/5bwvS5MzY7zQ2bNnlZ2drZCQEIf2TZs2qWrVqqpbt64eeeQR/fbbby6t/XI5O8aMjAzVqFFDERER6tGjh8N7yZOOoyuO4bx589S3b1+VL1/eod1TjqEzLvVedMXrdjGWuYOyp0tOTpakfF9hERoaal+XnJysqlWrOqz38fFRSEiIvU9pcUUt8+bNU/369dW2bVuH9ilTpui2225TuXLltG7dOj366KPKyMjQiBEjXFb/5XB2jPfee69q1Kih8PBw7d69W0888YT279+vFStW2Pdb0HE+v660uOIY/vrrr5o6daqGDBni0O6OY/jrr78qNze3wNf2hx9+KHCbwo7F399z59sK61OanBnjhZ544gmFh4c7/NHo3LmzevXqpVq1aunQoUP617/+pS5duigxMVHe3t4uHcOlODPGunXrav78+brhhhuUlpamWbNmqW3bttqzZ4+uvfZajzqOxT2GW7du1ffff6958+Y5tHvSMXRGYe/F9PR0/fHHH/r999+L/bN/MYSdv3nyySc1Y8aMi/bZt2+f6tWrV0oVud7ljrG4/vjjDy1ZskQTJkzIt+7vbc2aNVNmZqaee+45l/2hLOkx/v0Pf+PGjVWtWjV16tRJhw4dUp06dZze7+UqrWOYnp6ubt26qUGDBnrmmWcc1pX0MYRzpk+frmXLlmnTpk0OE3j79u1r/3fjxo11ww03qE6dOtq0aZM6derkjlKLpE2bNmrTpo39cdu2bVW/fn29/vrrmjp1qhsrc7158+apcePGatWqlUP7lX4M3Y2w8zdjxozRwIEDL9qndu3aTu07LCxMkpSSkqJq1arZ21NSUtS0aVN7n1OnTjlsl5OTo9OnT9u3L67LHWNxa3nvvfd09uxZ3X///Zfs27p1a02dOlVZWVku+d6U0hrjea1bt5YkJSUlqU6dOgoLC8t3BUFKSookueQ4lsb4zpw5o86dOysgIEArV65UmTJlLtrf1cewIJUrV5a3t7f9tTwvJSWl0PGEhYVdtP/lvC9LkzNjPG/WrFmaPn26PvnkE91www0X7Vu7dm1VrlxZSUlJpf6HsjhjPK9MmTJq1qyZkpKSJHnWcSzO+DIzM7Vs2TJNmTLlks/jzmPojMLei4GBgfL395e3t3exfy4uqtizfq5yRZ2gPGvWLHtbWlpagROUt23bZu/z8ccfu3WCsrO1tG/fPt8VPIWZNm2aqVixotO1OstVr/cXX3xhJJlvv/3WGPN/E5T/fgXB66+/bgIDA825c+dcN4BLcHZ8aWlp5qabbjLt27c3mZmZl/VcpXUMW7VqZYYNG2Z/nJuba6655pqLTlC+4447HNratGmTb4Lyxd6Xpa2oYzTGmBkzZpjAwECTmJh4Wc9x/PhxY7PZzPvvv1/sep3hzBj/Licnx9StW9c89thjxhjPO47Oji8hIcH4+vqaX3/99ZLP4e5j+He6zAnKjRo1cmjr169fvgnKxfm5uGiNxd7DVero0aNm586d9kurd+7caXbu3OlwiXXdunXNihUr7I+nT59ugoODzfvvv292795tevToUeCl582aNTNbtmwxX3zxhYmMjHTrpecXq+Wnn34ydevWNVu2bHHY7uDBg8Zms5k1a9bk2+cHH3xg3nzzTfPdd9+ZgwcPmldffdWUK1fOTJw4scTHU5CijjEpKclMmTLFbNu2zRw+fNi8//77pnbt2ubWW2+1b3P+0vPbb7/d7Nq1y6xdu9ZUqVLFbZeeF2V8aWlppnXr1qZx48YmKSnJ4TLXnJwcY4x7j+GyZcuMr6+vWbBggdm7d68ZMmSICQ4Otl/5NmDAAPPkk0/a+3/55ZfGx8fHzJo1y+zbt89MmjSpwEvPL/W+LE1FHeP06dNN2bJlzXvvvedwvM7/Ljpz5owZO3asSUxMNIcPHzaffPKJad68uYmMjCzV8F2cMU6ePNl8/PHH5tChQ2b79u2mb9++xs/Pz+zZs8fex5OOY1HHd167du3MPffck6/dE4/hmTNn7H/3JJnZs2ebnTt3mqNHjxpjjHnyySfNgAED7P3PX3o+btw4s2/fPhMfH1/gpecXe92Kg7DjpJiYGCMp3/Lpp5/a++j/34vkvLy8PDNhwgQTGhpqfH19TadOncz+/fsd9vvbb7+Zfv36mQoVKpjAwEAzaNAghwBVmi5Vy+HDh/ON2Rhjxo8fbyIiIkxubm6+fa5Zs8Y0bdrUVKhQwZQvX940adLEzJ07t8C+paGoYzx27Ji59dZbTUhIiPH19TXXXXedGTdunMN9dowx5siRI6ZLly7G39/fVK5c2YwZM8bh0u3SUtTxffrppwX+XEsyhw8fNsa4/xi+/PLLpnr16qZs2bKmVatW5uuvv7ava9++vYmJiXHo/+6775rrr7/elC1b1jRs2NB89NFHDusv531Z2ooyxho1ahR4vCZNmmSMMebs2bPm9ttvN1WqVDFlypQxNWrUMIMHD3bJH5DiKMoYR40aZe8bGhpqunbtanbs2OGwP087jkX9Of3hhx+MJLNu3bp8+/LEY1jY74rz44qJiTHt27fPt03Tpk1N2bJlTe3atR3+Pp53sdetOGzGuOGaXwAAgFLCfXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAWFaHDh00atQod5cBwM0IOwA8Uvfu3dW5c+cC133++eey2WzavXt3KVcF4EpE2AHgkWJjY7V+/Xr99NNP+dYlJCSoZcuWl/x2bwCQCDsAPNQdd9yhKlWqaMGCBQ7tGRkZWr58uXr27Kl+/frpmmuuUbly5dS4cWMtXbr0ovu02WxatWqVQ1twcLDDcxw/flx9+vRRcHCwQkJC1KNHDx05csQ1gwLgFoQdAB7Jx8dH999/vxYsWKC/f4Xf8uXLlZubq/vuu08tWrTQRx99pO+//15DhgzRgAEDtHXrVqefMzs7W9HR0QoICNDnn3+uL7/8UhUqVFDnzp31559/umJYANyAsAPAYz3wwAM6dOiQPvvsM3tbQkKCevfurRo1amjs2LFq2rSpateureHDh6tz58569913nX6+d955R3l5eXrrrbfUuHFj1a9fXwkJCTp27Jg2bdrkghEBcAfCDgCPVa9ePbVt21bz58+XJCUlJenzzz9XbGyscnNzNXXqVDVu3FghISGqUKGCPv74Yx07dszp5/v222+VlJSkgIAAVahQQRUqVFBISIjOnTunQ4cOuWpYAEqZj7sLAICLiY2N1fDhwxUfH6+EhATVqVNH7du314wZM/Tiiy9qzpw5aty4scqXL69Ro0Zd9OMmm83m8JGY9NdHV+dlZGSoRYsWWrx4cb5tq1Sp4rpBAShVhB0AHq1Pnz4aOXKklixZokWLFumRRx6RzWbTl19+qR49eui+++6TJOXl5enAgQNq0KBBofuqUqWKTp48aX988OBBnT171v64efPmeuedd1S1alUFBgaW3KAAlCo+xgLg0SpUqKB77rlH48eP18mTJzVw4EBJUmRkpNavX6+vvvpK+/bt00MPPaSUlJSL7uu2227TK6+8op07d2rbtm16+OGHVaZMGfv6/v37q3LlyurRo4c+//xzHT58WJs2bdKIESMKvAQewJWBsAPA48XGxur3339XdHS0wsPDJUlPP/20mjdvrujoaHXo0EFhYWHq2bPnRffz/PPPKyIiQrfccovuvfdejR07VuXKlbOvL1eunDZv3qzq1aurV69eql+/vmJjY3Xu3DnO9ABXMJu58ANsAAAAC+HMDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsLT/B8bcdIV1QLtUAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert lds.A to a numpy array and plot the histogram\n",
    "A_data = new_lds_filtered.A.data.cpu().numpy()\n",
    "plt.hist(A_data, bins=100, alpha=0.75)\n",
    "plt.title('Histogram of lds.A')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of values over 0.9: 32.95%\n"
     ]
    }
   ],
   "source": [
    "percent_over_0_9 = np.sum(A_data > 0.9) / len(A_data) * 100\n",
    "print(f\"Percentage of values over 0.9: {percent_over_0_9:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LDS' object has no attribute 'phi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m stu \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstu)\n\u001b[1;32m----> 2\u001b[0m stu\u001b[38;5;241m.\u001b[39mphi \u001b[38;5;241m=\u001b[39m \u001b[43mstu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphi\u001b[49m\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[0;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(new_lds_filtered\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m      5\u001b[0m lds_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2001\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\devan\\anaconda3\\envs\\flashstu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1933\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LDS' object has no attribute 'phi'"
     ]
    }
   ],
   "source": [
    "stu = copy.deepcopy(model.layers[0].stu)\n",
    "stu.phi = stu.phi.to(torch.bfloat16)\n",
    "\n",
    "optimizer = torch.optim.Adam(new_lds_filtered.parameters(), lr = 0.0001)\n",
    "lds_epochs = 2001\n",
    "lds_loss_values = []\n",
    "\n",
    "for epoch in range(lds_epochs):\n",
    "    inputs = torch.randn(5, 4096, 768).to(device).to(torch.bfloat16)\n",
    "    stu_outputs = stu(inputs).to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = new_lds_filtered.compute_loss(inputs.to(stu_outputs.dtype), stu_outputs)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(new_lds_filtered.parameters(), max_norm=1)\n",
    "    lds_loss_values.append(loss.item())\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        new_lds_filtered.A.data.clamp_(max=1, min = -1)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-25 19:34:56,704 - INFO - Found 1 shards for split val\n"
     ]
    }
   ],
   "source": [
    "from dataloader import DataLoader\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    bsz=1,\n",
    "    seq_len=seq_len, \n",
    "    dataset='./fineweb-edu', \n",
    "    split=\"val\", \n",
    "    main_process=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import autocast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "def evaluate(model):\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    val_loss = 0.0\n",
    "    torch_dtype = getattr(torch, 'bfloat16')\n",
    "    val_steps = 5 # Arbitrarily set to reduce long evaluations, >20 typically used\n",
    "    model.eval()\n",
    "    val_loader.reset()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in zip(range(val_steps), val_loader, strict=False):\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            if torch_dtype != torch.float32:\n",
    "                with autocast(device_type=device.type, dtype=torch_dtype, cache_enabled=True):\n",
    "                    preds = model(inputs)\n",
    "            else:\n",
    "                preds = model(inputs)\n",
    "\n",
    "            loss = loss_fn(preds.flatten(0, 1), targets.flatten(0, 1))\n",
    "            loss = loss / val_steps\n",
    "            val_loss += loss.detach().float()\n",
    "    return(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.0742, device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stu = copy.deepcopy(model.layers[0].stu)\n",
    "model.layers[0].stu = lds_model.to(device)\n",
    "evaluate(model)\n",
    "\n",
    "#FOR SOME REASON, THE NEW LDS underperforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lds.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.2266, device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stu = copy.deepcopy(model.layers[0].stu)\n",
    "model.layers[0].stu = new_lds.to(device) #filtered, slow\n",
    "evaluate(model)\n",
    "\n",
    "#FOR SOME REASON, THE NEW LDS underperforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.6250, device='cuda:0')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stu = copy.deepcopy(model.layers[0].stu)\n",
    "model.layers[0].stu = new_lds_filtered.to(device) #filtered, fast\n",
    "evaluate(model)\n",
    "\n",
    "#FOR SOME REASON, THE NEW LDS underperforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs.to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 768])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lds_filtered = new_lds_filtered.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_t = new_lds_filtered.h0.expand(1, new_lds_filtered.state_dim).to(device)\n",
    "A = new_lds_filtered.A.flatten()\n",
    "all_h_t = []\n",
    "for t in range(1024):\n",
    "    u_t = inputs[:, t, :]\n",
    "    h_t = A * h_t + (u_t @ new_lds_filtered.B)\n",
    "    all_h_t.append(h_t.unsqueeze(1))\n",
    "all_h_t = torch.cat(all_h_t, dim=1)\n",
    "lds_out = torch.matmul(all_h_t, new_lds_filtered.C)\n",
    "new_ht = lds_out.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0559, -0.0489, -0.0393,  ..., -0.0129, -0.0298, -0.0301],\n",
       "         [ 0.0111,  0.1352, -0.0287,  ..., -0.1449,  0.0984, -0.0868],\n",
       "         [ 0.0075, -0.2174, -0.0297,  ...,  0.1494,  0.1317,  0.0024],\n",
       "         ...,\n",
       "         [-0.1460,  0.0488, -0.2218,  ..., -0.0125, -0.1097,  0.1176],\n",
       "         [-0.1231,  0.0952,  0.0334,  ..., -0.0448, -0.0257, -0.0463],\n",
       "         [-0.1197,  0.0074,  0.1675,  ..., -0.1491, -0.0926, -0.1805]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ht + compute_ar_x_preds(new_lds_filtered.M,inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "lds_model= lds_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_t = lds_model.h0.expand(1, lds_model.state_dim).to(device)\n",
    "A = lds_model.A.flatten()\n",
    "all_h_t = []\n",
    "for t in range(1024):\n",
    "    u_t = inputs[:, t, :]\n",
    "    h_t = A * h_t + (u_t @ lds_model.B)\n",
    "    all_h_t.append(h_t.unsqueeze(1))\n",
    "all_h_t = torch.cat(all_h_t, dim=1)\n",
    "lds_out = torch.matmul(all_h_t, lds_model.C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0369, -0.1756, -0.0140,  ..., -0.0095,  0.2034,  0.2664],\n",
       "         [-0.0109, -0.1869, -0.1448,  ..., -0.0877, -0.2060, -0.1952],\n",
       "         [-0.3321,  0.3115, -0.0473,  ..., -0.2979, -0.5652, -0.3678],\n",
       "         ...,\n",
       "         [-0.0103, -0.1460,  0.1361,  ..., -0.0476, -0.1077, -0.1046],\n",
       "         [-0.0336, -0.2587,  0.1905,  ...,  0.1077, -0.0199, -0.0399],\n",
       "         [ 0.1209, -0.1252, -0.2284,  ..., -0.1752, -0.1115,  0.1808]]],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can we remove D\n",
    "# finetune model after residual\n",
    "# factorization\n",
    "# is gaussian most realistic input data (uniform?, from data)\n",
    "# greedy fitting\n",
    "\n",
    "#why does an LDS exist that fits the STU\n",
    "#why does gradient descent find it\n",
    "\n",
    "#stu filters are 24 dim subspace of R^L, this subclass captures LDSs,\n",
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(-0.9963, device='cuda:0'),\n",
       " tensor(-0.9950, device='cuda:0'),\n",
       " tensor(-0.9950, device='cuda:0'),\n",
       " tensor(-0.9949, device='cuda:0'),\n",
       " tensor(-0.9948, device='cuda:0')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lds.A.data)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.9945, device='cuda:0'),\n",
       " tensor(0.9948, device='cuda:0'),\n",
       " tensor(0.9958, device='cuda:0'),\n",
       " tensor(0.9960, device='cuda:0'),\n",
       " tensor(0.9960, device='cuda:0')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lds.A.data)[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devan\\AppData\\Local\\Temp\\ipykernel_21096\\3432883371.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  lds_layer_0.load_state_dict(torch.load(\"./lds_layer_0.pt\"))\n",
      "C:\\Users\\devan\\AppData\\Local\\Temp\\ipykernel_21096\\3432883371.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  lds_layer_2.load_state_dict(torch.load(\"./lds_layer_2.pt\"))\n",
      "C:\\Users\\devan\\AppData\\Local\\Temp\\ipykernel_21096\\3432883371.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  lds_layer_4.load_state_dict(torch.load(\"./lds_layer_4.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.224609375\n"
     ]
    }
   ],
   "source": [
    "# Load LDS models for layers 0, 2, and 4\n",
    "\n",
    "lds_layer_0 = LDS(10000, 768, 768).to(device)\n",
    "lds_layer_0.load_state_dict(torch.load(\"./lds_layer_0.pt\"))\n",
    "\n",
    "lds_layer_2 = LDS(10000, 768, 768).to(device)\n",
    "lds_layer_2.load_state_dict(torch.load(\"./lds_layer_2.pt\"))\n",
    "\n",
    "lds_layer_4 = LDS(10000, 768, 768).to(device)\n",
    "lds_layer_4.load_state_dict(torch.load(\"./lds_layer_4.pt\"))\n",
    "\n",
    "# Substitute LDS models into model2\n",
    "model2.layers[0].stu = lds_layer_0\n",
    "model2.layers[2].stu = lds_layer_2\n",
    "model2.layers[4].stu = lds_layer_4\n",
    "\n",
    "# Run evaluation\n",
    "val_loss = evaluate(model2)\n",
    "print(f\"Validation Loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.3359375\n"
     ]
    }
   ],
   "source": [
    "model2 =  copy.deepcopy(model)\n",
    "model2.layers[0].stu = lds_layer_0\n",
    "# model2.layers[2].stu = lds_layer_2\n",
    "model2.layers[4].stu = lds_layer_4\n",
    "val_loss = evaluate(model2)\n",
    "print(f\"Validation Loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.8779, device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlashSTU(\n",
       "  (tok_emb): Embedding(200064, 768)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): STULayer(\n",
       "      (stu_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (stu): STU()\n",
       "      (mlp_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): AttentionLayer(\n",
       "      (attn_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (mlp_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): STULayer(\n",
       "      (stu_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (stu): STU()\n",
       "      (mlp_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): AttentionLayer(\n",
       "      (attn_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (mlp_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): STULayer(\n",
       "      (stu_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (stu): LDS()\n",
       "      (mlp_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): AttentionLayer(\n",
       "      (attn_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (mlp_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): STULayer(\n",
       "      (stu_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (stu): STU()\n",
       "      (mlp_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): AttentionLayer(\n",
       "      (attn_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (mlp_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): STULayer(\n",
       "      (stu_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (stu): STU()\n",
       "      (mlp_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): AttentionLayer(\n",
       "      (attn_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (mlp_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): STULayer(\n",
       "      (stu_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (stu): STU()\n",
       "      (mlp_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): AttentionLayer(\n",
       "      (attn_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (mlp_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=768, out_features=200064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[4].stu = stu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlashSTU(\n",
       "  (tok_emb): Embedding(200064, 768)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): STULayer(\n",
       "      (stu_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (stu): STU()\n",
       "      (mlp_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): AttentionLayer(\n",
       "      (attn_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (mlp_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): STULayer(\n",
       "      (stu_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (stu): STU()\n",
       "      (mlp_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): AttentionLayer(\n",
       "      (attn_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (mlp_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): STULayer(\n",
       "      (stu_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (stu): LDS()\n",
       "      (mlp_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): AttentionLayer(\n",
       "      (attn_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (mlp_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): STULayer(\n",
       "      (stu_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (stu): STU()\n",
       "      (mlp_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): AttentionLayer(\n",
       "      (attn_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (mlp_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): STULayer(\n",
       "      (stu_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (stu): STU()\n",
       "      (mlp_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): AttentionLayer(\n",
       "      (attn_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (mlp_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): STULayer(\n",
       "      (stu_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (stu): STU()\n",
       "      (mlp_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): AttentionLayer(\n",
       "      (attn_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (mlp_norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm((768,), eps=None, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=768, out_features=200064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flashstu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
